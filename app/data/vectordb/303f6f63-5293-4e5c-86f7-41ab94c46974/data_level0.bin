essor(images=image, query_images=query_image, return_tensors="pt")
        >>> with torch.no_grad():
        ...     outputs = model.image_guided_detection(**inputs)
        >>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]
        >>> target_sizes = torch.Tensor([image.size[::-1]])
        >>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
        >>> results = processor.post_process_image_guided_detection(
        ...     outputs=outputs, threshold=0.6, nms_threshold=0.3, target_sizes=target_sizes
        ... )
        >>> i = 0  # Retrieve predictions for the first image
        >>> boxes, scores = results[i]["boxes"], results[i]["scores"]
        >>> for box, score in zip(boxes, scores):
        ...     box = [round(i, 2) for i in box.tolist()]
        ...     print(f"Detected similar object with confidence {round(score.item(), 3)} at location {box}")
        Detected similar object with confidence 0.856 at location [10.94, 50.4, 315.8, 471.39]
        Detected similar object with confidence 1.0 at location [334.84, 25.33, 636.16, 374.71]
        ```"""
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.return_dict

        # Compute feature maps for the input and query images
        query_feature_map = self.image_embedder(
            pixel_values=query_pixel_values, interpolate_pos_encoding=interpolate_pos_encoding
        )[0]
        feature_map, vision_outputs = self.image_embedder(
            pixel_values=pixel_values,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            interpolate_pos_encoding=interpolate_pos_encoding,
        )

        batch_size, num_patches_height, num_patches_width, hidden_dim = feature_map.shape
        image_feats = torch.reshape(feature_map, (batch_size, num_patches_height * num_patches_width, hidden_dim))

        batch_size, num_patches_height, num_patches_width, hidden_dim = query_feature_map.shape
        query_image_feats = torch.reshape(
            query_feature_map, (batch_size, num_patches_height * num_patches_width, hidden_dim)
        )
        # Get top class embedding and best box index for each query image in batch
        query_embeds, best_box_indices, query_pred_boxes = self.embed_image_query(
            query_image_feats, query_feature_map, interpolate_pos_encoding
        )

        # Predict object classes [batch_size, num_patches, num_queries+1]
        (pred_logits, class_embeds) = self.class_predictor(image_feats=image_feats, query_embeds=query_embeds)

        # Predict object boxes
        target_pred_boxes = self.box_predictor(image_feats, feature_map, interpolate_pos_encoding)

        if not return_dict:
            output = (
                feature_map,
                query_feature_map,
                target_pred_boxes,
                query_pred_boxes,
                pred_logits,
                class_embeds,
                vision_outputs.to_tuple(),
            )
            output = tuple(x for x in output if x is not None)
            return output

        return OwlViTImageGuidedObjectDetectionOutput(
            image_embeds=feature_map,
            query_image_embeds=query_feature_map,
            target_pred_boxes=target_pred_boxes,
            query_pred_boxes=query_pred_boxes,
            logits=pred_logits,
            class_embeds=class_embeds,
            text_model_output=None,
            vision_model_output=vision_outputs,
        )

    @auto_docstring
    def forward(
        self,
        input_ids: torch.Tensor,
        pixel_values: torch.FloatTensor,
        attention_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        interpolate_pos_encoding: bool = False,
        return_dict: Optional[bool] = None,
    ) -> OwlViTObjectDetectionOutput:
        r"""
        input_ids (`torch.LongTensor` of shape `(batch_size * num_max_text_queries, sequence_length)`, *optional*):
            Indices of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`]. See
            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details. [What are input
            IDs?](../glossary#input-ids).
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the last hidden state. See `text_model_last_hidden_state` and
            `vision_model_last_hidden_state` under returned tensors for more detail.

        Examples:
        ```python
        >>> import requests
        >>> from PIL import Image
        >>> import torch

        >>> from transformers import OwlViTProcessor, OwlViTForObjectDetection

        >>> processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
        >>> model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)
        >>> text_labels = [["a photo of a cat", "a photo of a dog"]]
        >>> inputs = processor(text=text_labels, images=image, return_tensors="pt")
        >>> outputs = model(**inputs)

        >>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]
        >>> target_sizes = torch.tensor([(image.height, image.width)])
        >>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
        >>> results = processor.post_process_grounded_object_detection(
        ...     outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels
        ... )
        >>> # Retrieve predictions for the first image for the corresponding text queries
        >>> result = results[0]
        >>> boxes, scores, text_labels = result["boxes"], result["scores"], result["text_labels"]
        >>> for box, score, text_label in zip(boxes, scores, text_labels):
        ...     box = [round(i, 2) for i in box.tolist()]
        ...     print(f"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}")
        Detected a photo of a cat with confidence 0.707 at location [324.97, 20.44, 640.58, 373.29]
        Detected a photo of a cat with confidence 0.717 at location [1.46, 55.26, 315.55, 472.17]
        ```"""
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.return_dict

        # Embed images and text queries
        query_embeds, feature_map, outputs = self.image_text_embedder(
            input_ids=input_ids,
            pixel_values=pixel_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            interpolate_pos_encoding=interpolate_pos_encoding,
        )

        # Text and vision model outputs
        text_outputs = outputs.text_model_output
        vision_outputs = outputs.vision_model_output

        batch_size, num_patches_height, num_patches_width, hidden_dim = feature_map.shape
        image_feats = torch.reshape(feature_map, (batch_size, num_patches_height * num_patches_width, hidden_dim))

        # Reshape from [batch_size * max_text_queries, hidden_dim] -> [batch_size, max_text_queries, hidden_dim]
        max_text_queries = input_ids.shape[0] // batch_size
        query_embeds = query_embeds.reshape(batch_size, max_text_queries, query_embeds.shape[-1])

        # If first token is 0, then this is a padded query [batch_size, num_queries].
        input_ids = input_ids.reshape(batch_size, max_text_queries, input_ids.shape[-1])
        query_mask = input_ids[..., 0] > 0

        # Predict object classes [batch_size, num_patches, num_queries+1]
        (pred_logits, class_embeds) = self.class_predictor(image_feats, query_embeds, query_mask)

        # Predict object boxes
        pred_boxes = self.box_predictor(image_feats, feature_map, interpolate_pos_encoding)

        if not return_dict:
            output = (
                pred_logits,
                pred_boxes,
                query_embeds,
                feature_map,
                class_embeds,
                text_outputs.to_tuple(),
                vision_outputs.to_tuple(),
            )
            output = tuple(x for x in output if x is not None)
            return output

        return OwlViTObjectDetectionOutput(
            image_embeds=feature_map,
            text_embeds=query_embeds,
            pred_boxes=pred_boxes,
            logits=pred_logits,
            class_embeds=class_embeds,
            text_model_output=text_outputs,
            vision_model_output=vision_outputs,
        )


__all__ = ["OwlViTModel", "OwlViTPreTrainedModel", "OwlViTTextModel", "OwlViTVisionModel", "OwlViTForObjectDetection"]
         `   "   r   e   f   l   e   c   t   "   `   :       p   a   d   s       w   i   t   h       t   h   e       r   e   f   l   e   c   t   i   o   n       o   f       t   h   e       v   e   c   t   o   r       m   i   r   r   o   r   e   d       o   n       t   h   e       f   i   r   s   t       a   n   d       l   a   s   t       v   a   l   u   e   s       o   f       t   h   e   
                                                                                   v   e   c   t   o   r       a   l   o   n   g       e   a   c   h       a   x   i   s   .   
                                                                                   -       `   "   r   e   p   l   i   c   a   t   e   "   `   :       p   a   d   s       w   i   t   h       t   h   e       r   e   p   l   i   c   a   t   i   o   n       o   f       t   h   e       l   a   s   t       v   a   l   u   e       o   n       t   h   e       e   d   g   e       o   f       t   h   e       a   r   r   a   y       a   l   o   n   g       e   a   c   h       a   x   i   s   .   
                                                                                   -       `   "   s   y   m   m   e   t   r   i   c   "   `   :       p   a   d   s       w   i   t   h       t   h   e       r   e   f   l   e   c   t   i   o   n       o   f       t   h   e       v   e   c   t   o   r       m   i   r   r   o   r   e   d       a   l   o   n   g       t   h   e       e   d   g   e       o   f       t   h   e       a   r   r   a   y   .   
                                                   c   o   n   s   t   a   n   t   _   v   a   l   u   e   s       (   `   f   l   o   a   t   `       o   r       `   I   t   e   r   a   b   l   e   [   f   l   o   a   t   ]   `   ,       *   o   p   t   i   o   n   a   l   *   )   :   
                                                                   T   h   e       v   a   l   u   e       t   o       u   s   e       f   o   r       t   h   e       p   a   d   d   i   n   g       i   f       `   m   o   d   e   `       i   s       `   "   c   o   n   s   t   a   n   t   "   `   .   
                                                   d   a   t   a   _   f   o   r   m   a   t       (   `   s   t   r   `       o   r       `   C   h   a   n   n   e   l   D   i   m   e   n   s   i   o   n   `   ,       *   o   p   t   i   o   n   a   l   *   )   :   
                                                                   T   h   e       c   h   a   n   n   e   l       d   i   m   e   n   s   i   o   n       f   o   r   m   a   t       f   o   r       t   h   e       o   u   t   p   u   t       i   m   a   g   e   .       C   a   n       b   e       o   n   e       o   f   :   
                                                                                   -       `   "   c   h   a   n   n   e   l   s   _   f   i   r   s   t   "   `       o   r       `   C   h   a   n   n   e   l   D   i   m   e   n   s   i   o   n   .   F   I   R   S   T   `   :       i   m   a   g   e       i   n       (   n   u   m   _   c   h   a   n   n   e   l   s   ,       h   e   i   g   h   t   ,       w   i   d   t   h   )       f   o   r   m   a   t   .   
                                                                                   -       `   "   c   h   a   n   n   e   l   s   _   l   a   s   t   "   `       o   r       `   C   h   a   n   n   e   l   D   i   m   e   n   s   i   o   n   .   L   A   S   T   `   :       i   m   a   g   e       i   n       (   h   e   i   g   h   t   ,       w   i   d   t   h   ,       n   u   m   _   c   h   a   n   n   e   l   s   )       f   o   r   m   a   t   .   
                                                                   I   f       u   n   s   e   t   ,       w   i   l   l       u   s   e       s   a   m   e       a   s       t   h   e       i   n   p   u   t       i   m   a   g   e   .   
                                                   i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t       (   `   s   t   r   `       o   r       `   C   h   a   n   n   e   l   D   i   m   e   n   s   i   o   n   `   ,       *   o   p   t   i   o   n   a   l   *   )   :   
                                                                   T   h   e       c   h   a   n   n   e   l       d   i   m   e   n   s   i   o   n       f   o   r   m   a   t       f   o   r       t   h   e       i   n   p   u   t       i   m   a   g   e   .       C   a   n       b   e       o   n   e       o   f   :   
                                                                                   -       `   "   c   h   a   n   n   e   l   s   _   f   i   r   s   t   "   `       o   r       `   C   h   a   n   n   e   l   D   i   m   e   n   s   i   o   n   .   F   I   R   S   T   `   :       i   m   a   g   e       i   n       (   n   u   m   _   c   h   a   n   n   e   l   s   ,       h   e   i   g   h   t   ,       w   i   d   t   h   )       f   o   r   m   a   t   .   
                                                                                   -       `   "   c   h   a   n   n   e   l   s   _   l   a   s   t   "   `       o   r       `   C   h   a   n   n   e   l   D   i   m   e   n   s   i   o   n   .   L   A   S   T   `   :       i   m   a   g   e       i   n       (   h   e   i   g   h   t   ,       w   i   d   t   h   ,       n   u   m   _   c   h   a   n   n   e   l   s   )       f   o   r   m   a   t   .   
                                                                   I   f       u   n   s   e   t   ,       w   i   l   l       u   s   e       t   h   e       i   n   f   e   r   r   e   d       f   o   r   m   a   t       o   f       t   h   e       i   n   p   u   t       i   m   a   g   e   .   
   
                                   R   e   t   u   r   n   s   :   
                                                   `   n   p   .   n   d   a   r   r   a   y   `   :       T   h   e       p   a   d   d   e   d       i   m   a   g   e   .   
   
                                   "   "   "   
   
                                   #       c   a   l   l       t   h   e       g   e   n   e   r   a   l       `   p   a   d   `       i   f       p   a   d   d   i   n   g       o   n       `   h   e   i   g   h   t   /   w   i   d   t   h   `   ,       o   t   h   e   r   w   i   s   e       i   t   '   s       t   h   e       `   n   u   m   _   p   a   t   c   h   e   d   `       d   i   m   
                                   i   f       i   s   i   n   s   t   a   n   c   e   (   p   a   d   d   i   n   g   ,       i   n   t   )       o   r       l   e   n   (   p   a   d   d   i   n   g   )       !   =       4   :   
                                                   r   e   t   u   r   n       p   a   d   (   i   m   a   g   e   ,       p   a   d   d   i   n   g   ,       m   o   d   e   ,       c   o   n   s   t   a   n   t   _   v   a   l   u   e   s   ,       d   a   t   a   _   f   o   r   m   a   t   ,       i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t   )   
   
                                   i   f       i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t       i   s       N   o   n   e   :   
                                                   i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t       =       i   n   f   e   r   _   c   h   a   n   n   e   l   _   d   i   m   e   n   s   i   o   n   _   f   o   r   m   a   t   (   i   m   a   g   e   )   
   
                                   p   a   d   d   i   n   g   _   m   o   d   e   _   m   a   p   p   i   n   g       =       {   
                                                   P   a   d   d   i   n   g   M   o   d   e   .   C   O   N   S   T   A   N   T   :       "   c   o   n   s   t   a   n   t   "   ,   
                                                   P   a   d   d   i   n   g   M   o   d   e   .   R   E   F   L   E   C   T   :       "   r   e   f   l   e   c   t   "   ,   
                                                   P   a   d   d   i   n   g   M   o   d   e   .   R   E   P   L   I   C   A   T   E   :       "   e   d   g   e   "   ,   
                                                   P   a   d   d   i   n   g   M   o   d   e   .   S   Y   M   M   E   T   R   I   C   :       "   s   y   m   m   e   t   r   i   c   "   ,   
                                   }   
                                   i   m   a   g   e       =       n   p   .   p   a   d   (   i   m   a   g   e   ,       p   a   d   d   i   n   g   ,       m   o   d   e   =   p   a   d   d   i   n   g   _   m   o   d   e   _   m   a   p   p   i   n   g   [   m   o   d   e   ]   ,       c   o   n   s   t   a   n   t   _   v   a   l   u   e   s   =   c   o   n   s   t   a   n   t   _   v   a   l   u   e   s   )   
                                   i   m   a   g   e       =       (   
                                                   t   o   _   c   h   a   n   n   e   l   _   d   i   m   e   n   s   i   o   n   _   f   o   r   m   a   t   (   i   m   a   g   e   ,       d   a   t   a   _   f   o   r   m   a   t   ,       i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t   )       i   f       d   a   t   a   _   f   o   r   m   a   t       i   s       n   o   t       N   o   n   e       e   l   s   e       i   m   a   g   e   
                                   )   
                                   r   e   t   u   r   n       i   m   a   g   e   
   
                   d   e   f       g   e   t   _   i   m   a   g   e   _   p   a   t   c   h   e   s   (   
                                   s   e   l   f   ,   
                                   i   m   a   g   e   :       n   p   .   a   r   r   a   y   ,   
                                   g   r   i   d   _   p   i   n   p   o   i   n   t   s   :       L   i   s   t   [   T   u   p   l   e   [   i   n   t   ,       i   n   t   ]   ]   ,   
                                   p   a   t   c   h   _   s   i   z   e   :       i   n   t   ,   
                                   r   e   s   a   m   p   l   e   :       P   I   L   I   m   a   g   e   R   e   s   a   m   p   l   i   n   g   ,   
                                   d   a   t   a   _   f   o   r   m   a   t   :       C   h   a   n   n   e   l   D   i   m   e   n   s   i   o   n   ,   
                                   i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t   :       C   h   a   n   n   e   l   D   i   m   e   n   s   i   o   n   ,   
                   )       -   >       L   i   s   t   [   n   p   .   a   r   r   a   y   ]   :   
                                   "   "   "   
                                   P   r   o   c   e   s   s       a   n       i   m   a   g   e       w   i   t   h       v   a   r   i   a   b   l   e       r   e   s   o   l   u   t   i   o   n   s       b   y       d   i   v   i   d   i   n   g       i   t       i   n   t   o       p   a   t   c   h   e   s   .   
   
                                   A   r   g   s   :   
                                                   i   m   a   g   e       (   `   n   p   .   a   r   r   a   y   `   )   :   
                                                                   T   h   e       i   n   p   u   t       i   m   a   g   e       t   o       b   e       p   r   o   c   e   s   s   e   d   .   
                                                   g   r   i   d   _   p   i   n   p   o   i   n   t   s       (   L   i   s   t   [   T   u   p   l   e   [   i   n   t   ,       i   n   t   ]   ]   )   :   
                                                                   A       l   i   s   t       o   f       p   o   s   s   i   b   l   e       r   e   s   o   l   u   t   i   o   n   s       a   s       t   u   p   l   e   s   .   
                                                   p   a   t   c   h   _   s   i   z   e       (   `   i   n   t   `   )   :   
                                                                   S   i   z   e       o   f       t   h   e       p   a   t   c   h   e   s       t   o       d   i   v   i   d   e       t   h   e       i   m   a   g   e       i   n   t   o   .   
                                                   r   e   s   a   m   p   l   e       (   `   P   I   L   I   m   a   g   e   R   e   s   a   m   p   l   i   n   g   `   )   :   
                                                                   R   e   s   a   m   p   l   i   n   g       f   i   l   t   e   r       t   o       u   s   e       i   f       r   e   s   i   z   i   n   g       t   h   e       i   m   a   g   e   .   
                                                   d   a   t   a   _   f   o   r   m   a   t       (   `   C   h   a   n   n   e   l   D   i   m   e   n   s   i   o   n   `       o   r       `   s   t   r   `   )   :   
                                                                   T   h   e       c   h   a   n   n   e   l       d   i   m   e   n   s   i   o   n       f   o   r   m   a   t       f   o   r       t   h   e       o   u   t   p   u   t       i   m   a   g   e   .   
                                                   i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t       (   `   C   h   a   n   n   e   l   D   i   m   e   n   s   i   o   n   `       o   r       `   s   t   r   `   )   :   
                                                                   T   h   e       c   h   a   n   n   e   l       d   i   m   e   n   s   i   o   n       f   o   r   m   a   t       o   f       t   h   e       i   n   p   u   t       i   m   a   g   e   .   
   
                                   R   e   t   u   r   n   s   :   
                                                   `   L   i   s   t   [   n   p   .   a   r   r   a   y   ]   `   :       A       l   i   s   t       o   f       N   u   m   P   y       a   r   r   a   y   s       c   o   n   t   a   i   n   i   n   g       t   h   e       p   r   o   c   e   s   s   e   d       i   m   a   g   e       p   a   t   c   h   e   s   .   
                                   "   "   "   
                                   i   f       n   o   t       i   s   i   n   s   t   a   n   c   e   (   g   r   i   d   _   p   i   n   p   o   i   n   t   s   ,       l   i   s   t   )   :   
                                                   r   a   i   s   e       T   y   p   e   E   r   r   o   r   (   "   g   r   i   d   _   p   i   n   p   o   i   n   t   s       m   u   s   t       b   e       a       l   i   s   t       o   f       p   o   s   s   i   b   l   e       r   e   s   o   l   u   t   i   o   n   s   .   "   )   
   
                                   p   o   s   s   i   b   l   e   _   r   e   s   o   l   u   t   i   o   n   s       =       g   r   i   d   _   p   i   n   p   o   i   n   t   s   
   
                                   i   m   a   g   e   _   s   i   z   e       =       g   e   t   _   i   m   a   g   e   _   s   i   z   e   (   i   m   a   g   e   ,       c   h   a   n   n   e   l   _   d   i   m   =   i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t   )   
                                   b   e   s   t   _   r   e   s   o   l   u   t   i   o   n       =       s   e   l   e   c   t   _   b   e   s   t   _   r   e   s   o   l   u   t   i   o   n   (   i   m   a   g   e   _   s   i   z   e   ,       p   o   s   s   i   b   l   e   _   r   e   s   o   l   u   t   i   o   n   s   )   
                                   r   e   s   i   z   e   d   _   i   m   a   g   e       =       s   e   l   f   .   _   r   e   s   i   z   e   _   f   o   r   _   p   a   t   c   h   i   n   g   (   
                                                   i   m   a   g   e   ,       b   e   s   t   _   r   e   s   o   l   u   t   i   o   n   ,       r   e   s   a   m   p   l   e   =   r   e   s   a   m   p   l   e   ,       i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t   =   i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t   
                                   )   
                                   p   a   d   d   e   d   _   i   m   a   g   e       =       s   e   l   f   .   _   p   a   d   _   f   o   r   _   p   a   t   c   h   i   n   g   (   r   e   s   i   z   e   d   _   i   m   a   g   e   ,       b   e   s   t   _   r   e   s   o   l   u   t   i   o   n   ,       i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t   =   i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t   )   
   
                                   p   a   t   c   h   e   s       =       d   i   v   i   d   e   _   t   o   _   p   a   t   c   h   e   s   (   p   a   d   d   e   d   _   i   m   a   g   e   ,       p   a   t   c   h   _   s   i   z   e   =   p   a   t   c   h   _   s   i   z   e   ,       i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t   =   i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t   )   
   
                                   #       m   a   k   e       s   u   r   e       t   h   a   t       a   l   l       p   a   t   c   h   e   s       a   r   e       i   n       t   h   e       i   n   p   u   t       d   a   t   a       f   o   r   m   a   t   
                                   p   a   t   c   h   e   s       =       [   
                                                   t   o   _   c   h   a   n   n   e   l   _   d   i   m   e   n   s   i   o   n   _   f   o   r   m   a   t   (   p   a   t   c   h   ,       c   h   a   n   n   e   l   _   d   i   m   =   d   a   t   a   _   f   o   r   m   a   t   ,       i   n   p   u   t   _   c   h   a   n   n   e   l   _   d   i   m   =   i   n   p   u   t   _   d   a   t   a   _   f   o   r   m   a   t   )   
                                                   f   o   r       p   a   t   c   h       i   n       p   a   t   c   h   e   s   
                                   ]   
                                   r   e   t   u   r   n       p   a   t   c   h   e   s   
   
   
   _   _   a   l   l   _   _       =       [   "   A   r   i   a   I   m   a   g   e   P   r   o   c   e   s   s   o   r   "   ]   
       l   f   .   n   o   r   m   _   t   o   p   k   _   p   r   o   b   :   
                                                   d   e   n   o   m   i   n   a   t   o   r       =       t   o   p   k   _   w   e   i   g   h   t   s   .   s   u   m   (   d   i   m   =   -   1   ,       k   e   e   p   d   i   m   =   T   r   u   e   )       +       1   e   -   2   0   
                                                   t   o   p   k   _   w   e   i   g   h   t   s       /   =       d   e   n   o   m   i   n   a   t   o   r   
                                   t   o   p   k   _   w   e   i   g   h   t   s       =       t   o   p   k   _   w   e   i   g   h   t   s       *       s   e   l   f   .   r   o   u   t   e   d   _   s   c   a   l   i   n   g   _   f   a   c   t   o   r   
                                   r   e   t   u   r   n       t   o   p   k   _   i   n   d   i   c   e   s   ,       t   o   p   k   _   w   e   i   g   h   t   s   
   
   
   c   l   a   s   s       D   e   e   p   s   e   e   k   V   3   M   o   E   (   n   n   .   M   o   d   u   l   e   )   :   
                   "   "   "   
                   A           i   x   e   d       e   x   p   e   r   t       m   o   d   u   l   e       c   o   n   t   a   i   n   i   n   g       s   h   a   r   e   d       e   x   p   e   r   t   s   .   
                   "   "   "   
   
                   d   e   f       _   _   i   n   i   t   _   _   (   s   e   l   f   ,       c   o   n   f   i   g   )   :   
                                   s   u   p   e   r   (   )   .   _   _   i   n   i   t   _   _   (   )   
                                   s   e   l   f   .   c   o   n   f   i   g       =       c   o   n   f   i   g   
                                   s   e   l   f   .   e   x   p   e   r   t   s       =       n   n   .   M   o   d   u   l   e   L   i   s   t   (   
                                                   [   
                                                                   D   e   e   p   s   e   e   k   V   3   M   L   P   (   c   o   n   f   i   g   ,       i   n   t   e   r   m   e   d   i   a   t   e   _   s   i   z   e   =   c   o   n   f   i   g   .   m   o   e   _   i   n   t   e   r   m   e   d   i   a   t   e   _   s   i   z   e   )   
                                                                   f   o   r       _       i   n       r   a   n   g   e   (   c   o   n   f   i   g   .   n   _   r   o   u   t   e   d   _   e   x   p   e   r   t   s   )   
                                                   ]   
                                   )   
                                   s   e   l   f   .   g   a   t   e       =       D   e   e   p   s   e   e   k   V   3   T   o   p   k   R   o   u   t   e   r   (   c   o   n   f   i   g   )   
                                   s   e   l   f   .   s   h   a   r   e   d   _   e   x   p   e   r   t   s       =       D   e   e   p   s   e   e   k   V   3   M   L   P   (   
                                                   c   o   n   f   i   g   =   c   o   n   f   i   g   ,       i   n   t   e   r   m   e   d   i   a   t   e   _   s   i   z   e   =   c   o   n   f   i   g   .   m   o   e   _   i   n   t   e   r   m   e   d   i   a   t   e   _   s   i   z   e       *       c   o   n   f   i   g   .   n   _   s   h   a   r   e   d   _   e   x   p   e   r   t   s   
                                   )   
   
                   d   e   f       m   o   e   (   s   e   l   f   ,       h   i   d   d   e   n   _   s   t   a   t   e   s   :       t   o   r   c   h   .   T   e   n   s   o   r   ,       t   o   p   k   _   i   n   d   i   c   e   s   :       t   o   r   c   h   .   T   e   n   s   o   r   ,       t   o   p   k   _   w   e   i   g   h   t   s   :       t   o   r   c   h   .   T   e   n   s   o   r   )   :   
                                   r   "   "   "   
                                   C   A   L   L       F   O   R       C   O   N   T   R   I   B   U   T   I   O   N   !       I       d   o   n   '   t       h   a   v   e       t   i   m   e       t   o       o   p   t   i   m   i   s   e       t   h   i   s       r   i   g   h   t       n   o   w   ,       b   u   t       e   x   p   e   r   t       w   e   i   g   h   t   s       n   e   e   d       t   o       b   e       f   u   s   e   d   
                                   t   o       n   o   t       h   a   v   e       t   o       d   o       a       l   o   o   p       h   e   r   e       (   d   e   e   p   s   e   e   k       h   a   s       2   5   6       e   x   p   e   r   t   s       s   o   o   o   o       y   e   a   h   )   .   
                                   "   "   "   
                                   f   i   n   a   l   _   h   i   d   d   e   n   _   s   t   a   t   e   s       =       t   o   r   c   h   .   z   e   r   o   s   _   l   i   k   e   (   h   i   d   d   e   n   _   s   t   a   t   e   s   ,       d   t   y   p   e   =   t   o   p   k   _   w   e   i   g   h   t   s   .   d   t   y   p   e   )   
                                   e   x   p   e   r   t   _   m   a   s   k       =       t   o   r   c   h   .   n   n   .   f           ÈØÔ   Ç$     ÿÿÿÿÿÿÿÿd   .   # coding=utf-8
# Copyright 2022 Google AI and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""PyTorch OWL-ViT model."""

from dataclasses import dataclass
from functools import lru_cache
from typing import Any, Dict, Optional, Tuple, Union

import torch
import torch.utils.checkpoint
from torch import Tensor, nn

from ...activations import ACT2FN
from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask
from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring, is_vision_available, logging, torch_int
from .configuration_owlvit import OwlViTConfig, OwlViTTextConfig, OwlViTVisionConfig


if is_vision_available():
    from transformers.image_transforms import center_to_corners_format


logger = logging.get_logger(__name__)


# See all OwlViT models at https://huggingface.co/models?filter=owlvit


# Copied from transformers.models.clip.modeling_clip.contrastive_loss with clip->owlvit
def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:
    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))


# Copied from transformers.models.clip.modeling_clip.clip_loss with clip->owlvit
def owlvit_loss(similarity: torch.Tensor) -> torch.Tensor:
    caption_loss = contrastive_loss(similarity)
    image_loss = contrastive_loss(similarity.t())
    return (caption_loss + image_loss) / 2.0


@dataclass
class OwlViTOutput(ModelOutput):
    """
    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):
            Contrastive loss for image-text similarity.
        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):
            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text
            similarity scores.
        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):
            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image
            similarity scores.
        text_embeds (`torch.FloatTensor` of shape `(batch_size * num_max_text_queries, output_dim`):
            The text embeddings obtained by applying the projection layer to the pooled output of [`OwlViTTextModel`].
        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):
            The image embeddings obtained by applying the projection layer to the pooled output of
            [`OwlViTVisionModel`].
        text_model_output (Tuple[`BaseModelOutputWithPooling`]):
            The output of the [`OwlViTTextModel`].
        vision_model_output (`BaseModelOutputWithPooling`):
            The output of the [`OwlViTVisionModel`].
    """

    loss: Optional[torch.FloatTensor] = None
    logits_per_image: Optional[torch.FloatTensor] = None
    logits_per_text: Optional[torch.FloatTensor] = None
    text_embeds: Optional[torch.FloatTensor] = None
    image_embeds: Optional[torch.FloatTensor] = None
    text_model_output: BaseModelOutputWithPooling = None
    vision_model_output: BaseModelOutputWithPooling = None

    def to_tuple(self) -> Tuple[Any]:
        return tuple(
            self[k] if k not in ["text_model_output", "vision_model_output"] else getattr(self, k).to_tuple()
            for k in self.keys()
        )


# Copied from transformers.loss.loss_for_object_detection._upcast
def _upcast(t: Tensor) -> Tensor:
    # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type
    if t.is_floating_point():
        return t if t.dtype in (torch.float32, torch.float64) else t.float()
    else:
        return t if t.dtype in (torch.int32, torch.int64) else t.int()


# Copied from transformers.loss.loss_for_object_detection.box_area
def box_area(boxes: Tensor) -> Tensor:
    """
    Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.

    Args:
        boxes (`torch.FloatTensor` of shape `(number_of_boxes, 4)`):
            Boxes for which the area will be computed. They are expected to be in (x1, y1, x2, y2) format with `0 <= x1
            < x2` and `0 <= y1 < y2`.

    Returns:
        `torch.FloatTensor`: a tensor containing the area for each box.
    """
    boxes = _upcast(boxes)
    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])


# Copied from transformers.loss.loss_for_object_detection.box_iou
def box_iou(boxes1, boxes2):
    area1 = box_area(boxes1)
    area2 = box_area(boxes2)

    left_top = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]
    right_bottom = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]

    width_height = (right_bottom - left_top).clamp(min=0)  # [N,M,2]
    inter = width_height[:, :, 0] * width_height[:, :, 1]  # [N,M]

    union = area1[:, None] + area2 - inter

    iou = inter / union
    return iou, union


# Copied from transformers.loss.loss_for_object_detection.generalized_box_iou
def generalized_box_iou(boxes1, boxes2):
    """
    Generalized IoU from https://giou.stanford.edu/. The boxes should be in [x0, y0, x1, y1] (corner) format.

    Returns:
        `torch.FloatTensor`: a [N, M] pairwise matrix, where N = len(boxes1) and M = len(boxes2)
    """
    # degenerate boxes gives inf / nan results
    # so do an early check
    if not (boxes1[:, 2:] >= boxes1[:, :2]).all():
        raise ValueError(f"boxes1 must be in [x0, y0, x1, y1] (corner) format, but got {boxes1}")
    if not (boxes2[:, 2:] >= boxes2[:, :2]).all():
        raise ValueError(f"boxes2 must be in [x0, y0, x1, y1] (corner) format, but got {boxes2}")
    iou, union = box_iou(boxes1, boxes2)

    top_left = torch.min(boxes1[:, None, :2], boxes2[:, :2])
    bottom_right = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])

    width_height = (bottom_right - top_left).clamp(min=0)  # [N,M,2]
    area = width_height[:, :, 0] * width_height[:, :, 1]

    return iou - (area - union) / area


@dataclass
class OwlViTObjectDetectionOutput(ModelOutput):
    """
    Output type of [`OwlViTForObjectDetection`].

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):
            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
            scale-invariant IoU loss.
        loss_dict (`Dict`, *optional*):
            A dictionary containing the individual losses. Useful for logging.
        logits (`torch.FloatTensor` of shape `(batch_size, num_patches, num_queries)`):
            Classification logits (including no-object) for all queries.
        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):
            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
            possible padding). You can use [`~OwlViTImageProcessor.post_process_object_detection`] to retrieve the
            unnormalized bounding boxes.
        text_embeds (`torch.FloatTensor` of shape `(batch_size, num_max_text_queries, output_dim`):
            The text embeddings obtained by applying the projection layer to the pooled output of [`OwlViTTextModel`].
        image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):
            Pooled output of [`OwlViTVisionModel`]. OWL-ViT represents images as a set of image patches and computes
            image embeddings for each patch.
        class_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`):
            Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total
            number of patches is (image_size / patch_size)**2.
        text_model_output (Tuple[`BaseModelOutputWithPooling`]):
            The output of the [`OwlViTTextModel`].
        vision_model_output (`BaseModelOutputWithPooling`):
            The output of the [`OwlViTVisionModel`].
    """

    loss: Optional[torch.FloatTensor] = None
    loss_dict: Optional[Dict] = None
    logits: Optional[torch.FloatTensor] = None
    pred_boxes: Optional[torch.FloatTensor] = None
    text_embeds: Optional[torch.FloatTensor] = None
    image_embeds: Optional[torch.FloatTensor] = None
    class_embeds: Optional[torch.FloatTensor] = None
    text_model_output: BaseModelOutputWithPooling = None
    vision_model_output: BaseModelOutputWithPooling = None

    def to_tuple(self) -> Tuple[Any]:
        return tuple(
            self[k] if k not in ["text_model_output", "vision_model_output"] else getattr(self, k).to_tuple()
            for k in self.keys()
        )


@dataclass
class OwlViTImageGuidedObjectDetectionOutput(ModelOutput):
    """
    Output type of [`OwlViTForObjectDetection.image_guided_detection`].

    Args:
        logits (`torch.FloatTensor` of shape `(batch_size, num_patches, num_queries)`):
            Classification logits (including no-object) for all queries.
        target_pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):
            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
            values are normalized in [0, 1], relative to the size of each individual target image in the batch
            (disregarding possible padding). You can use [`~OwlViTImageProcessor.post_process_object_detection`] to
            retrieve the unnormalized bounding boxes.
        query_pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):
            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
            values are normalized in [0, 1], relative to the size of each individual query image in the batch
            (disregarding possible padding). You can use [`~OwlViTImageProcessor.post_process_object_detection`] to
            retrieve the unnormalized bounding boxes.
        image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):
            Pooled output of [`OwlViTVisionModel`]. OWL-ViT represents images as a set of image patches and computes
            image embeddings for each patch.
        query_image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):
            Pooled output of [`OwlViTVisionModel`]. OWL-ViT represents images as a set of image patches and computes
            image embeddings for each patch.
        class_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`):
            Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total
            number of patches is (image_size / patch_size)**2.
        text_model_output (Tuple[`BaseModelOutputWithPooling`]):
            The output of the [`OwlViTTextModel`].
        vision_model_output (`BaseModelOutputWithPooling`):
            The output of the [`OwlViTVisionModel`].
    """

    logits: Optional[torch.FloatTensor] = None
    image_embeds: Optional[torch.FloatTensor] = None
    query_image_embeds: Optional[torch.FloatTensor] = None
    target_pred_boxes: Optional[torch.FloatTensor] = None
    query_pred_boxes: Optional[torch.FloatTensor] = None
    class_embeds: Optional[torch.FloatTensor] = None
    text_model_output: BaseModelOutputWithPooling = None
    vision_model_output: BaseModelOutputWithPooling = None

    def to_tuple(self) -> Tuple[Any]:
        return tuple(
            self[k] if k not in ["text_model_output", "vision_model_output"] else getattr(self, k).to_tuple()
            for k in self.keys()
        )


class OwlViTVisionEmbeddings(nn.Module):
    def __init__(self, config: OwlViTVisionConfig):
        super().__init__()
        self.patch_size = config.patch_size
        self.config = config
        self.embed_dim = config.hidden_size
        self.class_embedding = nn.Parameter(torch.randn(config.hidden_size))

        self.patch_embedding = nn.Conv2d(
            in_channels=config.num_channels,
            out_channels=self.embed_dim,
            kernel_size=config.patch_size,
            stride=config.patch_size,
            bias=False,
        )

        self.num_patches = (config.image_size // config.patch_size) ** 2
        self.num_positions = self.num_patches + 1
        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)
        self.register_buffer("position_ids", torch.arange(self.num_positions).expand((1, -1)), persistent=False)

    # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings.interpolate_pos_encoding
    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:
        """
        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution
        images. This method is also adapted to support torch.jit tracing.

        Adapted from:
        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and
        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211
        """

        num_patches = embeddings.shape[1] - 1
        position_embedding = self.position_embedding.weight.unsqueeze(0)
        num_positions = position_embedding.shape[1] - 1

        # always interpolate when tracing to ensure the exported model works for dynamic input shapes
        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:
            return self.position_embedding(self.position_ids)

        class_pos_embed = position_embedding[:, :1]
        patch_pos_embed = position_embedding[:, 1:]

        dim = embeddings.shape[-1]

        new_height = height // self.patch_size
        new_width = width // self.patch_size

        sqrt_num_positions = torch_int(num_positions**0.5)
        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)
        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)
        patch_pos_embed = nn.functional.interpolate(
            patch_pos_embed,
            size=(new_height, new_width),
            mode="bicubic",
            align_corners=False,
        )
        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)

    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:
        batch_size, _, height, width = pixel_values.shape
        patch_embeds = self.patch_embedding(pixel_values)  # shape = [batch_size, num_channels, height, width]
        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)
        class_embeds = self.class_embedding.expand(batch_size, 1, -1)
        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)
        if interpolate_pos_encoding:
            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)
        else:
            embeddings = embeddings + self.position_embedding(self.position_ids)
        return embeddings


class OwlViTTextEmbeddings(nn.Module):
    def __init__(self, config: OwlViTTextConfig):
        super().__init__()
        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)
        self.position_embedding = nn.Embedding(config.max_position_embeddings, config.hidden_size)

        # position_ids (1, len position emb) is contiguous in memory and exported when serialized
        self.register_buffer(
            "position_ids", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False
        )

    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
    ) -> torch.Tensor:
        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]

        if position_ids is None:
            position_ids = self.position_ids[:, :seq_length]

        if inputs_embeds is None:
            inputs_embeds = self.token_embedding(input_ids)

        position_embeddings = self.position_embedding(position_ids)
        embeddings = inputs_embeds + position_embeddings

        return embeddings


class OwlViTAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.embed_dim = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.embed_dim // self.num_heads
        if self.head_dim * self.num_heads != self.embed_dim:
            raise ValueError(
                f"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:"
                f" {self.num_heads})."
            )
        self.scale = self.head_dim**-0.5
        self.dropout = config.attention_dropout

        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)
        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)
        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)
        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)

    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        causal_attention_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        """Input shape: Batch x Time x Channel"""

        bsz, tgt_len, embed_dim = hidden_states.size()

        # get query proj
        query_states = self.q_proj(hidden_states) * self.scale
        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)

        proj_shape = (bsz * self.num_heads, -1, self.head_dim)
        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)
        key_states = key_states.view(*proj_shape)
        value_states = value_states.view(*proj_shape)

        src_len = key_states.size(1)
        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))

        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is"
                f" {attn_weights.size()}"
            )

        # apply the causal_attention_mask first
        if causal_attention_mask is not None:
            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):
                raise ValueError(
                    f"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is"
                    f" {causal_attention_mask.size()}"
                )
            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)

        if attention_mask is not None:
            if attention_mask.size() != (bsz, 1, tgt_len, src_len):
                raise ValueError(
                    f"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}"
                )
            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)

        attn_weights = nn.functional.softmax(attn_weights, dim=-1)

        if output_attentions:
            # this operation is a bit akward, but it's required to
            # make sure that attn_weights keeps its gradient.
            # In order to do so, attn_weights have to reshaped
            # twice and have to be reused in the following
            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)
        else:
            attn_weights_reshaped = None

        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)

        # For int8 compatibility, sometimes the `attn_probs` are in `fp32`
        attn_probs = attn_probs.to(value_states.dtype)

        attn_output = torch.bmm(attn_probs, value_states)

        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)
        attn_output = attn_output.transpose(1, 2)
        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)

        attn_output = self.out_proj(attn_output)

        return attn_output, attn_weights_reshaped


# Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->OwlViT
class OwlViTMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.activation_fn = ACT2FN[config.hidden_act]
        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)
        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        hidden_states = self.fc1(hidden_states)
        hidden_states = self.activation_fn(hidden_states)
        hidden_states = self.fc2(hidden_states)
        return hidden_states


# Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->OwlViT
class OwlViTEncoderLayer(nn.Module):
    def __init__(self, config: OwlViTConfig):
        super().__init__()
        self.embed_dim = config.hidden_size
        self.self_attn = OwlViTAttention(config)
        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)
        self.mlp = OwlViTMLP(config)
        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor,
        causal_attention_mask: torch.Tensor,
        output_attentions: Optional[bool] = False,
    ) -> Tuple[torch.FloatTensor]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`): attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
                `(config.encoder_attention_heads,)`.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
        """
        residual = hidden_states

        hidden_states = self.layer_norm1(hidden_states)
        hidden_states, attn_weights = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            causal_attention_mask=causal_attention_mask,
            output_attentions=output_attentions,
        )
        hidden_states = residual + hidden_states

        residual = hidden_states
        hidden_states = self.layer_norm2(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (attn_weights,)

        return outputs


@auto_docstring
class OwlViTPreTrainedModel(PreTrainedModel):
    config_class = OwlViTConfig
    base_model_prefix = "owlvit"
    supports_gradient_checkpointing = True
    _no_split_modules = ["OwlViTEncoderLayer"]

    def _init_weights(self, module):
        """Initialize the weights"""
        factor = self.config.initializer_factor
        if isinstance(module, OwlViTTextEmbeddings):
            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)
            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)
        elif isinstance(module, OwlViTVisionEmbeddings):
            factor = self.config.initializer_factor
            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)
            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)
            nn.init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)
        elif isinstance(module, OwlViTAttention):
            factor = self.config.initializer_factor
            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor
            out_proj_std = (module.embed_dim**-0.5) * factor
            nn.init.normal_(module.q_proj.weight, std=in_proj_std)
            nn.init.normal_(module.k_proj.weight, std=in_proj_std)
            nn.init.normal_(module.v_proj.weight, std=in_proj_std)
            nn.init.normal_(module.out_proj.weight, std=out_proj_std)
        elif isinstance(module, OwlViTMLP):
            factor = self.config.initializer_factor
            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor
            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor
            nn.init.normal_(module.fc1.weight, std=fc_std)
            nn.init.normal_(module.fc2.weight, std=in_proj_std)
        elif isinstance(module, OwlViTModel):
            nn.init.normal_(
                module.text_projection.weight,
                std=module.text_embed_dim**-0.5 * self.config.initializer_factor,
            )
            nn.init.normal_(
                module.visual_projection.weight,
                std=module.vision_embed_dim**-0.5 * self.config.initializer_factor,
            )
        if isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        if isinstance(module, nn.Linear) and module.bias is not None:
            module.bias.data.zero_()


class OwlViTEncoder(nn.Module):
    """
    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a
    [`OwlViTEncoderLayer`].

    Args:
        config: OwlViTConfig
    """

    def __init__(self, config: OwlViTConfig):
        super().__init__()
        self.layers = nn.ModuleList([OwlViTEncoderLayer(config) for _ in range(config.num_hidden_layers)])
        self.gradient_checkpointing = False

    def forward(
        self,
        inputs_embeds,
        attention_mask: Optional[torch.Tensor] = None,
        causal_attention_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutput]:
        r"""
        Args:
            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`).
            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
                - 1 for tokens that are **not masked**,
                - 0 for tokens that are **masked**.
                [What are attention masks?](../glossary#attention-mask)
            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
                Causal mask for the text model. Mask values selected in `[0, 1]`:
                - 1 for tokens that are **not masked**,
                - 0 for tokens that are **masked**.
                [What are attention masks?](../glossary#attention-mask)
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            output_hidden_states (`bool`, *optional*):
                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
                for more detail.
            return_dict (`bool`, *optional*):
                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        """
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        encoder_states = () if output_hidden_states else None
        all_attentions = () if output_attentions else None

        hidden_states = inputs_embeds
        for encoder_layer in self.layers:
            if output_hidden_states:
                encoder_states = encoder_states + (hidden_states,)
            if self.gradient_checkpointing and self.training:
                layer_outputs = self._gradient_checkpointing_func(
                    encoder_layer.__call__,
                    hidden_states,
                    attention_mask,
                    causal_attention_mask,
                    output_attentions,
                )
            else:
                layer_outputs = encoder_layer(
                    hidden_states,
                    attention_mask,
                    causal_attention_mask,
                    output_attentions=output_attentions,
                )

            hidden_states = layer_outputs[0]

            if output_attentions:
                all_attentions = all_attentions + (layer_outputs[1],)

        if output_hidden_states:
            encoder_states = encoder_states + (hidden_states,)

        if not return_dict:
            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)
        return BaseModelOutput(
            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions
        )


class OwlViTTextTransformer(nn.Module):
    def __init__(self, config: OwlViTTextConfig):
        super().__init__()
        self.config = config
        embed_dim = config.hidden_size
        self.embeddings = OwlViTTextEmbeddings(config)
        self.encoder = OwlViTEncoder(config)
        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)

    @auto_docstring
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutputWithPooling]:
        r"""
        input_ids (`torch.LongTensor` of shape `(batch_size * num_max_text_queries, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`]. See
            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details. [What are input
            IDs?](../glossary#input-ids)
        """
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        input_shape = input_ids.size()
        input_ids = input_ids.view(-1, input_shape[-1])
        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)

        # num_samples, seq_len = input_shape  where num_samples = batch_size * num_max_text_queries
        # OWLVIT's text model uses causal mask, prepare it here.
        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324
        causal_attention_mask = _create_4d_causal_attention_mask(
            input_shape, hidden_states.dtype, device=hidden_states.device
        )
        # expand attention_mask
        if attention_mask is not None:
            # [num_samples, seq_len] -> [num_samples, 1, tgt_seq_len, src_seq_len]
            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)

        encoder_outputs = self.encoder(
            inputs_embeds=hidden_states,
            attention_mask=attention_mask,
            causal_attention_mask=causal_attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        last_hidden_state = encoder_outputs[0]
        last_hidden_state = self.final_layer_norm(last_hidden_state)

        # take features from the end of tokens embedding (end of token is the highest number in each sequence)
        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14
        pooled_output = last_hidden_state[
            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),
            input_ids.to(torch.int).argmax(dim=-1).to(last_hidden_state.device),
        ]

        if not return_dict:
            return (last_hidden_state, pooled_output) + encoder_outputs[1:]

        return BaseModelOutputWithPooling(
            last_hidden_state=last_hidden_state,
            pooler_output=pooled_output,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        )


class OwlViTTextModel(OwlViTPreTrainedModel):
    config_class = OwlViTTextConfig

    def __init__(self, config: OwlViTTextConfig):
        super().__init__(config)
        self.text_model = OwlViTTextTransformer(config)
        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self) -> nn.Module:
        return self.text_model.embeddings.token_embedding

    def set_input_embeddings(self, value):
        self.text_model.embeddings.token_embedding = value

    @auto_docstring
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutputWithPooling]:
        r"""
        input_ids (`torch.LongTensor` of shape `(batch_size * num_max_text_queries, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`]. See
            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details. [What are input
            IDs?](../glossary#input-ids)

        Examples:
        ```python
        >>> from transformers import AutoProcessor, OwlViTTextModel

        >>> model = OwlViTTextModel.from_pretrained("google/owlvit-base-patch32")
        >>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch32")
        >>> inputs = processor(
        ...     text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
        ... )
        >>> outputs = model(**inputs)
        >>> last_hidden_state = outputs.last_hidden_state
        >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states
        ```"""

        # Get embeddings for all text queries in all batch samples
        return self.text_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )


class OwlViTVisionTransformer(nn.Module):
    def __init__(self, config: OwlViTVisionConfig):
        super().__init__()
        self.config = config

        self.embeddings = OwlViTVisionEmbeddings(config)
        self.pre_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.encoder = OwlViTEncoder(config)
        self.post_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    @auto_docstring
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        interpolate_pos_encoding: Optional[bool] = False,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutputWithPooling]:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # Cast the input to the expected `dtype`
        expected_input_dtype = self.embeddings.patch_embedding.weight.dtype
        pixel_values = pixel_values.to(expected_input_dtype)

        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)
        hidden_states = self.pre_layernorm(hidden_states)

        encoder_outputs = self.encoder(
            inputs_embeds=hidden_states,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        last_hidden_state = encoder_outputs[0]
        pooled_output = last_hidden_state[:, 0, :]

        pooled_output = self.post_layernorm(pooled_output)

        if not return_dict:
            return (last_hidden_state, pooled_output) + encoder_outputs[1:]

        return BaseModelOutputWithPooling(
            last_hidden_state=last_hidden_state,
            pooler_output=pooled_output,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        )


class OwlViTVisionModel(OwlViTPreTrainedModel):
    config_class = OwlViTVisionConfig
    main_input_name = "pixel_values"

    def __init__(self, config: OwlViTVisionConfig):
        super().__init__(config)
        self.vision_model = OwlViTVisionTransformer(config)
        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self) -> nn.Module:
        return self.vision_model.embeddings.patch_embedding

    @auto_docstring
    def forward(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        interpolate_pos_encoding: bool = False,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutputWithPooling]:
        r"""
        Examples:
        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, OwlViTVisionModel

        >>> model = OwlViTVisionModel.from_pretrained("google/owlvit-base-patch32")
        >>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch32")
        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> inputs = processor(images=image, return_tensors="pt")

        >>> outputs = model(**inputs)
        >>> last_hidden_state = outputs.last_hidden_state
        >>> pooled_output = outputs.pooler_output  # pooled CLS states
        ```"""
        return self.vision_model(
            pixel_values=pixel_values,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            interpolate_pos_encoding=interpolate_pos_encoding,
            return_dict=return_dict,
        )


@auto_docstring
class OwlViTModel(OwlViTPreTrainedModel):
    config_class = OwlViTConfig

    def __init__(self, config: OwlViTConfig):
        super().__init__(config)

        if not isinstance(config.text_config, OwlViTTextConfig):
            raise TypeError(
                "config.text_config is expected to be of type OwlViTTextConfig but is of type"
                f" {type(config.text_config)}."
            )

        if not isinstance(config.vision_config, OwlViTVisionConfig):
            raise TypeError(
                "config.vision_config is expected to be of type OwlViTVisionConfig but is of type"
                f" {type(config.vision_config)}."
            )

        text_config = config.text_config
        vision_config = config.vision_config

        self.projection_dim = config.projection_dim
        self.text_embed_dim = text_config.hidden_size
        self.vision_embed_dim = vision_config.hidden_size

        self.text_model = OwlViTTextTransformer(text_config)
        self.vision_model = OwlViTVisionTransformer(vision_config)

        self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)
        self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)
        self.logit_scale = nn.Parameter(torch.tensor(config.logit_scale_init_value))

        # Initialize weights and apply final processing
        self.post_init()

    @auto_docstring
    def get_text_features(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> torch.FloatTensor:
        r"""
        input_ids (`torch.LongTensor` of shape `(batch_size * num_max_text_queries, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`]. See
            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details. [What are input
            IDs?](../glossary#input-ids)

        Returns:
            text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by
            applying the projection layer to the pooled output of [`OwlViTTextModel`].

        Examples:
        ```python
        >>> from transformers import AutoProcessor, OwlViTModel

        >>> model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
        >>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch32")
        >>> inputs = processor(
        ...     text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
        ... )
        >>> text_features = model.get_text_features(**inputs)
        ```"""
        # Use OWL-ViT model's config for some fields (if specified) instead of those of vision & text components.
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # Get embeddings for all text queries in all batch samples
        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask, return_dict=return_dict)
        pooled_output = text_output[1]
        text_features = self.text_projection(pooled_output)

        return text_features

    @auto_docstring
    def get_image_features(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        interpolate_pos_encoding: bool = False,
        return_dict: Optional[bool] = None,
    ) -> torch.FloatTensor:
        r"""
        Returns:
            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by
            applying the projection layer to the pooled output of [`OwlViTVisionModel`].

        Examples:
        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, OwlViTModel

        >>> model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
        >>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch32")
        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)
        >>> inputs = processor(images=image, return_tensors="pt")
        >>> image_features = model.get_image_features(**inputs)
        ```"""
        # Use OWL-ViT model's config for some fields (if specified) instead of those of vision & text components.
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        vision_outputs = self.vision_model(
            pixel_values=pixel_values,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            interpolate_pos_encoding=interpolate_pos_encoding,
            return_dict=return_dict,
        )

        pooled_output = vision_outputs[1]
        image_features = self.visual_projection(pooled_output)

        return image_features

    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        pixel_values: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        return_loss: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        interpolate_pos_encoding: bool = False,
        return_base_image_embeds: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, OwlViTOutput]:
        r"""
        return_loss (`bool`, *optional*):
            Whether or not to return the contrastive loss.
        return_base_image_embeds (`bool`, *optional*):
            Whether or not to return the base image embeddings.

        Examples:
        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, OwlViTModel

        >>> model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
        >>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch32")
        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)
        >>> inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
        >>> outputs = model(**inputs)
        >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
        ```"""
        # Use OWL-ViT model's config for some fields (if specified) instead of those of vision & text components.
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        vision_outputs = self.vision_model(
            pixel_values=pixel_values,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            interpolate_pos_encoding=interpolate_pos_encoding,
            return_dict=return_dict,
        )

        # Get embeddings for all text queries in all batch samples
        text_outputs = self.text_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        text_embeds = text_outputs[1]
        text_embeds = self.text_projection(text_embeds)
        image_embeds = vision_outputs[1]
        image_embeds = self.visual_projection(image_embeds)

        # normalized features
        image_embeds = image_embeds / torch.linalg.norm(image_embeds, ord=2, dim=-1, keepdim=True)
        text_embeds_norm = text_embeds / torch.linalg.norm(text_embeds, ord=2, dim=-1, keepdim=True)

        # cosine similarity as logits and set it on the correct device
        logit_scale = self.logit_scale.exp().to(image_embeds.device)

        logits_per_text = torch.matmul(text_embeds_norm, image_embeds.t()) * logit_scale
        logits_per_image = logits_per_text.t()

        loss = None
        if return_loss:
            loss = owlvit_loss(logits_per_text)

        text_embeds = text_embeds_norm

        if not return_dict:
            output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)
            return ((loss,) + output) if loss is not None else output

        return OwlViTOutput(
            loss=loss,
            logits_per_image=logits_per_image,
            logits_per_text=logits_per_text,
            text_embeds=text_embeds,
            image_embeds=image_embeds,
            text_model_output=text_outputs,
            vision_model_output=vision_outputs,
        )


class OwlViTBoxPredictionHead(nn.Module):
    def __init__(self, config: OwlViTConfig, out_dim: int = 4):
        super().__init__()

        width = config.vision_config.hidden_size
        self.dense0 = nn.Linear(width, width)
        self.dense1 = nn.Linear(width, width)
        self.gelu = nn.GELU()
        self.dense2 = nn.Linear(width, out_dim)

    def forward(self, image_features: torch.Tensor) -> torch.FloatTensor:
        output = self.dense0(image_features)
        output = self.gelu(output)
        output = self.dense1(output)
        output = self.gelu(output)
        output = self.dense2(output)
        return output


class OwlViTClassPredictionHead(nn.Module):
    def __init__(self, config: OwlViTConfig):
        super().__init__()

        out_dim = config.text_config.hidden_size
        self.query_dim = config.vision_config.hidden_size

        self.dense0 = nn.Linear(self.query_dim, out_dim)
        self.logit_shift = nn.Linear(self.query_dim, 1)
        self.logit_scale = nn.Linear(self.query_dim, 1)
        self.elu = nn.ELU()

    def forward(
        self,
        image_embeds: torch.FloatTensor,
        query_embeds: Optional[torch.FloatTensor],
        query_mask: Optional[torch.Tensor],
    ) -> Tuple[torch.FloatTensor]:
        image_class_embeds = self.dense0(image_embeds)
        if query_embeds is None:
            device = image_class_embeds.device
            batch_size, num_patches = image_class_embeds.shape[:2]
            pred_logits = torch.zeros((batch_size, num_patches, self.query_dim)).to(device)
            return (pred_logits, image_class_embeds)

        # Normalize image and text features
        image_class_embeds = image_class_embeds / (torch.linalg.norm(image_class_embeds, dim=-1, keepdim=True) + 1e-6)
        query_embeds = query_embeds / (torch.linalg.norm(query_embeds, dim=-1, keepdim=True) + 1e-6)

        # Get class predictions
        pred_logits = torch.einsum("...pd,...qd->...pq", image_class_embeds, query_embeds)

        # Apply a learnable shift and scale to logits
        logit_shift = self.logit_shift(image_embeds)
        logit_scale = self.logit_scale(image_embeds)
        logit_scale = self.elu(logit_scale) + 1
        pred_logits = (pred_logits + logit_shift) * logit_scale

        if query_mask is not None:
            if query_mask.ndim > 1:
                query_mask = torch.unsqueeze(query_mask, dim=-2)

            pred_logits = torch.where(query_mask == 0, torch.finfo(pred_logits.dtype).min, pred_logits)
            pred_logits = pred_logits.to(torch.float32)

        return (pred_logits, image_class_embeds)


class OwlViTForObjectDetection(OwlViTPreTrainedModel):
    config_class = OwlViTConfig

    def __init__(self, config: OwlViTConfig):
        super().__init__(config)

        self.owlvit = OwlViTModel(config)
        self.class_head = OwlViTClassPredictionHead(config)
        self.box_head = OwlViTBoxPredictionHead(config)

        self.layer_norm = nn.LayerNorm(config.vision_config.hidden_size, eps=config.vision_config.layer_norm_eps)
        self.sigmoid = nn.Sigmoid()
        self.config = config
        self.num_patches_height = self.config.vision_config.image_size // self.config.vision_config.patch_size
        self.num_patches_width = self.config.vision_config.image_size // self.config.vision_config.patch_size
        self.box_bias = self.compute_box_bias(self.num_patches_height, self.num_patches_width)

    @staticmethod
    def normalize_grid_corner_coordinates(num_patches_height: int, num_patches_width: int) -> torch.Tensor:
        # Create grid coordinates using torch
        x_coordinates = torch.arange(1, num_patches_width + 1, dtype=torch.float32)
        y_coordinates = torch.arange(1, num_patches_height + 1, dtype=torch.float32)
        xx, yy = torch.meshgrid(x_coordinates, y_coordinates, indexing="xy")

        # Stack the coordinates and divide by their respective patch counts
        box_coordinates = torch.stack((xx, yy), dim=-1)
        box_coordinates[..., 0] /= num_patches_width
        box_coordinates[..., 1] /= num_patches_height

        # Flatten (h, w, 2) -> (h*w, 2)
        box_coordinates = box_coordinates.view(-1, 2)

        return box_coordinates

    @lru_cache(maxsize=2)
    def compute_box_bias(
        self, num_patches_height: int, num_patches_width: int, feature_map: Optional[torch.FloatTensor] = None
    ) -> torch.Tensor:
        if feature_map is not None:
            raise ValueError("feature_map has been deprecated as an input. Please pass in num_patches instead")
        # The box center is biased to its position on the feature grid
        box_coordinates = self.normalize_grid_corner_coordinates(num_patches_height, num_patches_width)
        box_coordinates = torch.clip(box_coordinates, 0.0, 1.0)

        # Unnormalize xy
        box_coord_bias = torch.log(box_coordinates + 1e-4) - torch.log1p(-box_coordinates + 1e-4)

        # The box size is biased to the patch size
        box_size = torch.full_like(box_coord_bias, 1.0)
        box_size[..., 0] /= num_patches_width
        box_size[..., 1] /= num_patches_height
        box_size_bias = torch.log(box_size + 1e-4) - torch.log1p(-box_size + 1e-4)

        # Compute box bias
        box_bias = torch.cat([box_coord_bias, box_size_bias], dim=-1)
        return box_bias

    def box_predictor(
        self,
        image_feats: torch.FloatTensor,
        feature_map: torch.FloatTensor,
        interpolate_pos_encoding: bool = False,
    ) -> torch.FloatTensor:
        """
        Args:
            image_feats:
                Features extracted from the image, returned by the `image_text_embedder` method.
            feature_map:
                A spatial re-arrangement of image_features, also returned by the `image_text_embedder` method.
            interpolate_pos_encoding:
                Whether to interpolate the pre-trained position encodings.
        Returns:
            pred_boxes:
                List of predicted boxes (cxcywh normalized to 0, 1) nested within a dictionary.
        """
        # Bounding box detection head [batch_size, num_boxes, 4].
        pred_boxes = self.box_head(image_feats)

        # Compute the location of each token on the grid and use it to compute a bias for the bbox prediction
        if interpolate_pos_encoding:
            _, num_patches_height, num_patches_width, _ = feature_map.shape
            box_bias = self.compute_box_bias(num_patches_height, num_patches_width)
        else:
            box_bias = self.box_bias

        box_bias = box_bias.to(feature_map.device)
        pred_boxes += box_bias
        pred_boxes = self.sigmoid(pred_boxes)
        return pred_boxes

    def class_predictor(
        self,
        image_feats: torch.FloatTensor,
        query_embeds: Optional[torch.FloatTensor] = None,
        query_mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.FloatTensor]:
        """
        Args:
            image_feats:
                Features extracted from the `image_text_embedder`.
            query_embeds:
                Text query embeddings.
            query_mask:
                Must be provided with query_embeddings. A mask indicating which query embeddings are valid.
        """
        (pred_logits, image_class_embeds) = self.class_head(image_feats, query_embeds, query_mask)

        return (pred_logits, image_class_embeds)

    def image_text_embedder(
        self,
        input_ids: torch.Tensor,
        pixel_values: torch.FloatTensor,
        attention_mask: torch.Tensor,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        interpolate_pos_encoding: bool = False,
    ) -> Tuple[torch.FloatTensor]:
        # Encode text and image
        outputs = self.owlvit(
            pixel_values=pixel_values,
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            interpolate_pos_encoding=interpolate_pos_encoding,
            return_dict=True,
        )

        if interpolate_pos_encoding:
            _, _, height, width = pixel_values.shape
            num_patches_height = height // self.config.vision_config.patch_size
            num_patches_width = width // self.config.vision_config.patch_size
        else:
            num_patches_height = self.num_patches_height
            num_patches_width = self.num_patches_width

        # Get image embeddings
        last_hidden_state = outputs.vision_model_output[0]
        image_embeds = self.owlvit.vision_model.post_layernorm(last_hidden_state)

        # Resize class token
        class_token_out = torch.broadcast_to(image_embeds[:, :1, :], image_embeds[:, :-1].shape)

        # Merge image embedding with class tokens
        image_embeds = image_embeds[:, 1:, :] * class_token_out
        image_embeds = self.layer_norm(image_embeds)

        # Resize to [batch_size, num_patches_height, num_patches_width, hidden_size]
        new_size = (
            image_embeds.shape[0],
            num_patches_height,
            num_patches_width,
            image_embeds.shape[-1],
        )
        image_embeds = image_embeds.reshape(new_size)
        text_embeds = outputs[-4]

        return (text_embeds, image_embeds, outputs)

    def image_embedder(
        self,
        pixel_values: torch.FloatTensor,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        interpolate_pos_encoding: bool = False,
    ) -> Tuple[torch.FloatTensor]:
        # Get OwlViTModel vision embeddings (same as CLIP)
        vision_outputs = self.owlvit.vision_model(
            pixel_values=pixel_values, interpolate_pos_encoding=interpolate_pos_encoding, return_dict=True
        )

        if interpolate_pos_encoding:
            _, _, height, width = pixel_values.shape
            num_patches_height = height // self.config.vision_config.patch_size
            num_patches_width = width // self.config.vision_config.patch_size
        else:
            num_patches_height = self.num_patches_height
            num_patches_width = self.num_patches_width

        # Apply post_layernorm to last_hidden_state, return non-projected output
        last_hidden_state = vision_outputs[0]
        image_embeds = self.owlvit.vision_model.post_layernorm(last_hidden_state)

        # Resize class token
        class_token_out = torch.broadcast_to(image_embeds[:, :1, :], image_embeds[:, :-1].shape)

        # Merge image embedding with class tokens
        image_embeds = image_embeds[:, 1:, :] * class_token_out
        image_embeds = self.layer_norm(image_embeds)

        # Resize to [batch_size, num_patches_height, num_patches_width, hidden_size]
        new_size = (
            image_embeds.shape[0],
            num_patches_height,
            num_patches_width,
            image_embeds.shape[-1],
        )
        image_embeds = image_embeds.reshape(new_size)

        return (image_embeds, vision_outputs)

    def embed_image_query(
        self,
        query_image_features: torch.FloatTensor,
        query_feature_map: torch.FloatTensor,
        interpolate_pos_encoding: bool = False,
    ) -> torch.FloatTensor:
        _, class_embeds = self.class_predictor(query_image_features)
        pred_boxes = self.box_predictor(query_image_features, query_feature_map, interpolate_pos_encoding)
        pred_boxes_as_corners = center_to_corners_format(pred_boxes)

        # Loop over query images
        best_class_embeds = []
        best_box_indices = []
        pred_boxes_device = pred_boxes_as_corners.device

        for i in range(query_image_features.shape[0]):
            each_query_box = torch.tensor([[0, 0, 1, 1]], device=pred_boxes_device)
            each_query_pred_boxes = pred_boxes_as_corners[i]
            ious, _ = box_iou(each_query_box, each_query_pred_boxes)

            # If there are no overlapping boxes, fall back to generalized IoU
            if torch.all(ious[0] == 0.0):
                ious = generalized_box_iou(each_query_box, each_query_pred_boxes)

            # Use an adaptive threshold to include all boxes within 80% of the best IoU
            iou_threshold = torch.max(ious) * 0.8

            selected_inds = (ious[0] >= iou_threshold).nonzero()
            if selected_inds.numel():
                selected_embeddings = class_embeds[i][selected_inds.squeeze(1)]
                mean_embeds = torch.mean(class_embeds[i], axis=0)
                mean_sim = torch.einsum("d,id->i", mean_embeds, selected_embeddings)
                best_box_ind = selected_inds[torch.argmin(mean_sim)]
                best_class_embeds.append(class_embeds[i][best_box_ind])
                best_box_indices.append(best_box_ind)

        if best_class_embeds:
            query_embeds = torch.stack(best_class_embeds)
            box_indices = torch.stack(best_box_indices)
        else:
            query_embeds, box_indices = None, None

        return query_embeds, box_indices, pred_boxes

    @auto_docstring
    def image_guided_detection(
        self,
        pixel_values: torch.FloatTensor,
        query_pixel_values: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        interpolate_pos_encoding: bool = False,
        return_dict: Optional[bool] = None,
    ) -> OwlViTImageGuidedObjectDetectionOutput:
        r"""
        query_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):
            Pixel values of query image(s) to be detected. Pass in one query image per target image.

        Examples:
        ```python
        >>> import requests
        >>> from PIL import Image
        >>> import torch
        >>> from transformers import AutoProcessor, OwlViTForObjectDetection

        >>> processor = AutoProcessor.from_pretrained("google/owlvit-base-patch16")
        >>> model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch16")
        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)
        >>> query_url = "http://images.cocodataset.org/val2017/000000001675.jpg"
        >>> query_image = Image.open(requests.get(query_url, stream=True).raw)
        >>> inputs = processor(images=image, query_images=query_image, return_tensors="pt")
        >>> with torch.no_grad():
        ...     outputs = model.image_guided_detection(**inputs)
        >>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]
        >>> target_sizes = torch.Tensor([image.size[::-1]])
        >>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
        >>> results = processor.post_process_image_guided_detection(
        ...     outputs=outputs, threshold=0.6, nms_threshold=0.3, target_sizes=target_sizes
        ... )
        >>> i = 0  # Retrieve predictions for the first image
        >>> boxes, scores = results[i]["boxes"], results[i]["scores"]
        >>> for box, score in zip(boxes, scores):
        ...     box = [round(i, 2) for i in box.tolist()]
        ...     print(f"Detected similar object with confidence {round(score.item(), 3)} at location {box}")
        Detected similar object with confidence 0.856 at location [10.94, 50.4, 315.8, 471.39]
        Detected similar object with confidence 1.0 at location [334.84, 25.33, 636.16, 374.71]
        ```"""
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.return_dict

        # Compute feature maps for the input and query images
        query_feature_map = self.image_embedder(
            pixel_values=query_pixel_values, interpolate_pos_encoding=interpolate_pos_encoding
        )[0]
        feature_map, vision_outputs = self.image_embedder(
            pixel_values=pixel_values,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            interpolate_pos_encoding=interpolate_pos_encoding,
        )

        batch_size, num_patches_height, num_patches_width, hidden_dim = feature_map.shape
        image_feats = torch.reshape(feature_map, (batch_size, num_patches_height * num_patches_width, hidden_dim))

        batch_size, num_patches_height, num_patches_width, hidden_dim = query_feature_map.shape
        query_image_feats = torch.reshape(
            query_feature_map, (batch_size, num_patches_height * num_patches_width, hidden_dim)
        )
        # Get top class embedding and best box index for each query image in batch
        query_embeds, best_box_indices, query_pred_boxes = self.embed_image_query(
            query_image_feats, query_feature_map, interpolate_pos_encoding
        )

        # Predict object classes [batch_size, num_patches, num_queries+1]
        (pred_logits, class_embeds) = self.class_predictor(image_feats=image_feats, query_embeds=query_embeds)

        # Predict object boxes
        target_pred_boxes = self.box_predictor(image_feats, feature_map, interpolate_pos_encoding)

        if not return_dict:
            output = (
                feature_map,
                query_feature_map,
                target_pred_boxes,
                query_pred_boxes,
                pred_logits,
                class_embeds,
                vision_outputs.to_tuple(),
            )
            output = tuple(x for x in output if x is not None)
            return output

        return OwlViTImageGuidedObjectDetectionOutput(
            image_embeds=feature_map,
            query_image_embeds=query_feature_map,
            target_pred_boxes=target_pred_boxes,
            query_pred_boxes=query_pred_boxes,
            logits=pred_logits,
            class_embeds=class_embeds,
            text_model_output=None,
            vision_model_output=vision_outputs,
        )

    @auto_docstring
    def forward(
        self,
        input_ids: torch.Tensor,
        pixel_values: torch.FloatTensor,
        attention_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        interpolate_pos_encoding: bool = False,
        return_dict: Optional[bool] = None,
    ) -> OwlViTObjectDetectionOutput:
        r"""
        input_ids (`torch.LongTensor` of shape `(batch_size * num_max_text_queries, sequence_length)`, *optional*):
            Indices of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`]. See
            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details. [What are input
            IDs?](../glossary#input-ids).
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the last hidden state. See `text_model_last_hidden_state` and
            `vision_model_last_hidden_state` under returned tensors for more detail.

        Examples:
        ```python
        >>> import requests
        >>> from PIL import Image
        >>> import torch

        >>> from transformers import OwlViTProcessor, OwlViTForObjectDetection

        >>> processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
        >>> model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)
        >>> text_labels = [["a photo of a cat", "a photo of a dog"]]
        >>> inputs = processor(text=text_labels, images=image, return_tensors="pt")
        >>> outputs = model(**inputs)

        >>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]
        >>> target_sizes = torch.tensor([(image.height, image.width)])
        >>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
        >>> results = processor.post_process_grounded_object_detection(
        ...     outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels
        ... )
        >>> # Retrieve predictions for the first image for the corresponding text queries
        >>> result = results[0]
        >>> boxes, scores, text_labels = result["boxes"], result["scores"], result["text_labels"]
        >>> for box, score, text_label in zip(boxes, scores, text_labels):
        ...     box = [round(i, 2) for i in box.tolist()]
        ...     print(f"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}")
        Detected a photo of a cat with confidence 0.707 at location [324.97, 20.44, 640.58, 373.29]
        Detected a photo of a cat with confidence 0.717 at location [1.46, 55.26, 315.55, 472.17]
        ```"""
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.return_dict

        # Embed images and text queries
        query_embeds, feature_map, outputs = self.image_text_embedder(
            input_ids=input_ids,
            pixel_values=pixel_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            interpolate_pos_encoding=interpolate_pos_encoding,
        )

        # Text and vision model outputs
        text_outputs = outputs.text_model_output
        vision_outputs = outputs.vision_model_output

        batch_size, num_patches_height, num_patches_width, hidden_dim = feature_map.shape
        image_feats = torch.reshape(feature_map, (batch_size, num_patches_height * num_patches_width, hidden_dim))

        # Reshape from [batch_size * max_text_queries, hidden_dim] -> [batch_size, max_text_queries, hidden_dim]
        max_text_queries = input_ids.shape[0] // batch_size
        query_embeds = query_embeds.reshape(batch_size, max_text_queries, query_embeds.shape[-1])

        # If first token is 0, then this is a padded query [batch_size, num_queries].
        input_ids = input_ids.reshape(batch_size, max_text_queries, input_ids.shape[-1])
        query_mask = input_ids[..., 0] > 0

        # Predict object classes [batch_size, num_patches, num_queries+1]
        (pred_logits, class_embeds) = self.class_predictor(image_feats, query_embeds, query_mask)

        # Predict object boxes
        pred_boxes = self.box_predictor(image_feats, feature_map, interpolate_pos_encoding)

        if not return_dict:
            output = (
                pred_logits,
                pred_boxes,
                query_embeds,
                feature_map,
                class_embeds,
                text_outputs.to_tuple(),
                vision_outputs.to_tuple(),
            )
            output = tuple(x for x in output if x is not None)
            return output

        return OwlViTObjectDetectionOutput(
            image_embeds=feature_map,
            text_embeds=query_embeds,
            pred_boxes=pred_boxes,
            logits=pred_logits,
            class_embeds=class_embeds,
            text_model_output=text_outputs,
            vision_model_output=vision_outputs,
        )


__all__ = ["OwlViTModel", "OwlViTPreTrainedModel", "OwlViTTextModel", "OwlViTVisionModel", "OwlViTForObjectDetection"]
 e   d   s       i   s       N   o   n   e   :   
                                                   i   n   p   u   t   s   _   e   m   b   e   d   s       =       s   e   l   f   .   e   m   b   e   d   _   t   o   k   e   n   s   (   i   n   p   u   t   _   i   d   s   )   
   
                                   i   f       u   s   e   _   c   a   c   h   e       a   n   d       p   a   s   t   _   k   e   y   _   v   a   l   u   e   s       i   s       N   o   n   e   :   
                                                   p   a   s   t   _   k   e   y   _   v   a   l   u   e   s       =       D   y   n   a   m   i   c   C   a   c   h   e   (   )   
   
                                   i   f       c   a   c   h   e   _   p   o   s   i   t   i   o   n       i   s       N   o   n   e   :   
                                                   p   a   s   t   _   s   e   e   n   _   t   o   k   e   n   s       =       p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   .   g   e   t   _   s   e   q   _   l   e   n   g   t   h   (   )       i   f       p   a   s   t   _   k   e   y   _   v   a   l   u   e   s       i   s       n   o   t       N   o   n   e       e   l   s   e       0   
                                                   c   a   c   h   e   _   p   o   s   i   t   i   o   n       =       t   o   r   c   h   .   a   r   a   n   g   e   (   
                                                                   p   a   s   t   _   s   e   e   n   _   t   o   k   e   n   s   ,       p   a   s   t   _   s   e   e   n   _   t   o   k   e   n   s       +       i   n   p   u   t   s   _   e   m   b   e   d   s   .   s   h   a   p   e   [   1   ]   ,       d   e   v   i   c   e   =   i   n   p   u   t   s   _   e   m   b   e   d   s   .   d   e   v   i   c   e   
                                                   )   
   
                                   i   f       p   o   s   i   t   i   o   n   _   i   d   s       i   s       N   o   n   e   :   
                                                   p   o   s   i   t   i   o   n   _   i   d   s       =       c   a   c   h   e   _   p   o   s   i   t   i   o   n   .   u   n   s   q   u   e   e   z   e   (   0   )   
   
                                   c   a   u   s   a   l   _   m   a   s   k       =       s   e   l   f   .   _   u   p   d   a   t   e   _   c   a   u   s   a   l   _   m   a   s   k   (   
                                                   a   t   t   e   n   t   i   o   n   _   m   a   s   k   ,       i   n   p   u   t   s   _   e   m   b   e   d   s   ,       c   a   c   h   e   _   p   o   s   i   t   i   o   n   ,       p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   ,       o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s   
                                   )   
   
                                   h   i   d   d   e   n   _   s   t   a   t   e   s       =       i   n   p   u   t   s   _   e   m   b   e   d   s   
   
                                   #       c   r   e   a   t   e       p   o   s   i   t   i   o   n       e   m   b   e   d   d   i   n   g   s       t   o       b   e       s   h   a   r   e   d       a   c   r   o   s   s       t   h   e       d   e   c   o   d   e   r       l   a   y   e   r   s   
                                   p   o   s   i   t   i   o   n   _   e   m   b   e   d   d   i   n   g   s       =       s   e   l   f   .   r   o   t   a   r   y   _   e   m   b   (   h   i   d   d   e   n   _   s   t   a   t   e   s   ,       p   o   s   i   t   i   o   n   _   i   d   s   )   
   
                                   #       d   e   c   o   d   e   r       l   a   y   e   r   s   
                                   a   l   l   _   h   i   d   d   e   n   _   s   t   a   t   e   s       =       (   )       i   f       o   u   t   p   u   t   _   h   i   d   d   e   n   _   s   t   a   t   e   s       e   l   s   e       N   o   n   e   
                                   a   l   l   _   s   e   l   f   _   a   t   t   n   s       =       (   )       i   f       o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s       e   l   s   e       N   o   n   e   
   
                                   f   o   r       d   e   c   o   d   e   r   _   l   a   y   e   r       i   n       s   e   l   f   .   l   a   y   e   r   s   [   :       s   e   l   f   .   c   o   n   f   i   g   .   n   u   m   _   h   i   d   d   e   n   _   l   a   y   e   r   s   ]   :   
                                                   i   f       o   u   t   p   u   t   _   h   i   d   d   e   n   _   s   t   a   t   e   s   :   
                                                                   a   l   l   _   h   i   d   d   e   n   _   s   t   a   t   e   s       +   =       (   h   i   d   d   e   n   _   s   t   a   t   e   s   ,   )   
   
                                                   l   a   y   e   r   _   o   u   t   p   u   t   s       =       d   e   c   o   d   e   r   _   l   a   y   e   r   (   
                                                                   h   i   d   d   e   n   _   s   t   a   t   e   s   ,   
                                                                   a   t   t   e   n   t   i   o   n   _   m   a   s   k   =   c   a   u   s   a   l   _   m   a   s   k   ,   
                                                                   p   o   s   i   t   i   o   n   _   i   d   s   =   p   o   s   i   t   i   o   n   _   i   d   s   ,   
                                                                   p   a   s   t   _   k   e   y   _   v   a   l   u   e   =   p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   ,   
                                                                   o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s   =   o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s   ,   
                                                                   u   s   e   _   c   a   c   h   e   =   u   s   e   _   c   a   c   h   e   ,   
                                                                   c   a   c   h   e   _   p   o   s   i   t   i   o   n   =   c   a   c   h   e   _   p   o   s   i   t   i   o   n   ,   
                                                                   p   o   s   i   t   i   o   n   _   e   m   b   e   d   d   i   n   g   s   =   p   o   s   i   t   i   o   n   _   e   m   b   e   d   d   i   n   g   s   ,   
                                                                   *   *   f   l   a   s   h   _   a   t   t   n   _   k   w   a   r   g   s   ,   
                                                   )   
   
                                                   h   i   d   d   e   n   _   s   t   a   t   e   s       =       l   a   y   e   r   _   o   u   t   p   u   t   s   [   0   ]   
   
                                                   i   f       o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s   :   
                                                                   a   l   l   _   s   e   l   f   _   a   t   t   n   s       +   =       (   l   a   y   e   r   _   o   u   t   p   u   t   s   [   1   ]   ,   )   
   
                                   h   i   d   d   e   n   _   s   t   a   t   e   s       =       s   e   l   f   .   n   o   r   m   (   h   i   d   d   e   n   _   s   t   a   t   e   s   )   
   
                                   #       a   d   d       h   i   d   d   e   n       s   t   a   t   e   s       f   r   o   m       t   h   e       l   a   s   t       d   e   c   o   d   e   r       l   a   y   e   r   
                                   i   f       o   u   t   p   u   t   _   h   i   d   d   e   n   _   s   t   a   t   e   s   :   
                                                   a   l   l   _   h   i   d   d   e   n   _   s   t   a   t   e   s       +   =       (   h   i   d   d   e   n   _   s   t   a   t   e   s   ,   )   
   
                                   r   e   t   u   r   n       B   a   s   e   M   o   d   e   l   O   u   t   p   u   t   W   i   t   h   P   a   s   t   (   
                                                   l   a   s   t   _   h   i   d   d   e   n   _   s   t   a   t   e   =   h   i   d   d   e   n   _   s   t   a   t   e   s   ,   
                                                   p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   =   p   a   s   t   _   k   e   y   _   v   a   l   u   e   s       i   f       u   s   e   _   c   a   c   h   e       e   l   s   e       N   o   n   e   ,   
                                                   h   i   d   d   e   n   _   s   t   a   t   e   s   =   a   l   l   _   h   i   d   d   e   n   _   s   t   a   t   e   s   ,   
                                                   a   t   t   e   n   t   i   o   n   s   =   a   l   l   _   s   e   l   f   _   a   t   t   n   s   ,   
                                   )   
   
                   d   e   f       _   u   p   d   a   t   e   _   c   a   u   s   a   l   _   m   a   s   k   (   
                                   s   e   l   f   ,   
                                   a   t   t   e   n   t   i   o   n   _   m   a   s   k   :       U   n   i   o   n   [   t   o   r   c   h   .   T   e   n   s   o   r   ,       "   B   l   o   c   k   M   a   s   k   "   ]   ,   
                                   i   n   p   u   t   _   t   e   n   s   o   r   :       t   o   r   c   h   .   T   e   n   s   o   r   ,   
                                   c   a   c   h   e   _   p   o   s   i   t   i   o   n   :       t   o   r   c   h   .   T   e   n   s   o   r   ,   
                                   p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   :       C   a   c   h   e   ,   
                                   o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s   :       b   o   o   l       =       F   a   l   s   e   ,   
                   )   :   
                                   i   f       s   e   l   f   .   c   o   n   f   i   g   .   _   a   t   t   n   _   i   m   p   l   e   m   e   n   t   a   t   i   o   n       =   =       "   f   l   a   s   h   _   a   t   t   e   n   t   i   o   n   _   2   "   :   
                                                   i   f       a   t   t   e   n   t   i   o   n   _   m   a   s   k       i   s       n   o   t       N   o   n   e       a   n   d       (   a   t   t   e   n   t   i   o   n   _   m   a   s   k       =   =       0   .   0   )   .   a   n   y   (   )   :   
                                                                   r   e   t   u   r   n       a   t   t   e   n   t   i   o   n   _   m   a   s   k   
                                                   r   e   t   u   r   n       N   o   n   e   
                                   i   f       s   e   l   f   .   c   o   n   f   i   g   .   _   a   t   t   n   _   i   m   p   l   e   m   e   n   t   a   t   i   o   n       =   =       "   f   l   e   x   _   a   t   t   e   n   t   i   o   n   "   :   
                                                   i   f       i   s   i   n   s   t   a   n   c   e   (   a   t   t   e   n   t   i   o   n   _   m   a   s   k   ,       t   o   r   c   h   .   T   e   n   s   o   r   )   :   
                                                                   a   t   t   e   n   t   i   o   n   _   m   a   s   k       =       m   a   k   e   _   f   l   e   x   _   b   l   o   c   k   _   c   a   u   s   a   l   _   m   a   s   k   (   a   t   t   e   n   t   i   o   n   _   m   a   s   k   )   
                                                   r   e   t   u   r   n       a   t   t   e   n   t   i   o   n   _   m   a   s   k   
   
                                   #       F   o   r       S   D   P   A   ,       w   h   e   n       p   o   s   s   i   b   l   e   ,       w   e       w   i   l   l       r   e   l   y       o   n       i   t   s       `   i   s   _   c   a   u   s   a   l   `       a   r   g   u   m   e   n   t       i   n   s   t   e   a   d       o   f       i   t   s       `   a   t   t   n   _   m   a   s   k   `       a   r   g   u   m   e   n   t   ,       i   n   
                                   #       o   r   d   e   r       t   o       d   i   s   p   a   t   c   h       o   n       F   l   a   s   h       A   t   t   e   n   t   i   o   n       2   .       T   h   i   s       f   e   a   t   u   r   e       i   s       n   o   t       c   o   m   p   a   t   i   b   l   e       w   i   t   h       s   t   a   t   i   c       c   a   c   h   e   ,       a   s       S   D   P   A       w   i   l   l       f   a   i   l   
                                   #       t   o       i   n   f   e   r       t   h   e       a   t   t   e   n   t   i   o   n       m   a   s   k   .   
                                   p   a   s   t   _   s   e   e   n   _   t   o   k   e   n   s       =       p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   .   g   e   t   _   s   e   q   _   l   e   n   g   t   h   (   )       i   f       p   a   s   t   _   k   e   y   _   v   a   l   u   e   s       i   s       n   o   t       N   o   n   e       e   l   s   e       0   
                                   u   s   i   n   g   _   c   o   m   p   i   l   a   b   l   e   _   c   a   c   h   e       =       p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   .   i   s   _   c   o   m   p   i   l   e   a   b   l   e       i   f       p   a   s   t   _   k   e   y   _   v   a   l   u   e   s       i   s       n   o   t       N   o   n   e       e   l   s   e       F   a   l   s   e   
   
                                   #       W   h   e   n       o   u   t   p   u   t       a   t   t   e   n   t   i   o   n   s       i   s       T   r   u   e   ,       s   d   p   a       i   m   p   l   e   m   e   n   t   a   t   i   o   n   '   s       f   o   r   w   a   r   d       m   e   t   h   o   d       c   a   l   l   s       t   h   e       e   a   g   e   r       i   m   p   l   e   m   e   n   t   a   t   i   o   n   '   s       f   o   r   w   a   r   d   
                                   i   f       s   e   l   f   .   c   o   n   f   i   g   .   _   a   t   t   n   _   i   m   p   l   e   m   e   n   t   a   t   i   o   n       =   =       "   s   d   p   a   "       a   n   d       n   o   t       u   s   i   n   g   _   c   o   m   p   i   l   a   b   l   e   _   c   a   c   h   e       a   n   d       n   o   t       o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s   :   
                                                   i   f       A   t   t   e   n   t   i   o   n   M   a   s   k   C   o   n   v   e   r   t   e   r   .   _   i   g   n   o   r   e   _   c   a   u   s   a   l   _   m   a   s   k   _   s   d   p   a   (   
                                                                   a   t   t   e   n   t   i   o   n   _   m   a   s   k   ,   
                                                                   i   n   p   u   t   s   _   e   m   b   e   d   s   =   i   n   p   u   t   _   t   e   n   s   o   r   ,   
                                                                   p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   _   l   e   n   g   t   h   =   p   a   s   t   _   s   e   e   n   _   t   o   k   e   n   s   ,   
                                                                   i   s   _   t   r   a   i   n   i   n   g   =   s   e   l   f   .   t   r   a   i   n   i   n   g   ,   
                                                   )   :   
                                                                   r   e   t   u   r   n       N   o   n   e   
   
                                   d   t   y   p   e       =       i   n   p   u   t   _   t   e   n   s   o   r   .   d   t   y   p   e   
                                   s   e   q   u   e   n   c   e   _   l   e   n   g   t   h       =       i   n   p   u   t   _   t   e   n   s   o   r   .   s   h   a   p   e   [   1   ]   
                                   i   f       u   s   i   n   g   _   c   o   m   p   i   l   a   b   l   e   _   c   a   c   h   e   :   
                                                   t   a   r   g   e   t   _   l   e   n   g   t   h       =       p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   .   g   e   t   _   m   a   x   _   c   a   c   h   e   _   s   h   a   p   e   (   )   
                                   e   l   s   e   :   
                                                   t   a   r   g   e   t   _   l   e   n   g   t   h       =       (   
                                                                   a   t   t   e   n   t   i   o   n   _   m   a   s   k   .   s   h   a   p   e   [   -   1   ]   
                                                                   i   f       i   s   i   n   s   t   a   n   c   e   (   a   t   t   e   n   t   i   o   n   _   m   a   s   k   ,       t   o   r   c   h   .   T   e   n   s   o   r   )   
                                                                   e   l   s   e       p   a   s   t   _   s   e   e   n   _   t   o   k   e   n   s       +       s   e   q   u   e   n   c   e   _   l   e   n   g   t   h       +       1   
                                                   )   
   
                                   #       I   n       c   a   s   e       t   h   e       p   r   o   v   i   d   e   d       `   a   t   t   e   n   t   i   o   n   `       m   a   s   k       i   s       2   D   ,       w   e       g   e   n   e   r   a   t   e       a       c   a   u   s   a   l       m   a   s   k       h   e   r   e       (   4   D   )   .   
                                   c   a   u   s   a   l   _   m   a   s   k       =       s   e   l   f   .   _   p   r   e   p   a   r   e   _   4   d   _   c   a   u   s   a   l   _   a   t   t   e   n   t   i   o   n   _   m   a   s   k   _   w   i   t   h   _   c   a   c   h   e   _   p   o   s   i   t   i   o   n   (   
                                                   a   t   t   e   n   t   i   o   n   _   m   a   s   k   ,   
                                                   s   e   q   u   e   n   c   e   _   l   e   n   g   t   h   =   s   e   q   u   e   n   c   e   _   l   e   n   g   t   h   ,   
                                                   t   a   r   g   e   t   _   l   e   n   g   t   h   =   t   a   r   g   e   t   _   l   e   n   g   t   h   ,   
                                                   d   t   y   p   e   =   d   t   y   p   e   ,   
                                                   c   a   c   h   e   _   p   o   s   i   t   i   o   n   =   c   a   c   h   e   _   p   o   s   i   t   i   o   n   ,   
                                                   b   a   t   c   h   _   s   i   z   e   =   i   n   p   u   t   _   t   e   n   s   o   r   .   s   h   a   p   e   [   0   ]   ,   
                                   )   
   
                                   i   f       (   
                                                   s   e   l   f   .   c   o   n   f   i   g   .   _   a   t   t   n   _   i   m   p   l   e   m   e   n   t   a   t   i   o   n       =   =       "   s   d   p   a   "   
                                                   a   n   d       a   t   t   e   n   t   i   o   n   _   m   a   s   k       i   s       n   o   t       N   o   n   e   
                                                   a   n   d       a   t   t   e   n   t   i   o   n   _   m   a   s   k   .   d   e   v   i   c   e   .   t   y   p   e       i   n       [   "   c   u   d   a   "   ,       "   x   p   u   "   ,       "   n   p   u   "   ]   
                                                   a   n   d       n   o   t       o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s   
                                   )   :   
                                                   #       A   t   t   e   n   d       t   o       a   l   l       t   o   k   e   n   s       i   n       f   u   l   l   y       m   a   s   k   e   d       r   o   w   s       i   n       t   h   e       c   a   u   s   a   l   _   m   a   s   k   ,       f   o   r       e   x   a   m   p   l   e       t   h   e       r   e   l   e   v   a   n   t       f   i   r   s   t       r   o   w   s       w   h   e   n   
                                                   #       u   s   i   n   g       l   e   f   t       p   a   d   d   i   n   g   .       T   h   i   s       i   s       r   e   q   u   i   r   e   d       b   y       F   .   s   c   a   l   e   d   _   d   o   t   _   p   r   o   d   u   c   t   _   a   t   t   e   n   t   i   o   n       m   e   m   o   r   y   -   e   f   f   i   c   i   e   n   t       a   t   t   e   n   t   i   o   n       p   a   t   h   .   
                                                   #       D   e   t   a   i   l   s   :       h   t   t   p   s   :   /   /   g   i   t   h   u   b   .   c   o   m   /   p   y   t   o   r   c   h   /   p   y   t   o   r   c   h   /   i   s   s   u   e   s   /   1   1   0   2   1   3   
                                                   m   i   n   _   d   t   y   p   e       =       t   o   r   c   h   .   f   i   n   f   o   (   d   t   y   p   e   )   .   m   i   n   
                                                   c   a   u   s   a   l   _   m   a   s   k       =       A   t   t   e   n   t   i   o   n   M   a   s   k   C   o   n   v   e   r   t   e   r   .   _   u   n   m   a   s   k   _   u   n   a   t   t   e   n   d   e   d   (   c   a   u   s   a   l   _   m   a   s   k   ,       m   i   n   _   d   t   y   p   e   )   
   
                                   r   e   t   u   r   n       c   a   u   s   a   l   _   m   a   s   k   
   
                   @   s   t   a   t   i   c   m   e   t   h   o   d   
                   d   e   f       _   p   r   e   p   a   r   e   _   4   d   _   c   a   u   s   a   l   _   a   t   t   e   n   t   i   o   n   _   m   a   s   k   _   w   i   t   h   _   c   a   c   h   e   _   p   o   s   i   t   i   o   n   (   
                                   a   t   t   e   n   t   i   o   n   _   m   a   s   k   :       t   o   r   c   h   .   T   e   n   s   o   r   ,   
                                   s   e   q   u   e   n   c   e   _   l   e   n   g   t   h   :       i   n   t   ,   
                                   t   a   r   g   e   t   _   l   e   n   g   t   h   :       i   n   t   ,   
                                   d   t   y   p   e   :       t   o   r   c   h   .   d   t   y   p   e   ,   
                                   c   a   c   h   e   _   p   o   s   i   t   i   o   n   :       t   o   r   c   h   .   T   e   n   s   o   r   ,   
                                   b   a   t   c   h   _   s   i   z   e   :       i   n   t   ,   
                                   *   *   k   w   a   r   g   s   ,   
                   )   :   
                                   "   "   "   
                                   C   r   e   a   t   e   s       a       c   a   u   s   a   l       4   D       m   a   s   k       o   f       s   h   a   p   e       `   (   b   a   t   c   h   _   s   i   z   e   ,       1   ,       q   u   e   r   y   _   l   e   n   g   t   h   ,       k   e   y   _   v   a   l   u   e   _   l   e   n   g   t   h   )   `       f   r   o   m       a       2   D       m   a   s   k       o   f       s   h   a   p   e   
                                   `   (   b   a   t   c   h   _   s   i   z   e   ,       k   e   y   _   v   a   l   u   e   _   l   e   n   g   t   h   )   `   ,       o   r       i   f       t   h   e       i   n   p   u   t       `   a   t   t   e   n   t   i   o   n   _   m   a   s   k   `       i   s       a   l   r   e   a   d   y       4   D   ,       d   o       n   o   t   h   i   n   g   .   
   
                                   A   r   g   s   :   
                                                   a   t   t   e   n   t   i   o   n   _   m   a   s   k       (   `   t   o   r   c   h   .   T   e   n   s   o   r   `   )   :   
                                                                   A       2   D       a   t   t   e   n   t   i   o   n       m   a   s   k       o   f       s   h   a   p   e       `   (   b   a   t   c   h   _   s   i   z   e   ,       k   e   y   _   v   a   l   u   e   _   l   e   n   g   t   h   )   `       o   r       a       4   D       a   t   t   e   n   t   i   o   n       m   a   s   k       o   f       s   h   a   p   e   
                                                                   `   (   b   a   t   c   h   _   s   i   z   e   ,       1   ,       q   u   e   r   y   _   l   e   n   g   t   h   ,       k   e   y   _   v   a   l   u   e   _   l   e   n   g   t   h   )   `   .   
                                                   s   e   q   u   e   n   c   e   _   l   e   n   g   t   h       (   `   i   n   t   `   )   :   
                                                                   T   h   e       s   e   q   u   e   n   c   e       l   e   n   g   t   h       b   e   i   n   g       p   r   o   c   e   s   s   e   d   .   
                                                   t   a   r   g   e   t   _   l   e   n   g   t   h       (   `   i   n   t   `   )   :   
                                                                   T   h   e       t   a   r   g   e   t       l   e   n   g   t   h   :       w   h   e   n       g   e   n   e   r   a   t   i   n   g       w   i   t   h       s   t   a   t   i   c       c   a   c   h   e   ,       t   h   e       m   a   s   k       s   h   o   u   l   d       b   e       a   s       l   o   n   g       a   s       t   h   e       s   t   a   t   i   c       c   a   c   h   e   ,   
                                                                   t   o       a   c   c   o   u   n   t       f   o   r       t   h   e       0       p   a   d   d   i   n   g   ,       t   h   e       p   a   r   t       o   f       t   h   e       c   a   c   h   e       t   h   a   t       i   s       n   o   t       f   i   l   l   e   d       y   e   t   .   
                                                   d   t   y   p   e       (   `   t   o   r   c   h   .   d   t   y   p   e   `   )   :   
                                                                   T   h   e       d   t   y   p   e       t   o       u   s   e       f   o   r       t   h   e       4   D       a   t   t   e   n   t   i   o   n       m   a   s   k   .   
                                                   c   a   c   h   e   _   p   o   s   i   t   i   o   n       (   `   t   o   r   c   h   .   T   e   n   s   o   r   `   )   :   
                                                                   I   n   d   i   c   e   s       d   e   p   i   c   t   i   n   g       t   h   e       p   o   s   i   t   i   o   n       o   f       t   h   e       i   n   p   u   t       s   e   q   u   e   n   c   e       t   o   k   e   n   s       i   n       t   h   e       s   e   q   u   e   n   c   e   .   
                                                   b   a   t   c   h   _   s   i   z   e       (   `   t   o   r   c   h   .   T   e   n   s   o   r   `   )   :   
                                                                   B   a   t   c   h       s   i   z   e   .   
                                   "   "   "   
                                   i   f       a   t   t   e   n   t   i   o   n   _   m   a   s   k       i   s       n   o   t       N   o   n   e       a   n   d       a   t   t   e   n   t   i   o   n   _   m   a   s   k   .   d   i   m   (   )       =   =       4   :   
                                                   #       I   n       t   h   i   s       c   a   s   e       w   e       a   s   s   u   m   e       t   h   a   t       t   h   e       m   a   s   k       c   o   m   e   s       a   l   r   e   a   d   y       i   n       i   n   v   e   r   t   e   d       f   o   r   m       a   n   d       r   e   q   u   i   r   e   s       n   o       i   n   v   e   r   s   i   o   n       o   r       s   l   i   c   i   n   g   .   
                                                   c   a   u   s   a   l   _   m   a   s   k       =       a   t   t   e   n   t   i   o   n   _   m   a   s   k   
                                   e   l   s   e   :   
                                                   m   i   n   _   d   t   y   p   e       =       t   o   r   c   h   .   f   i   n   f   o   (   d   t   y   p   e   )   .   m   i   n   
                                                   c   a   u   s   a   l   _   m   a   s   k       =       t   o   r   c   h   .   f   u   l   l   (   
                                                                   (   s   e   q   u   e   n   c   e   _   l   e   n   g   t   h   ,       t   a   r   g   e   t   _   l   e   n   g   t   h   )   ,       f   i   l   l   _   v   a   l   u   e   =   m   i   n   _   d   t   y   p   e   ,       d   t   y   p   e   =   d   t   y   p   e   ,       d   e   v   i   c   e   =   c   a   c   h   e   _   p   o   s   i   t   i   o   n   .   d   e   v   i   c   e   
                                                   )   
                                                   i   f       s   e   q   u   e   n   c   e   _   l   e   n   g   t   h       !   =       1   :   
                                                                   c   a   u   s   a   l   _   m   a   s   k       =       t   o   r   c   h   .   t   r   i   u   (   c   a   u   s   a   l   _   m   a   s   k   ,       d   i   a   g   o   n   a   l   =   1   )   
                                                   c   a   u   s   a   l   _   m   a   s   k       *   =       t   o   r   c   h   .   a   r   a   n   g   e   (   t   a   r   g   e   t   _   l   e   n   g   t   h   ,       d   e   v   i   c   e   =   c   a   c   h   e   _   p   o   s   i   t   i   o   n   .   d   e   v   i   c   e   )       >       c   a   c   h   e   _   p   o   s   i   t   i   o   n   .   r   e   s   h   a   p   e   (   -   1   ,       1   )   
                                                   c   a   u   s   a   l   _   m   a   s   k       =       c   a   u   s   a   l   _   m   a   s   k   [   N   o   n   e   ,       N   o   n   e   ,       :   ,       :   ]   .   e   x   p   a   n   d   (   b   a   t   c   h   _   s   i   z   e   ,       1   ,       -   1   ,       -   1   )   
                                                   i   f       a   t   t   e   n   t   i   o   n   _   m   a   s   k       i   s       n   o   t       N   o   n   e   :   
                                                                   c   a   u   s   a   l   _   m   a   s   k       =       c   a   u   s   a   l   _   m   a   s   k   .   c   l   o   n   e   (   )           #       c   o   p   y       t   o       c   o   n   t   i   g   u   o   u   s       m   e   m   o   r   y       f   o   r       i   n   -   p   l   a   c   e       e   d   i   t   
                                                                   m   a   s   k   _   l   e   n   g   t   h       =       a   t   t   e   n   t   i   o   n   _   m   a   s   k   .   s   h   a   p   e   [   -   1   ]   
                                                                   p   a   d   d   i   n   g   _   m   a   s   k       =       c   a   u   s   a   l   _   m   a   s   k   [   :   ,       :   ,       :   ,       :   m   a   s   k   _   l   e   n   g   t   h   ]       +       a   t   t   e   n   t   i   o   n   _   m   a   s   k   [   :   ,       N   o   n   e   ,       N   o   n   e   ,       :   ]   .   t   o   (   
                                                                                   c   a   u   s   a   l   _   m   a   s   k   .   d   e   v   i   c   e   
                                                                   )   
                                                                   p   a   d   d   i   n   g   _   m   a   s   k       =       p   a   d   d   i   n   g   _   m   a   s   k       =   =       0   
                                                                   c   a   u   s   a   l   _   m   a   s   k   [   :   ,       :   ,       :   ,       :   m   a   s   k   _   l   e   n   g   t   h   ]       =       c   a   u   s   a   l   _   m   a   s   k   [   :   ,       :   ,       :   ,       :   m   a   s   k   _   l   e   n   g   t   h   ]   .   m   a   s   k   e   d   _   f   i   l   l   (   
                                                                                   p   a   d   d   i   n   g   _   m   a   s   k   ,       m   i   n   _   d   t   y   p   e   
                                                                   )   
   
                                   r   e   t   u   r   n       c   a   u   s   a   l   _   m   a   s   k   
   
   
   c   l   a   s   s       K   w   a   r   g   s   F   o   r   C   a   u   s   a   l   L   M   (   F   l   a   s   h   A   t   t   e   n   t   i   o   n   K   w   a   r   g   s   ,       L   o   s   s   K   w   a   r   g   s   )   :       .   .   .   
   
   
   @   a   u   t   o   _   d   o   c   s   t   r   i   n   g   
   c   l   a   s   s       D   e   e   p   s   e   e   k   V   3   F   o   r   C   a   u   s   a   l   L   M   (   D   e   e   p   s   e   e   k   V   3   P   r   e   T   r   a   i   n   e   d   M   o   d   e   l   ,       G   e   n   e   r   a   t   i   o   n   M   i   x   i   n   )   :   
                   _   t   i   e   d   _   w   e   i   g   h   t   s   _   k   e   y   s       =       [   "   l   m   _   h   e   a   d   .   w   e   i   g   h   t   "   ]   
                   _   t   p   _   p   l   a   n       =       {   "   l   m   _   h   e   a   d   "   :       "   c   o   l   w   i   s   e   _   r   e   p   "   }   
                   _   p   p   _   p   l   a   n       =       {   "   l   m   _   h   e   a   d   "   :       (   [   "   h   i   d   d   e   n   _   s   t   a   t   e   s   "   ]   ,       [   "   l   o   g   i   t   s   "   ]   )   }   
   
                   d   e   f       _   _   i   n   i   t   _   _   (   s   e   l   f   ,       c   o   n   f   i   g   )   :   
                                   s   u   p   e   r   (   )   .   _   _   i   n   i   t   _   _   (   c   o   n   f   i   g   )   
                                   s   e   l   f   .   m   o   d   e   l       =       D   e   e   p   s   e   e   k   V   3   M   o   d   e   l   (   c   o   n   f   i   g   )   
                                   s   e   l   f   .   v   o   c   a   b   _   s   i   z   e       =       c   o   n   f   i   g   .   v   o   c   a   b   _   s   i   z   e   
                                   s   e   l   f   .   l   m   _   h   e   a   d       =       n   n   .   L   i   n   e   a   r   (   c   o   n   f   i   g   .   h   i   d   d   e   n   _   s   i   z   e   ,       c   o   n   f   i   g   .   v   o   c   a   b   _   s   i   z   e   ,       b   i   a   s   =   F   a   l   s   e   )   
   
                                   #       I   n   i   t   i   a   l   i   z   e       w   e   i   g   h   t   s       a   n   d       a   p   p   l   y       f   i   n   a   l       p   r   o   c   e   s   s   i   n   g   
                                   s   e   l   f   .   p   o   s   t   _   i   n   i   t   (   )   
   
                   d   e   f       g   e   t   _   i   n   p   u   t   _   e   m   b   e   d   d   i   n   g   s   (   s   e   l   f   )   :   
                                   r   e   t   u   r   n       s   e   l   f   .   m   o   d   e   l   .   e   m   b   e   d   _   t   o   k   e   n   s   
   
                   d   e   f       s   e   t   _   i   n   p   u   t   _   e   m   b   e   d   d   i   n   g   s   (   s   e   l   f   ,       v   a   l   u   e   )   :   
                                   s   e   l   f   .   m   o   d   e   l   .   e   m   b   e   d   _   t   o   k   e   n   s       =       v   a   l   u   e   
   
                   d   e   f       g   e   t   _   o   u   t   p   u   t   _   e   m   b   e   d   d   i   n   g   s   (   s   e   l   f   )   :   
                                   r   e   t   u   r   n       s   e   l   f   .   l   m   _   h   e   a   d   
   
                   d   e   f       s   e   t   _   o   u   t   p   u   t   _   e   m   b   e   d   d   i   n   g   s   (   s   e   l   f   ,       n   e   w   _   e   m   b   e   d   d   i   n   g   s   )   :   
                                   s   e   l   f   .   l   m   _   h   e   a   d       =       n   e   w   _   e   m   b   e   d   d   i   n   g   s   
   
                   d   e   f       s   e   t   _   d   e   c   o   d   e   r   (   s   e   l   f   ,       d   e   c   o   d   e   r   )   :   
                                   s   e   l   f   .   m   o   d   e   l       =       d   e   c   o   d   e   r   
   
                   d   e   f       g   e   t   _   d   e   c   o   d   e   r   (   s   e   l   f   )   :   
                                   r   e   t   u   r   n       s   e   l   f   .   m   o   d   e   l   
   
                   @   c   a   n   _   r   e   t   u   r   n   _   t   u   p   l   e   
                   @   a   u   t   o   _   d   o   c   s   t   r   i   n   g   
                   d   e   f       f   o   r   w   a   r   d   (   
                                   s   e   l   f   ,   
                                   i   n   p   u   t   _   i   d   s   :       O   p   t   i   o   n   a   l   [   t   o   r   c   h   .   L   o   n   g   T   e   n   s   o   r   ]       =       N   o   n   e   ,   
                                   a   t   t   e   n   t   i   o   n   _   m   a   s   k   :       O   p   t   i   o   n   a   l   [   t   o   r   c   h   .   T   e   n   s   o   r   ]       =       N   o   n   e   ,   
                                   p   o   s   i   t   i   o   n   _   i   d   s   :       O   p   t   i   o   n   a   l   [   t   o   r   c   h   .   L   o   n   g   T   e   n   s   o   r   ]       =       N   o   n   e   ,   
                                   p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   :       O   p   t   i   o   n   a   l   [   C   a   c   h   e   ]       =       N   o   n   e   ,   
                                   i   n   p   u   t   s   _   e   m   b   e   d   s   :       O   p   t   i   o   n   a   l   [   t   o   r   c   h   .   F   l   o   a   t   T   e   n   s   o   r   ]       =       N   o   n   e   ,   
                                   l   a   b   e   l   s   :       O   p   t   i   o   n   a   l   [   t   o   r   c   h   .   L   o   n   g   T   e   n   s   o   r   ]       =       N   o   n   e   ,   
                                   u   s   e   _   c   a   c   h   e   :       O   p   t   i   o   n   a   l   [   b   o   o   l   ]       =       N   o   n   e   ,   
                                   o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s   :       O   p   t   i   o   n   a   l   [   b   o   o   l   ]       =       N   o   n   e   ,   
                                   o   u   t   p   u   t   _   h   i   d   d   e   n   _   s   t   a   t   e   s   :       O   p   t   i   o   n   a   l   [   b   o   o   l   ]       =       N   o   n   e   ,   
                                   c   a   c   h   e   _   p   o   s   i   t   i   o   n   :       O   p   t   i   o   n   a   l   [   t   o   r   c   h   .   L   o   n   g   T   e   n   s   o   r   ]       =       N   o   n   e   ,   
                                   l   o   g   i   t   s   _   t   o   _   k   e   e   p   :       U   n   i   o   n   [   i   n   t   ,       t   o   r   c   h   .   T   e   n   s   o   r   ]       =       0   ,   
                                   *   *   k   w   a   r   g   s   :       U   n   p   a   c   k   [   K   w   a   r   g   s   F   o   r   C   a   u   s   a   l   L   M   ]   ,   
                   )       -   >       C   a   u   s   a   l   L   M   O   u   t   p   u   t   W   i   t   h   P   a   s   t   :   
                                   r   "   "   "   
                                   l   a   b   e   l   s       (   `   t   o   r   c   h   .   L   o   n   g   T   e   n   s   o   r   `       o   f       s   h   a   p   e       `   (   b   a   t   c   h   _   s   i   z   e   ,       s   e   q   u   e   n   c   e   _   l   e   n   g   t   h   )   `   ,       *   o   p   t   i   o   n   a   l   *   )   :   
                                                   L   a   b   e   l   s       f   o   r       c   o   m   p   u   t   i   n   g       t   h   e       m   a   s   k   e   d       l   a   n   g   u   a   g   e       m   o   d   e   l   i   n   g       l   o   s   s   .       I   n   d   i   c   e   s       s   h   o   u   l   d       e   i   t   h   e   r       b   e       i   n       `   [   0   ,       .   .   .   ,   
                                                   c   o   n   f   i   g   .   v   o   c   a   b   _   s   i   z   e   ]   `       o   r       -   1   0   0       (   s   e   e       `   i   n   p   u   t   _   i   d   s   `       d   o   c   s   t   r   i   n   g   )   .       T   o   k   e   n   s       w   i   t   h       i   n   d   i   c   e   s       s   e   t       t   o       `   -   1   0   0   `       a   r   e       i   g   n   o   r   e   d   
                                                   (   m   a   s   k   e   d   )   ,       t   h   e       l   o   s   s       i   s       o   n   l   y       c   o   m   p   u   t   e   d       f   o   r       t   h   e       t   o   k   e   n   s       w   i   t   h       l   a   b   e   l   s       i   n       `   [   0   ,       .   .   .   ,       c   o   n   f   i   g   .   v   o   c   a   b   _   s   i   z   e   ]   `   .   
   
                                   E   x   a   m   p   l   e   :   
   
                                   `   `   `   p   y   t   h   o   n   
                                   >   >   >       f   r   o   m       t   r   a   n   s   f   o   r   m   e   r   s       i   m   p   o   r   t       A   u   t   o   T   o   k   e   n   i   z   e   r   ,       D   e   e   p   s   e   e   k   V   3   F   o   r   C   a   u   s   a   l   L   M   
   
                                   >   >   >       m   o   d   e   l       =       D   e   e   p   s   e   e   k   V   3   F   o   r   C   a   u   s   a   l   L   M   .   f   r   o   m   _   p   r   e   t   r   a   i   n   e   d   (   "   m   e   t   a   -   d   e   e   p   s   e   e   k   _   v   3   /   D   e   e   p   s   e   e   k   V   3   -   2   -   7   b   -   h   f   "   )   
                                   >   >   >       t   o   k   e   n   i   z   e   r       =       A   u   t   o   T   o   k   e   n   i   z   e   r   .   f   r   o   m   _   p   r   e   t   r   a   i   n   e   d   (   "   m   e   t   a   -   d   e   e   p   s   e   e   k   _   v   3   /   D   e   e   p   s   e   e   k   V   3   -   2   -   7   b   -   h   f   "   )   
   
                                   >   >   >       p   r   o   m   p   t       =       "   H   e   y   ,       a   r   e       y   o   u       c   o   n   s   c   i   o   u   s   ?       C   a   n       y   o   u       t   a   l   k       t   o       m   e   ?   "   
                                   >   >   >       i   n   p   u   t   s       =       t   o   k   e   n   i   z   e   r   (   p   r   o   m   p   t   ,       r   e   t   u   r   n   _   t   e   n   s   o   r   s   =   "   p   t   "   )   
   
                                   >   >   >       #       G   e   n   e   r   a   t   e   
                                   >   >   >       g   e   n   e   r   a   t   e   _   i   d   s       =       m   o   d   e   l   .   g   e   n   e   r   a   t   e   (   i   n   p   u   t   s   .   i   n   p   u   t   _   i   d   s   ,       m   a   x   _   l   e   n   g   t   h   =   3   0   )   
                                   >   >   >       t   o   k   e   n   i   z   e   r   .   b   a   t   c   h   _   d   e   c   o   d   e   (   g   e   n   e   r   a   t   e   _   i   d   s   ,       s   k   i   p   _   s   p   e   c   i   a   l   _   t   o   k   e   n   s   =   T   r   u   e   ,       c   l   e   a   n   _   u   p   _   t   o   k   e   n   i   z   a   t   i   o   n   _   s   p   a   c   e   s   =   F   a   l   s   e   )   [   0   ]   
                                   "   H   e   y   ,       a   r   e       y   o   u       c   o   n   s   c   i   o   u   s   ?       C   a   n       y   o   u       t   a   l   k       t   o       m   e   ?   \   n   I   '   m       n   o   t       c   o   n   s   c   i   o   u   s   ,       b   u   t       I       c   a   n       t   a   l   k       t   o       y   o   u   .   "   
                                   `   `   `   "   "   "   
                                   o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s       =       o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s       i   f       o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s       i   s       n   o   t       N   o   n   e       e   l   s   e       s   e   l   f   .   c   o   n   f   i   g   .   o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s   
                                   o   u   t   p   u   t   _   h   i   d   d   e   n   _   s   t   a   t   e   s       =       (   
                                                   o   u   t   p   u   t   _   h   i   d   d   e   n   _   s   t   a   t   e   s       i   f       o   u   t   p   u   t   _   h   i   d   d   e   n   _   s   t   a   t   e   s       i   s       n   o   t       N   o   n   e       e   l   s   e       s   e   l   f   .   c   o   n   f   i   g   .   o   u   t   p   u   t   _   h   i   d   d   e   n   _   s   t   a   t   e   s   
                                   )   
   
                                   #       d   e   c   o   d   e   r       o   u   t   p   u   t   s       c   o   n   s   i   s   t   s       o   f       (   d   e   c   _   f   e   a   t   u   r   e   s   ,       l   a   y   e   r   _   s   t   a   t   e   ,       d   e   c   _   h   i   d   d   e   n   ,       d   e   c   _   a   t   t   n   )   
                                   o   u   t   p   u   t   s   :       B   a   s   e   M   o   d   e   l   O   u   t   p   u   t   W   i   t   h   P   a   s   t       =       s   e   l   f   .   m   o   d   e   l   (   
                                                   i   n   p   u   t   _   i   d   s   =   i   n   p   u   t   _   i   d   s   ,   
                                                   a   t   t   e   n   t   i   o   n   _   m   a   s   k   =   a   t   t   e   n   t   i   o   n   _   m   a   s   k   ,   
                                                   p   o   s   i   t   i   o   n   _   i   d   s   =   p   o   s   i   t   i   o   n   _   i   d   s   ,   
                                                   p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   =   p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   ,   
                                                   i   n   p   u   t   s   _   e   m   b   e   d   s   =   i   n   p   u   t   s   _   e   m   b   e   d   s   ,   
                                                   u   s   e   _   c   a   c   h   e   =   u   s   e   _   c   a   c   h   e   ,   
                                                   o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s   =   o   u   t   p   u   t   _   a   t   t   e   n   t   i   o   n   s   ,   
                                                   o   u   t   p   u   t   _   h   i   d   d   e   n   _   s   t   a   t   e   s   =   o   u   t   p   u   t   _   h   i   d   d   e   n   _   s   t   a   t   e   s   ,   
                                                   c   a   c   h   e   _   p   o   s   i   t   i   o   n   =   c   a   c   h   e   _   p   o   s   i   t   i   o   n   ,   
                                                   *   *   k   w   a   r   g   s   ,   
                                   )   
   
                                   h   i   d   d   e   n   _   s   t   a   t   e   s       =       o   u   t   p   u   t   s   .   l   a   s   t   _   h   i   d   d   e   n   _   s   t   a   t   e   
                                   #       O   n   l   y       c   o   m   p   u   t   e       n   e   c   e   s   s   a   r   y       l   o   g   i   t   s   ,       a   n   d       d   o       n   o   t       u   p   c   a   s   t       t   h   e   m       t   o       f   l   o   a   t       i   f       w   e       a   r   e       n   o   t       c   o   m   p   u   t   i   n   g       t   h   e       l   o   s   s   
                                   s   l   i   c   e   _   i   n   d   i   c   e   s       =       s   l   i   c   e   (   -   l   o   g   i   t   s   _   t   o   _   k   e   e   p   ,       N   o   n   e   )       i   f       i   s   i   n   s   t   a   n   c   e   (   l   o   g   i   t   s   _   t   o   _   k   e   e   p   ,       i   n   t   )       e   l   s   e       l   o   g   i   t   s   _   t   o   _   k   e   e   p   
                                   l   o   g   i   t   s       =       s   e   l   f   .   l   m   _   h   e   a   d   (   h   i   d   d   e   n   _   s   t   a   t   e   s   [   :   ,       s   l   i   c   e   _   i   n   d   i   c   e   s   ,       :   ]   )   
   
                                   l   o   s   s       =       N   o   n   e   
                                   i   f       l   a   b   e   l   s       i   s       n   o   t       N   o   n   e   :   
                                                   l   o   s   s       =       s   e   l   f   .   l   o   s   s   _   f   u   n   c   t   i   o   n   (   l   o   g   i   t   s   =   l   o   g   i   t   s   ,       l   a   b   e   l   s   =   l   a   b   e   l   s   ,       v   o   c   a   b   _   s   i   z   e   =   s   e   l   f   .   c   o   n   f   i   g   .   v   o   c   a   b   _   s   i   z   e   ,       *   *   k   w   a   r   g   s   )   
   
                                   r   e   t   u   r   n       C   a   u   s   a   l   L   M   O   u   t   p   u   t   W   i   t   h   P   a   s   t   (   
                                                   l   o   s   s   =   l   o   s   s   ,   
                                                   l   o   g   i   t   s   =   l   o   g   i   t   s   ,   
                                                   p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   =   o   u   t   p   u   t   s   .   p   a   s   t   _   k   e   y   _   v   a   l   u   e   s   ,   
                                                   h   i   d   d   e   n   _   s   t   a   t   e   s   =   o   u   t   p   u   t   s   .   h   i   d   d   e   n   _   s   t   a   t   e   s   ,   
                                                   a   t   t   e   n   t   i   o   n   s   =   o   u   t   p   u   t   s   .   a   t   t   e   n   t   i   o   n   s   ,   
                                   )   
   
   
   _   _   a   l   l   _   _       =       [   "   D   e   e   p   s   e   e   k   V   3   P   r   e   T   r   a   i   n   e   d   M   o   d   e   l   "   ,       "   D   e   e   p   s   e   e   k   V   3   M   o   d   e   l   "   ,       "   D   e   e   p   s   e   e   k   V   3   F   o   r   C   a   u   s   a   l   L   M   "   ]   
       = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs,
    ) -> Union[Tuple, Seq2SeqLMOutput]:
        r"""
        decoder_input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, target_sequence_length)`, *optional*):
            Indices of decoder input sequence tokens in the vocabulary, corresponding to the sequence of audio codes.

            Indices can be obtained by encoding an audio prompt with an audio encoder model to predict audio codes,
            such as with the [`EncodecModel`]. See [`EncodecModel.encode`] for details.

            [What are decoder input IDs?](../glossary#decoder-input-ids)

            <Tip warning={true}>

            The `decoder_input_ids` will automatically be converted from shape `(batch_size * num_codebooks,
            target_sequence_length)` t    batch_size, num_codebooks, target_sequence_length)` in the forward pass. If
            you obtain audio codes from an audio encoding model, such as [`EncodecModel`], ensure that the number of
            frames is equal to 1, and that you reshape the audio codes from `(frames, batch_size, num_codebooks,
            target_sequence_length)` to `(batch_size * num_codebooks, target_sequence_length)` prior to passing them as
            `decoder_input_ids`.

            </Tip>
        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
            be used by default.
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length, num_codebooks)`, *optional*):
            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
        padding_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)

        Examples:
        ```python
        >>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
        >>> import torch

        >>> processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
        >>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

        >>> inputs = processor(
        ...     text=["80s pop track with bassy drums and synth", "90s rock song with loud guitars and heavy drums"],
        ...     padding=True,
        ...     return_tensors="pt",
        ... )

        >>> pad_token_id = model.generation_config.pad_token_id
        >>> decoder_input_ids = (
        ...     torch.ones((inputs.input_ids.shape[0] * model.decoder.num_codebooks, 1), dtype=torch.long)
        ...     * pad_token_id
        ... )

        >>> logits = model(**inputs, decoder_input_ids=decoder_input_ids).logits
        >>> logits.shape  # (bsz * num_codebooks, tgt_len, vocab_size)
        torch.Size([8, 1, 2048])
        ```"""
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        kwargs_text_encoder = {
            argument[len("text_encoder_")]: value
            for argument, value in kwargs.items()
            if argument.startswith("text_encoder_")
        }

        kwargs_audio_encoder = {
            argument[len("audio_encoder_")]: value
            for argument, value in kwargs.items()
            if argument.startswith("audio_encoder_")
        }

        kwargs_decoder = {
            argument[len("decoder_") :]: value for argument, value in kwargs.items() if argument.startswith("decoder_")
        }

        if encoder_outputs is None:
            encoder_outputs = self.text_encoder(
                input_ids=input_ids,
                attention_mask=attention_mask,
                inputs_embeds=inputs_embeds,
                output_attentions=output_attentions,
                ou        (¨Ô   ÆQ     ÿÿÿÿÿÿÿÿ#                ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨
#           This file was automatically generated from src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_wav2vec2_conformer.py file directly. One of our CI enforces this.
#                ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨ð¨
import math
import warnings
from dataclasses import dataclass
from typing import Optional, Tuple, Union

import numpy as np
import torch
from torch import nn
from torch.nn import CrossEntropyLoss

from ...activations import ACT2FN
from ...integrations.deepspeed import is_deepspeed_zero3_enabled
from ...integrations.fsdp import is_fsdp_managed_module
from ...modeling_outputs import (
    BaseModelOutput,
    CausalLMOutput,
    SequenceClassifierOutput,
    TokenClassifierOutput,
    Wav2Vec2BaseModelOutput,
    XVectorOutput,
)
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring, is_peft_available
from .configuration_wav2vec2_conformer import Wav2Vec2ConformerConfig


@dataclass
class Wav2Vec2ConformerForPreTrainingOutput(ModelOutput):
    """
    Output type of [`Wav2Vec2ConformerForPreTraining`], with potential hidden states and attentions.

    Args:
        loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):
            Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official
            paper](https://arxiv.org/pdf/2006.11477.pdf) . (classification) loss.
        projected_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):
            Hidden-states of the model projected to *config.proj_codevector_dim* that can be used to predict the masked
            projected quantized states.
        projected_quantized_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):
            Quantized extracted feature vectors projected to *config.proj_codevector_dim* representing the positive
            target vectors for contrastive loss.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
            shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        contrastive_loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):
            The contrastive loss (L_m) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .
        diversity_loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):
            The diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .
    """

    loss: Optional[torch.FloatTensor] = None
    projected_states: Optional[torch.FloatTensor] = None
    projected_quantized_states: Optional[torch.FloatTensor] = None
    codevector_perplexity: Optional[torch.FloatTensor] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    contrastive_loss: Optional[torch.FloatTensor] = None
    diversity_loss: Optional[torch.FloatTensor] = None


class Wav2Vec2ConformerSamePadLayer(nn.Module):
    def __init__(self, num_conv_pos_embeddings):
        super().__init__()
        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0

    def forward(self, hidden_states):
        if self.num_pad_remove > 0:
            hidden_states = hidden_states[:, :, : -self.num_pad_remove]
        return hidden_states


class Wav2Vec2ConformerPositionalConvEmbedding(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.conv = nn.Conv1d(
            config.hidden_size,
            config.hidden_size,
            kernel_size=config.num_conv_pos_embeddings,
            padding=config.num_conv_pos_embeddings // 2,
            groups=config.num_conv_pos_embedding_groups,
        )

        weight_norm = nn.utils.weight_norm
        if hasattr(nn.utils.parametrizations, "weight_norm"):
            weight_norm = nn.utils.parametrizations.weight_norm

        if is_deepspeed_zero3_enabled():
            import deepspeed

            with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):
                self.conv = weight_norm(self.conv, name="weight", dim=2)
            if hasattr(self.conv, "parametrizations"):
                weight_g = self.conv.parametrizations.weight.original0
                weight_v = self.conv.parametrizations.weight.original1
            else:
                weight_g = self.conv.weight_g
                weight_v = self.conv.weight_v
            deepspeed.zero.register_external_parameter(self, weight_v)
            deepspeed.zero.register_external_parameter(self, weight_g)
        else:
            self.conv = weight_norm(self.conv, name="weight", dim=2)

        self.padding = Wav2Vec2ConformerSamePadLayer(config.num_conv_pos_embeddings)
        self.activation = ACT2FN[config.feat_extract_activation]

    def forward(self, hidden_states):
        hidden_states = hidden_states.transpose(1, 2)

        hidden_states = self.conv(hidden_states)
        hidden_states = self.padding(hidden_states)
        hidden_states = self.activation(hidden_states)

        hidden_states = hidden_states.transpose(1, 2)
        return hidden_states


class Wav2Vec2ConformerRotaryPositionalEmbedding(nn.Module):
    """Rotary positional embedding
    Reference : https://blog.eleuther.ai/rotary-embeddings/ Paper: https://arxiv.org/pdf/2104.09864.pdf
    """

    def __init__(self, config):
        super().__init__()
        dim = config.hidden_size // config.num_attention_heads
        base = config.rotary_embedding_base

        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))
        self.register_buffer("inv_freq", inv_freq)
        self.cached_sequence_length = None
        self.cached_rotary_positional_embedding = None

    def forward(self, hidden_states):
        sequence_length = hidden_states.shape[1]

        if sequence_length == self.cached_sequence_length and self.cached_rotary_positional_embedding is not None:
            return self.cached_rotary_positional_embedding

        self.cached_sequence_length = sequence_length
        # Embeddings are computed in the dtype of the inv_freq constant
        time_stamps = torch.arange(sequence_length).type_as(self.inv_freq)
        freqs = torch.einsum("i,j->ij", time_stamps, self.inv_freq)
        embeddings = torch.cat((freqs, freqs), dim=-1)

        cos_embeddings = embeddings.cos()[:, None, None, :]
        sin_embeddings = embeddings.sin()[:, None, None, :]
        # Computed embeddings are cast to the dtype of the hidden state inputs
        self.cached_rotary_positional_embedding = torch.stack([cos_embeddings, sin_embeddings]).type_as(hidden_states)
        return self.cached_rotary_positional_embedding


class Wav2Vec2ConformerRelPositionalEmbedding(nn.Module):
    """Relative positional encoding module."""

    def __init__(self, config):
        super().__init__()
        self.max_len = config.max_source_positions
        self.d_model = config.hidden_size
        self.pe = None
        self.extend_pe(torch.tensor(0.0).expand(1, self.max_len))

    def extend_pe(self, x):
        # Reset the positional encodings
        if self.pe is not None:
            # self.pe contains both positive and negative parts
            # the length of self.pe is 2 * input_len - 1
            if self.pe.size(1) >= x.size(1) * 2 - 1:
                if self.pe.dtype != x.dtype or self.pe.device != x.device:
                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)
                return
        # Suppose `i` is the position of query vector and `j` is the
        # position of key vector. We use positive relative positions when keys
        # are to the left (i>j) and negative relative positions otherwise (i<j).
        pe_positive = torch.zeros(x.size(1), self.d_model)
        pe_negative = torch.zeros(x.size(1), self.d_model)
        position = torch.arange(0, x.size(1), dtype=torch.int64).float().unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, self.d_model, 2, dtype=torch.int64).float() * -(math.log(10000.0) / self.d_model)
        )
        pe_positive[:, 0::2] = torch.sin(position * div_term)
        pe_positive[:, 1::2] = torch.cos(position * div_term)
        pe_negative[:, 0::2] = torch.sin(-1 * position * div_term)
        pe_negative[:, 1::2] = torch.cos(-1 * position * div_term)

        # Reverse the order of positive indices and concat both positive and
        # negative indices. This is used to support the shifting trick
        # as in https://arxiv.org/abs/1901.02860
        pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)
        pe_negative = pe_negative[1:].unsqueeze(0)
        pe = torch.cat([pe_positive, pe_negative], dim=1)
        self.pe = pe.to(device=x.device, dtype=x.dtype)

    def forward(self, hidden_states: torch.Tensor):
        self.extend_pe(hidden_states)
        start_idx = self.pe.size(1) // 2 - hidden_states.size(1) + 1
        end_idx = self.pe.size(1) // 2 + hidden_states.size(1)
        relative_position_embeddings = self.pe[:, start_idx:end_idx]

        return relative_position_embeddings


class Wav2Vec2ConformerNoLayerNormConvLayer(nn.Module):
    def __init__(self, config, layer_id=0):
        super().__init__()
        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1
        self.out_conv_dim = config.conv_dim[layer_id]

        self.conv = nn.Conv1d(
            self.in_conv_dim,
            self.out_conv_dim,
            kernel_size=config.conv_kernel[layer_id],
            stride=config.conv_stride[layer_id],
            bias=config.conv_bias,
        )
        self.activation = ACT2FN[config.feat_extract_activation]

    def forward(self, hidden_states):
        hidden_states = self.conv(hidden_states)
        hidden_states = self.activation(hidden_states)
        return hidden_states


class Wav2Vec2ConformerLayerNormConvLayer(nn.Module):
    def __init__(self, config, layer_id=0):
        super().__init__()
        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1
        self.out_conv_dim = config.conv_dim[layer_id]

        self.conv = nn.Conv1d(
            self.in_conv_dim,
            self.out_conv_dim,
            kernel_size=config.conv_kernel[layer_id],
            stride=config.conv_stride[layer_id],
            bias=config.conv_bias,
        )
        self.layer_norm = nn.LayerNorm(self.out_conv_dim, elementwise_affine=True)
        self.activation = ACT2FN[config.feat_extract_activation]

    def forward(self, hidden_states):
        hidden_states = self.conv(hidden_states)

        hidden_states = hidden_states.transpose(-2, -1)
        hidden_states = self.layer_norm(hidden_states)
        hidden_states = hidden_states.transpose(-2, -1)

        hidden_states = self.activation(hidden_states)
        return hidden_states


class Wav2Vec2ConformerGroupNormConvLayer(nn.Module):
    def __init__(self, config, layer_id=0):
        super().__init__()
        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1
        self.out_conv_dim = config.conv_dim[layer_id]

        self.conv = nn.Conv1d(
            self.in_conv_dim,
            self.out_conv_dim,
            kernel_size=config.conv_kernel[layer_id],
            stride=config.conv_stride[layer_id],
            bias=config.conv_bias,
        )
        self.activation = ACT2FN[config.feat_extract_activation]

        self.layer_norm = nn.GroupNorm(num_groups=self.out_conv_dim, num_channels=self.out_conv_dim, affine=True)

    def forward(self, hidden_states):
        hidden_states = self.conv(hidden_states)
        hidden_states = self.layer_norm(hidden_states)
        hidden_states = self.activation(hidden_states)
        return hidden_states


class Wav2Vec2ConformerFeatureEncoder(nn.Module):
    """Construct the features from raw audio waveform"""

    def __init__(self, config):
        super().__init__()

        if config.feat_extract_norm == "group":
            conv_layers = [Wav2Vec2ConformerGroupNormConvLayer(config, layer_id=0)] + [
                Wav2Vec2ConformerNoLayerNormConvLayer(config, layer_id=i + 1)
                for i in range(config.num_feat_extract_layers - 1)
            ]
        elif config.feat_extract_norm == "layer":
            conv_layers = [
                Wav2Vec2ConformerLayerNormConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)
            ]
        else:
            raise ValueError(
                f"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']"
            )
        self.conv_layers = nn.ModuleList(conv_layers)
        self.gradient_checkpointing = False
        self._requires_grad = True

    def _freeze_parameters(self):
        for param in self.parameters():
            param.requires_grad = False
        self._requires_grad = False

    def forward(self, input_values):
        hidden_states = input_values[:, None]

        # make sure hidden_states require grad for gradient_checkpointing
        if self._requires_grad and self.training:
            hidden_states.requires_grad = True

        for conv_layer in self.conv_layers:
            if self._requires_grad and self.gradient_checkpointing and self.training:
                hidden_states = self._gradient_checkpointing_func(
                    conv_layer.__call__,
                    hidden_states,
                )
            else:
                hidden_states = conv_layer(hidden_states)

        return hidden_states


class Wav2Vec2ConformerFeatureProjection(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.layer_norm_eps)
        self.projection = nn.Linear(config.conv_dim[-1], config.hidden_size)
        self.dropout = nn.Dropout(config.feat_proj_dropout)

    def forward(self, hidden_states):
        # non-projected hidden states are needed for quantization
        norm_hidden_states = self.layer_norm(hidden_states)
        hidden_states = self.projection(norm_hidden_states)
        hidden_states = self.dropout(hidden_states)
        return hidden_states, norm_hidden_states


class Wav2Vec2ConformerFeedForward(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.intermediate_dropout = nn.Dropout(config.activation_dropout)

        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size)
        if isinstance(config.hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        else:
            self.intermediate_act_fn = config.hidden_act

        self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.output_dropout = nn.Dropout(config.hidden_dropout)

    def forward(self, hidden_states):
        hidden_states = self.intermediate_dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        hidden_states = self.intermediate_dropout(hidden_states)

        hidden_states = self.output_dense(hidden_states)
        hidden_states = self.output_dropout(hidden_states)
        return hidden_states


class Wav2Vec2ConformerConvolutionModule(nn.Module):
    """Convolution block used in the conformer block"""

    def __init__(self, config):
        super().__init__()
        if (config.conv_depthwise_kernel_size - 1) % 2 == 1:
            raise ValueError("`config.conv_depthwise_kernel_size` should be a odd number for 'SAME' padding")
        self.layer_norm = nn.LayerNorm(config.hidden_size)
        self.pointwise_conv1 = nn.Conv1d(
            config.hidden_size,
            2 * config.hidden_size,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=False,
        )
        self.glu = nn.GLU(dim=1)
        self.depthwise_conv = nn.Conv1d(
            config.hidden_size,
            config.hidden_size,
            config.conv_depthwise_kernel_size,
            stride=1,
            padding=(config.conv_depthwise_kernel_size - 1) // 2,
            groups=config.hidden_size,
            bias=False,
        )
        self.batch_norm = nn.BatchNorm1d(config.hidden_size)
        self.activation = ACT2FN[config.hidden_act]
        self.pointwise_conv2 = nn.Conv1d(
            config.hidden_size,
            config.hidden_size,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=False,
        )
        self.dropout = nn.Dropout(config.conformer_conv_dropout)

    def forward(self, hidden_states):
        hidden_states = self.layer_norm(hidden_states)
        # exchange the temporal dimension and the feature dimension
        hidden_states = hidden_states.transpose(1, 2)

        # GLU mechanism
        # => (batch, 2*channel, dim)
        hidden_states = self.pointwise_conv1(hidden_states)
        # => (batch, channel, dim)
        hidden_states = self.glu(hidden_states)

        # 1D Depthwise Conv
        hidden_states = self.depthwise_conv(hidden_states)
        hidden_states = self.batch_norm(hidden_states)
        hidden_states = self.activation(hidden_states)

        hidden_states = self.pointwise_conv2(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = hidden_states.transpose(1, 2)
        return hidden_states


class Wav2Vec2ConformerSelfAttention(nn.Module):
    """Construct an Wav2Vec2ConformerSelfAttention object.
    Can be enhanced with rotary or relative position embeddings.
    """

    def __init__(self, config):
        super().__init__()

        self.head_size = config.hidden_size // config.num_attention_heads
        self.num_heads = config.num_attention_heads
        self.position_embeddings_type = config.position_embeddings_type

        self.linear_q = nn.Linear(config.hidden_size, config.hidden_size)
        self.linear_k = nn.Linear(config.hidden_size, config.hidden_size)
        self.linear_v = nn.Linear(config.hidden_size, config.hidden_size)
        self.linear_out = nn.Linear(config.hidden_size, config.hidden_size)

        self.dropout = nn.Dropout(p=config.attention_dropout)

        if self.position_embeddings_type == "relative":
            # linear transformation for positional encoding
            self.linear_pos = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
            # these two learnable bias are used in matrix c and matrix d
            # as described in https://arxiv.org/abs/1901.02860 Section 3.3
            self.pos_bias_u = nn.Parameter(torch.zeros(self.num_heads, self.head_size))
            self.pos_bias_v = nn.Parameter(torch.zeros(self.num_heads, self.head_size))

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        relative_position_embeddings: Optional[torch.Tensor] = None,
        output_attentions: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        # self-attention mechanism
        batch_size, sequence_length, hidden_size = hidden_states.size()

        # make sure query/key states can be != value states
        query_key_states = hidden_states
        value_states = hidden_states

        if self.position_embeddings_type == "rotary":
            if relative_position_embeddings is None:
                raise ValueError(
                    "`relative_position_embeddings` has to be defined when `self.position_embeddings_type == 'rotary'"
                )
            query_key_states = self._apply_rotary_embedding(query_key_states, relative_position_embeddings)

        # project query_key_states and value_states
        query = self.linear_q(query_key_states).view(batch_size, -1, self.num_heads, self.head_size)
        key = self.linear_k(query_key_states).view(batch_size, -1, self.num_heads, self.head_size)
        value = self.linear_v(value_states).view(batch_size, -1, self.num_heads, self.head_size)

        # => (batch, head, time1, d_k)
        query = query.transpose(1, 2)
        key = key.transpose(1, 2)
        value = value.transpose(1, 2)

        if self.position_embeddings_type == "relative":
            if relative_position_embeddings is None:
                raise ValueError(
                    "`relative_position_embeddings` has to be defined when `self.position_embeddings_type =="
                    " 'relative'"
                )
            # apply relative_position_embeddings to qk scores
            # as proposed in Transformer_XL: https://arxiv.org/abs/1901.02860
            scores = self._apply_relative_embeddings(
                query=query, key=key, relative_position_embeddings=relative_position_embeddings
            )
        else:
            scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_size)

        # apply attention_mask if necessary
        if attention_mask is not None:
            scores = scores + attention_mask

        # => (batch, head, time1, time2)
        probs = torch.softmax(scores, dim=-1)
        probs = self.dropout(probs)

        # => (batch, head, time1, d_k)
        hidden_states = torch.matmul(probs, value)

        # => (batch, time1, hidden_size)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, self.num_heads * self.head_size)
        hidden_states = self.linear_out(hidden_states)

        return hidden_states, probs

    def _apply_rotary_embedding(self, hidden_states, relative_position_embeddings):
        batch_size, sequence_length, hidden_size = hidden_states.size()
        hidden_states = hidden_states.view(batch_size, sequence_length, self.num_heads, self.head_size)

        cos = relative_position_embeddings[0, :sequence_length, ...]
        sin = relative_position_embeddings[1, :sequence_length, ...]

        # rotate hidden_states with rotary embeddings
        hidden_states = hidden_states.transpose(0, 1)
        rotated_states_begin = hidden_states[..., : self.head_size // 2]
        rotated_states_end = hidden_states[..., self.head_size // 2 :]
        rotated_states = torch.cat((-rotated_states_end, rotated_states_begin), dim=rotated_states_begin.ndim - 1)
        hidden_states = (hidden_states * cos) + (rotated_states * sin)
        hidden_states = hidden_states.transpose(0, 1)

        hidden_states = hidden_states.view(batch_size, sequence_length, self.num_heads * self.head_size)

        return hidden_states

    def _apply_relative_embeddings(self, query, key, relative_position_embeddings):
        # 1. project positional embeddings
        # => (batch, head, 2*time1-1, d_k)
        proj_relative_position_embeddings = self.linear_pos(relative_position_embeddings)
        proj_relative_position_embeddings = proj_relative_position_embeddings.view(
            relative_position_embeddings.size(0), -1, self.num_heads, self.head_size
        )
        proj_relative_position_embeddings = proj_relative_position_embeddings.transpose(1, 2)
        proj_relative_position_embeddings = proj_relative_position_embeddings.transpose(2, 3)

        # 2. Add bias to query
        # => (batch, head, time1, d_k)
        query = query.transpose(1, 2)
        q_with_bias_u = (query + self.pos_bias_u).transpose(1, 2)
        q_with_bias_v = (query + self.pos_bias_v).transpose(1, 2)

        # 3. attention score: first compute matrix a and matrix c
        # as described in https://arxiv.org/abs/1901.02860 Section 3.3
        # => (batch, head, time1, time2)
        scores_ac = torch.matmul(q_with_bias_u, key.transpose(-2, -1))

        # 4. then compute matrix b and matrix d
        # => (batch, head, time1, 2*time1-1)
        scores_bd = torch.matmul(q_with_bias_v, proj_relative_position_embeddings)

        # 5. shift matrix b and matrix d
        zero_pad = torch.zeros((*scores_bd.size()[:3], 1), device=scores_bd.device, dtype=scores_bd.dtype)
        scores_bd_padded = torch.cat([zero_pad, scores_bd], dim=-1)
        scores_bd_padded_shape = scores_bd.size()[:2] + (scores_bd.shape[3] + 1, scores_bd.shape[2])
        scores_bd_padded = scores_bd_padded.view(*scores_bd_padded_shape)
        scores_bd = scores_bd_padded[:, :, 1:].view_as(scores_bd)
        scores_bd = scores_bd[:, :, :, : scores_bd.size(-1) // 2 + 1]

        # 6. sum matrices
        # => (batch, head, time1, time2)
        scores = (scores_ac + scores_bd) / math.sqrt(self.head_size)

        return scores


class Wav2Vec2ConformerEncoderLayer(nn.Module):
    """Conformer block based on https://arxiv.org/abs/2005.08100."""

    def __init__(self, config):
        super().__init__()
        embed_dim = config.hidden_size
        dropout = config.attention_dropout

        # Feed-forward 1
        self.ffn1_layer_norm = nn.LayerNorm(embed_dim)
        self.ffn1 = Wav2Vec2ConformerFeedForward(config)

        # Self-Attention
        self.self_attn_layer_norm = nn.LayerNorm(embed_dim)
        self.self_attn_dropout = nn.Dropout(dropout)
        self.self_attn = Wav2Vec2ConformerSelfAttention(config)

        # Conformer Convolution
        self.conv_module = Wav2Vec2ConformerConvolutionModule(config)

        # Feed-forward 2
        self.ffn2_layer_norm = nn.LayerNorm(embed_dim)
        self.ffn2 = Wav2Vec2ConformerFeedForward(config)
        self.final_layer_norm = nn.LayerNorm(embed_dim)

    def forward(
        self,
        hidden_states,
        attention_mask: Optional[torch.Tensor] = None,
        relative_position_embeddings: Optional[torch.Tensor] = None,
        output_attentions: bool = False,
    ):
        hidden_states = hidden_states

        # 1. Feed-Forward 1 layer
        residual = hidden_states
        hidden_states = self.ffn1_layer_norm(hidden_states)
        hidden_states = self.ffn1(hidden_states)
        hidden_states = hidden_states * 0.5 + residual
        residual = hidden_states

        # 2. Self-Attention layer
        hidden_states = self.self_attn_layer_norm(hidden_states)
        hidden_states, attn_weigts = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            relative_position_embeddings=relative_position_embeddings,
            output_attentions=output_attentions,
        )
        hidden_states = self.self_attn_dropout(hidden_states)
        hidden_states = hidden_states + residual

        # 3. Convolutional Layer
        residual = hidden_states
        hidden_states = self.conv_module(hidden_states)
        hidden_states = residual + hidden_states

        # 4. Feed-Forward 2 Layer
        residual = hidden_states
        hidden_states = self.ffn2_layer_norm(hidden_states)
        hidden_states = self.ffn2(hidden_states)
        hidden_states = hidden_states * 0.5 + residual
        hidden_states = self.final_layer_norm(hidden_states)

        return hidden_states, attn_weigts


class Wav2Vec2ConformerEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        if config.position_embeddings_type == "relative":
            self.embed_positions = Wav2Vec2ConformerRelPositionalEmbedding(config)
        elif config.position_embeddings_type == "rotary":
            self.embed_positions = Wav2Vec2ConformerRotaryPositionalEmbedding(config)
        else:
            self.embed_positions = None

        self.pos_conv_embed = Wav2Vec2ConformerPositionalConvEmbedding(config)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout)
        self.layers = nn.ModuleList([Wav2Vec2ConformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])
        self.gradient_checkpointing = False

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        output_attentions=False,
        output_hidden_states=False,
        return_dict=True,
    ):
        all_hidden_states = () if output_hidden_states else None
        all_self_attentions = () if output_attentions else None

        if attention_mask is not None:
            # make sure padded tokens output 0
            expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])
            hidden_states[~expand_attention_mask] = 0.0

            # extend attention_mask
            attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)
            attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min
            attention_mask = attention_mask.expand(
                attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]
            )

        hidden_states = self.dropout(hidden_states)

        if self.embed_positions is not None:
            relative_position_embeddings = self.embed_positions(hidden_states)
        else:
            relative_position_embeddings = None

        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)

        for i, layer in enumerate(self.layers):
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
            dropout_probability = torch.rand([])

            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False
            if not skip_the_layer or synced_gpus:
                # under fsdp or deepspeed zero3 all gpus must run in sync
                if self.gradient_checkpointing and self.training:
                    layer_outputs = self._gradient_checkpointing_func(
                        layer.__call__,
                        hidden_states,
                        attention_mask,
                        relative_position_embeddings,
                        output_attentions,
                    )
                else:
                    layer_outputs = layer(
                        hidden_states,
                        attention_mask=attention_mask,
                        relative_position_embeddings=relative_position_embeddings,
                        output_attentions=output_attentions,
                    )
                hidden_states = layer_outputs[0]

            if skip_the_layer:
                layer_outputs = (None, None)

            if output_attentions:
                all_self_attentions = all_self_attentions + (layer_outputs[1],)

        hidden_states = self.layer_norm(hidden_states)
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        if not return_dict:
            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)
        return BaseModelOutput(
            last_hidden_state=hidden_states,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
        )


class Wav2Vec2ConformerGumbelVectorQuantizer(nn.Module):
    """
    Vector quantization using gumbel softmax. See `[CATEGORICAL REPARAMETERIZATION WITH
    GUMBEL-SOFTMAX](https://arxiv.org/pdf/1611.01144.pdf) for more information.
    """

    def __init__(self, config):
        super().__init__()
        self.num_groups = config.num_codevector_groups
        self.num_vars = config.num_codevectors_per_group

        if config.codevector_dim % self.num_groups != 0:
            raise ValueError(
                f"`config.codevector_dim {config.codevector_dim} must be divisible "
                f"by `config.num_codevector_groups` {self.num_groups} for concatenation"
            )

        # storage for codebook variables (codewords)
        self.codevectors = nn.Parameter(
            torch.FloatTensor(1, self.num_groups * self.num_vars, config.codevector_dim // self.num_groups)
        )
        self.weight_proj = nn.Linear(config.conv_dim[-1], self.num_groups * self.num_vars)

        # can be decayed for training
        self.temperature = 2

    @staticmethod
    def _compute_perplexity(probs, mask=None):
        if mask is not None:
            mask_extended = mask.flatten()[:, None, None].expand(probs.shape)
            probs = torch.where(mask_extended, probs, torch.zeros_like(probs))
            marginal_probs = probs.sum(dim=0) / mask.sum()
        else:
            marginal_probs = probs.mean(dim=0)

        perplexity = torch.exp(-torch.sum(marginal_probs * torch.log(marginal_probs + 1e-7), dim=-1)).sum()
        return perplexity

    def forward(self, hidden_states, mask_time_indices=None):
        batch_size, sequence_length, hidden_size = hidden_states.shape

        # project to codevector dim
        hidden_states = self.weight_proj(hidden_states)
        hidden_states = hidden_states.view(batch_size * sequence_length * self.num_groups, -1)

        if self.training:
            # sample code vector probs via gumbel in differentiateable way
            codevector_probs = nn.functional.gumbel_softmax(
                hidden_states.float(), tau=self.temperature, hard=True
            ).type_as(hidden_states)

            # compute perplexity
            codevector_soft_dist = torch.softmax(
                hidden_states.view(batch_size * sequence_length, self.num_groups, -1).float(), dim=-1
            )
            perplexity = self._compute_perplexity(codevector_soft_dist, mask_time_indices)
        else:
            # take argmax in non-differentiable way
            # comptute hard codevector distribution (one hot)
            codevector_idx = hidden_states.argmax(dim=-1)
            codevector_probs = hidden_states.new_zeros(hidden_states.shape).scatter_(
                -1, codevector_idx.view(-1, 1), 1.0
            )
            codevector_probs = codevector_probs.view(batch_size * sequence_length, self.num_groups, -1)

            perplexity = self._compute_perplexity(codevector_probs, mask_time_indices)

        codevector_probs = codevector_probs.view(batch_size * sequence_length, -1)
        # use probs to retrieve codevectors
        codevectors_per_group = codevector_probs.unsqueeze(-1) * self.codevectors
        codevectors = codevectors_per_group.view(batch_size * sequence_length, self.num_groups, self.num_vars, -1)
        codevectors = codevectors.sum(-2).view(batch_size, sequence_length, -1)

        return codevectors, perplexity


class Wav2Vec2ConformerAdapter(nn.Module):
    def __init__(self, config):
        super().__init__()

        # feature dim might need to be down-projected
        if config.output_hidden_size != config.hidden_size:
            self.proj = nn.Linear(config.hidden_size, config.output_hidden_size)
            self.proj_layer_norm = nn.LayerNorm(config.output_hidden_size)
        else:
            self.proj = self.proj_layer_norm = None

        self.layers = nn.ModuleList(Wav2Vec2ConformerAdapterLayer(config) for _ in range(config.num_adapter_layers))
        self.layerdrop = config.layerdrop

    def forward(self, hidden_states):
        # down project hidden_states if necessary
        if self.proj is not None and self.proj_layer_norm is not None:
            hidden_states = self.proj(hidden_states)
            hidden_states = self.proj_layer_norm(hidden_states)

        hidden_states = hidden_states.transpose(1, 2)

        for layer in self.layers:
            layerdrop_prob = np.random.random()
            if not self.training or (layerdrop_prob > self.layerdrop):
                hidden_states = layer(hidden_states)

        hidden_states = hidden_states.transpose(1, 2)
        return hidden_states


class Wav2Vec2ConformerAdapterLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.conv = nn.Conv1d(
            config.output_hidden_size,
            2 * config.output_hidden_size,
            config.adapter_kernel_size,
            stride=config.adapter_stride,
            padding=1,
        )

    def forward(self, hidden_states):
        hidden_states = self.conv(hidden_states)
        hidden_states = nn.functional.glu(hidden_states, dim=1)

        return hidden_states


@auto_docstring
class Wav2Vec2ConformerPreTrainedModel(PreTrainedModel):
    config_class = Wav2Vec2ConformerConfig
    base_model_prefix = "wav2vec2_conformer"
    main_input_name = "input_values"
    supports_gradient_checkpointing = True

    def _init_weights(self, module):
        """Initialize the weights"""
        # Wav2Vec2ForPreTraining last 2 linear layers need standard Linear init.
        if isinstance(module, Wav2Vec2ConformerForPreTraining):
            module.project_hid.reset_parameters()
            module.project_q.reset_parameters()
            module.project_hid._is_hf_initialized = True
            module.project_q._is_hf_initialized = True
        # gumbel softmax requires special init
        elif isinstance(module, Wav2Vec2ConformerGumbelVectorQuantizer):
            module.weight_proj.weight.data.normal_(mean=0.0, std=1)
            module.weight_proj.bias.data.zero_()
            nn.init.uniform_(module.codevectors)
        elif isinstance(module, Wav2Vec2ConformerSelfAttention):
            if hasattr(module, "pos_bias_u"):
                nn.init.xavier_uniform_(module.pos_bias_u)
            if hasattr(module, "pos_bias_v"):
                nn.init.xavier_uniform_(module.pos_bias_v)
        elif isinstance(module, Wav2Vec2ConformerPositionalConvEmbedding):
            nn.init.normal_(
                module.conv.weight,
                mean=0,
                std=2 * math.sqrt(1 / (module.conv.kernel_size[0] * module.conv.in_channels)),
            )
            nn.init.constant_(module.conv.bias, 0)
        elif isinstance(module, Wav2Vec2ConformerFeatureProjection):
            k = math.sqrt(1 / module.projection.in_features)
            nn.init.uniform_(module.projection.weight, a=-k, b=k)
            nn.init.uniform_(module.projection.bias, a=-k, b=k)
        elif isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)

            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        elif isinstance(module, nn.Conv1d):
            nn.init.kaiming_normal_(module.weight)

            if module.bias is not None:
                k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))
                nn.init.uniform_(module.bias, a=-k, b=k)

    def _get_feat_extract_output_lengths(
        self, input_lengths: Union[torch.LongTensor, int], add_adapter: Optional[bool] = None
    ):
        """
        Computes the output length of the convolutional layers
        """

        add_adapter = self.config.add_adapter if add_adapter is None else add_adapter

        def _conv_out_length(input_length, kernel_size, stride):
            # 1D convolutional layer output length formula taken
            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html
            return torch.div(input_length - kernel_size, stride, rounding_mode="floor") + 1

        for kernel_size, stride in zip(self.config.conv_kernel, self.config.conv_stride):
            input_lengths = _conv_out_length(input_lengths, kernel_size, stride)

        if add_adapter:
            for _ in range(self.config.num_adapter_layers):
                input_lengths = _conv_out_length(input_lengths, 1, self.config.adapter_stride)

        return input_lengths

    def _get_feature_vector_attention_mask(
        self, feature_vector_length: int, attention_mask: torch.LongTensor, add_adapter=None
    ):
        # Effectively attention_mask.sum(-1), but not inplace to be able to run
        # on inference mode.
        non_padded_lengths = attention_mask.cumsum(dim=-1)[:, -1]

        output_lengths = self._get_feat_extract_output_lengths(non_padded_lengths, add_adapter=add_adapter)
        output_lengths = output_lengths.to(torch.long)

        batch_size = attention_mask.shape[0]

        attention_mask = torch.zeros(
            (batch_size, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device
        )
        # these two operations makes sure that all values before the output lengths idxs are attended to
        attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1
        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()
        return attention_mask


def _compute_mask_indices(
    shape: Tuple[int, int],
    mask_prob: float,
    mask_length: int,
    attention_mask: Optional[torch.LongTensor] = None,
    min_masks: int = 0,
) -> np.ndarray:
    """
    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for
    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on
    CPU as part of the preprocessing during training.

    Args:
        shape: The shape for which to compute masks. This should be of a tuple of size 2 where
               the first element is the batch size and the second element is the length of the axis to span.
        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of
                    independently generated mask spans of length `mask_length` is computed by
                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the
                    actual percentage will be smaller.
        mask_length: size of the mask
        min_masks: minimum number of masked spans
        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of
                        each batch dimension.
    """
    batch_size, sequence_length = shape

    if mask_length < 1:
        raise ValueError("`mask_length` has to be bigger than 0.")

    if mask_length > sequence_length:
        raise ValueError(
            f"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}"
            f" and `sequence_length`: {sequence_length}`"
        )

    # epsilon is used for probabilistic rounding
    epsilon = np.random.rand(1).item()

    def compute_num_masked_span(input_length):
        """Given input length, compute how many spans should be masked"""
        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)
        num_masked_span = max(num_masked_span, min_masks)

        # make sure num masked span <= sequence_length
        if num_masked_span * mask_length > sequence_length:
            num_masked_span = sequence_length // mask_length

        # make sure num_masked span is also <= input_length - (mask_length - 1)
        if input_length - (mask_length - 1) < num_masked_span:
            num_masked_span = max(input_length - (mask_length - 1), 0)

        return num_masked_span

    # compute number of masked spans in batch
    input_lengths = (
        attention_mask.detach().sum(-1).tolist()
        if attention_mask is not None
        else [sequence_length for _ in range(batch_size)]
    )

    # SpecAugment mask to fill
    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)
    spec_aug_mask_idxs = []

    max_num_masked_span = compute_num_masked_span(sequence_length)

    if max_num_masked_span == 0:
        return spec_aug_mask

    for input_length in input_lengths:
        # compute num of masked spans for this input
        num_masked_span = compute_num_masked_span(input_length)

        # get random indices to mask
        spec_aug_mask_idx = np.random.choice(
            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False
        )

        # pick first sampled index that will serve as a dummy index to pad vector
        # to ensure same dimension for all batches due to probabilistic rounding
        # Picking first sample just pads those vectors twice.
        if len(spec_aug_mask_idx) == 0:
            # this case can only happen if `input_length` is strictly smaller then
            # `sequence_length` in which case the last token has to be a padding
            # token which we can use as a dummy mask id
            dummy_mask_idx = sequence_length - 1
        else:
            dummy_mask_idx = spec_aug_mask_idx[0]

        spec_aug_mask_idx = np.concatenate(
            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]
        )
        spec_aug_mask_idxs.append(spec_aug_mask_idx)

    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)

    # expand masked indices to masked spans
    spec_aug_mask_idxs = np.broadcast_to(
        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)
    )
    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)

    # add offset to the starting indexes so that indexes now create a span
    offsets = np.arange(mask_length)[None, None, :]
    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(
        batch_size, max_num_masked_span * mask_length
    )
    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets

    # ensure that we cannot have indices larger than sequence_length
    if spec_aug_mask_idxs.max() > sequence_length - 1:
        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1

    # scatter indices to mask
    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)

    return spec_aug_mask


Wav2Vec2ConformerBaseModelOutput = Wav2Vec2BaseModelOutput


@auto_docstring
class Wav2Vec2ConformerModel(Wav2Vec2ConformerPreTrainedModel):
    def __init__(self, config: Wav2Vec2ConformerConfig):
        super().__init__(config)
        self.config = config
        self.feature_extractor = Wav2Vec2ConformerFeatureEncoder(config)
        self.feature_projection = Wav2Vec2ConformerFeatureProjection(config)

        # model only needs masking vector if mask prob is > 0.0
        if config.mask_time_prob > 0.0 or config.mask_feature_prob > 0.0:
            self.masked_spec_embed = nn.Parameter(torch.Tensor(config.hidden_size).uniform_())

        self.encoder = Wav2Vec2ConformerEncoder(config)

        self.adapter = Wav2Vec2ConformerAdapter(config) if config.add_adapter else None

        # Initialize weights and apply final processing
        self.post_init()

    def freeze_feature_encoder(self):
        """
        Calling this function will disable the gradient computation for the feature encoder so that its parameter will
        not be updated during training.
        """
        self.feature_extractor._freeze_parameters()

    def _mask_hidden_states(
        self,
        hidden_states: torch.FloatTensor,
        mask_time_indices: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.LongTensor] = None,
    ):
        """
        Masks extracted features along time axis and/or along feature axis according to
        [SpecAugment](https://arxiv.org/abs/1904.08779).
        """

        # `config.apply_spec_augment` can set masking to False
        if not getattr(self.config, "apply_spec_augment", True):
            return hidden_states

        # generate indices & apply SpecAugment along time axis
        batch_size, sequence_length, hidden_size = hidden_states.size()

        if mask_time_indices is not None:
            # apply SpecAugment along time axis with given mask_time_indices
            hidden_states[mask_time_indices] = self.masked_spec_embed.to(hidden_states.dtype)
        elif self.config.mask_time_prob > 0 and self.training:
            mask_time_indices = _compute_mask_indices(
                (batch_size, sequence_length),
                mask_prob=self.config.mask_time_prob,
                mask_length=self.config.mask_time_length,
                attention_mask=attention_mask,
                min_masks=self.config.mask_time_min_masks,
            )
            mask_time_indices = torch.tensor(mask_time_indices, device=hidden_states.device, dtype=torch.bool)
            hidden_states[mask_time_indices] = self.masked_spec_embed.to(hidden_states.dtype)

        if self.config.mask_feature_prob > 0 and self.training:
            # generate indices & apply SpecAugment along feature axis
            mask_feature_indices = _compute_mask_indices(
                (batch_size, hidden_size),
                mask_prob=self.config.mask_feature_prob,
                mask_length=self.config.mask_feature_length,
                min_masks=self.config.mask_feature_min_masks,
            )
            mask_feature_indices = torch.tensor(mask_feature_indices, device=hidden_states.device, dtype=torch.bool)
            mask_feature_indices = mask_feature_indices[:, None].expand(-1, sequence_length, -1)
            hidden_states[mask_feature_indices] = 0

        return hidden_states

    @auto_docstring
    def forward(
        self,
        input_values: Optional[torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        mask_time_indices: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, Wav2Vec2ConformerBaseModelOutput]:
        r"""
        mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict
            masked extracted features in *config.proj_codevector_dim* space.
        """
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        extract_features = self.feature_extractor(input_values)
        extract_features = extract_features.transpose(1, 2)

        if attention_mask is not None:
            # compute reduced attention_mask corresponding to feature vectors
            attention_mask = self._get_feature_vector_attention_mask(
                extract_features.shape[1], attention_mask, add_adapter=False
            )

        hidden_states, extract_features = self.feature_projection(extract_features)
        hidden_states = self._mask_hidden_states(
            hidden_states, mask_time_indices=mask_time_indices, attention_mask=attention_mask
        )

        encoder_outputs = self.encoder(
            hidden_states,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        hidden_states = encoder_outputs[0]

        if self.adapter is not None:
            hidden_states = self.adapter(hidden_states)

        if not return_dict:
            return (hidden_states, extract_features) + encoder_outputs[1:]

        return Wav2Vec2ConformerBaseModelOutput(
            last_hidden_state=hidden_states,
            extract_features=extract_features,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        )


@auto_docstring(
    custom_intro="""
    Wav2Vec2Conformer Model with a quantizer and `VQ` head on top.
    """
)
class Wav2Vec2ConformerForPreTraining(Wav2Vec2ConformerPreTrainedModel):
    def __init__(self, config: Wav2Vec2ConformerConfig):
        super().__init__(config)
        self.wav2vec2_conformer = Wav2Vec2ConformerModel(config)
        self.dropout_features = nn.Dropout(config.feat_quantizer_dropout)

        self.quantizer = Wav2Vec2ConformerGumbelVectorQuantizer(config)

        self.project_hid = nn.Linear(config.hidden_size, config.proj_codevector_dim)
        self.project_q = nn.Linear(config.codevector_dim, config.proj_codevector_dim)

        # Initialize weights and apply final processing
        self.post_init()

    def set_gumbel_temperature(self, temperature: int):
        """
        Set the Gumbel softmax temperature to a given value. Only necessary for training
        """
        self.quantizer.temperature = temperature

    def freeze_feature_encoder(self):
        """
        Calling this function will disable the gradient computation for the feature encoder so that its parameter will
        not be updated during training.
        """
        self.wav2vec2_conformer.feature_extractor._freeze_parameters()

    @staticmethod
    def compute_contrastive_logits(
        target_features: torch.FloatTensor,
        negative_features: torch.FloatTensor,
        predicted_features: torch.FloatTensor,
        temperature: int = 0.1,
    ):
        """
        Compute logits for contrastive loss based using cosine similarity as the distance measure between
        `[positive_feature, negative_features]` and `[predicted_features]`. Additionally, temperature can be applied.
        """
        target_features = torch.cat([target_features, negative_features], dim=0)

        logits = torch.cosine_similarity(predicted_features.float(), target_features.float(), dim=-1).type_as(
            target_features
        )

        # apply temperature
        logits = logits / temperature
        return logits

    @auto_docstring
    def forward(
        self,
        input_values: Optional[torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        mask_time_indices: Optional[torch.BoolTensor] = None,
        sampled_negative_indices: Optional[torch.BoolTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, Wav2Vec2ConformerForPreTrainingOutput]:
        r"""
        mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict
            masked extracted features in *config.proj_codevector_dim* space.
        sampled_negative_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length, num_negatives)`, *optional*):
            Indices indicating which quantized target vectors are used as negative sampled vectors in contrastive loss.
            Required input for pre-training.

        Example:

        ```python
        >>> import torch
        >>> from transformers import AutoFeatureExtractor, Wav2Vec2ConformerForPreTraining
        >>> from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer import _compute_mask_indices, _sample_negative_indices
        >>> from datasets import load_dataset

        >>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2_conformer-base")
        >>> model = Wav2Vec2ConformerForPreTraining.from_pretrained("facebook/wav2vec2_conformer-base")

        >>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
        >>> input_values = feature_extractor(ds[0]["audio"]["array"], return_tensors="pt").input_values  # Batch size 1

        >>> # compute masked indices
        >>> batch_size, raw_sequence_length = input_values.shape
        >>> sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()
        >>> mask_time_indices = _compute_mask_indices(
        ...     shape=(batch_size, sequence_length), mask_prob=0.2, mask_length=2
        ... )
        >>> sampled_negative_indices = _sample_negative_indices(
        ...     features_shape=(batch_size, sequence_length),
        ...     num_negatives=model.config.num_negatives,
        ...     mask_time_indices=mask_time_indices,
        ... )
        >>> mask_time_indices = torch.tensor(data=mask_time_indices, device=input_values.device, dtype=torch.long)
        >>> sampled_negative_indices = torch.tensor(
        ...     data=sampled_negative_indices, device=input_values.device, dtype=torch.long
        ... )

        >>> with torch.no_grad():
        ...     outputs = model(input_values, mask_time_indices=mask_time_indices)

        >>> # compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)
        >>> cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)

        >>> # show that cosine similarity is much higher than random
        >>> cosine_sim[mask_time_indices.to(torch.bool)].mean() > 0.5
        tensor(True)

        >>> # for contrastive loss training model should be put into train mode
        >>> model = model.train()
        >>> loss = model(
        ...     input_values, mask_time_indices=mask_time_indices, sampled_negative_indices=sampled_negative_indices
        ... ).loss
        ```"""

        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if mask_time_indices is not None:
            mask_time_indices = mask_time_indices.to(torch.bool)

        outputs = self.wav2vec2_conformer(
            input_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            mask_time_indices=mask_time_indices,
            return_dict=return_dict,
        )

        # 1. project all transformed features (including masked) to final vq dim
        transformer_features = self.project_hid(outputs[0])

        # 2. quantize all (unmasked) extracted features and project to final vq dim
        extract_features = self.dropout_features(outputs[1])

        if attention_mask is not None:
            # compute reduced attention_mask corresponding to feature vectors
            attention_mask = self._get_feature_vector_attention_mask(
                extract_features.shape[1], attention_mask, add_adapter=False
            )

        quantized_features, codevector_perplexity = self.quantizer(
            extract_features, mask_time_indices=mask_time_indices
        )

        quantized_features = quantized_features.to(self.project_q.weight.dtype)
        quantized_features = self.project_q(quantized_features)

        loss = contrastive_loss = diversity_loss = None
        if sampled_negative_indices is not None:
            batch_size, sequence_length, hidden_size = quantized_features.shape

            # for training, we sample negatives
            # 3. sample K negatives (distractors) quantized states for contrastive loss
            # if attention_mask is passed, make sure that padded feature vectors cannot be sampled
            # sample negative quantized vectors BTC => (BxT)C
            negative_quantized_features = quantized_features.view(-1, hidden_size)[
                sampled_negative_indices.long().view(-1)
            ]
            negative_quantized_features = negative_quantized_features.view(
                batch_size, sequence_length, -1, hidden_size
            ).permute(2, 0, 1, 3)

            # 4. compute logits, corresponding to `logs = sim(c_t, [q_t, \sim{q}_t]) / \kappa`
            # of equation (3) in https://arxiv.org/pdf/2006.11477.pdf
            logits = self.compute_contrastive_logits(
                quantized_features[None, :],
                negative_quantized_features,
                transformer_features,
                self.config.contrastive_logits_temperature,
            )

            # 5. if a negative vector is identical to the positive (i.e. when codebook utilization is low),
            # its cosine similarity will be masked
            neg_is_pos = (quantized_features == negative_quantized_features).all(-1)

            if neg_is_pos.any():
                logits[1:][neg_is_pos] = float("-inf")

            # 6. compute contrastive loss \mathbf{L}_m = cross_entropy(logs) =
            # -log(exp(sim(c_t, q_t)/\kappa) / \sum_{\sim{q}} exp(sim(c_t, \sim{q})/\kappa))
            logits = logits.transpose(0, 2).reshape(-1, logits.size(0))
            target = ((1 - mask_time_indices.long()) * -100).transpose(0, 1).flatten()

            contrastive_loss = nn.functional.cross_entropy(logits.float(), target, reduction="sum")
            # 7. compute diversity loss: \mathbf{L}_d
            num_codevectors = self.config.num_codevectors_per_group * self.config.num_codevector_groups
            diversity_loss = ((num_codevectors - codevector_perplexity) / num_codevectors) * mask_time_indices.sum()

            # 8. \mathbf{L} = \mathbf{L}_m + \alpha * \mathbf{L}_d
            loss = contrastive_loss + self.config.diversity_loss_weight * diversity_loss

        if not return_dict:
            if loss is not None:
                return (loss, transformer_features, quantized_features, codevector_perplexity) + outputs[2:]
            return (transformer_features, quantized_features, codevector_perplexity) + outputs[2:]

        return Wav2Vec2ConformerForPreTrainingOutput(
            loss=loss,
            projected_states=transformer_features,
            projected_quantized_states=quantized_features,
            codevector_perplexity=codevector_perplexity,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            contrastive_loss=contrastive_loss,
            diversity_loss=diversity_loss,
        )


_HIDDEN_STATES_START_POSITION = 2


@auto_docstring(
    custom_intro="""
    Wav2Vec2Conformer Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).
    """
)
class Wav2Vec2ConformerForCTC(Wav2Vec2ConformerPreTrainedModel):
    def __init__(self, config, target_lang: Optional[str] = None):
        r"""
        target_lang (`str`, *optional*):
            Language id of adapter weights. Adapter weights are stored in the format adapter.<lang>.safetensors or
            adapter.<lang>.bin. Only relevant when using an instance of [`UniSpeechSatForCTC`] with adapters. Uses 'eng' by
            default.
        """
        super().__init__(config)

        self.wav2vec2_conformer = Wav2Vec2ConformerModel(config)
        self.dropout = nn.Dropout(config.final_dropout)

        self.target_lang = target_lang

        if config.vocab_size is None:
            raise ValueError(
                f"You are trying to instantiate {self.__class__} with a configuration that "
                "does not define the vocabulary size of the language model head. Please "
                "instantiate the model as follows: `Wav2Vec2ConformerForCTC.from_pretrained(..., vocab_size=vocab_size)`. "
                "or define `vocab_size` of your model's configuration."
            )
        output_hidden_size = (
            config.output_hidden_size if hasattr(config, "add_adapter") and config.add_adapter else config.hidden_size
        )
        self.lm_head = nn.Linear(output_hidden_size, config.vocab_size)

        # Initialize weights and apply final processing
        self.post_init()

    def freeze_feature_encoder(self):
        """
        Calling this function will disable the gradient computation for the feature encoder so that its parameter will
        not be updated during training.
        """
        self.wav2vec2_conformer.feature_extractor._freeze_parameters()

    @auto_docstring
    def forward(
        self,
        input_values: Optional[torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        labels: Optional[torch.Tensor] = None,
    ) -> Union[Tuple, CausalLMOutput]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):
            Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to
            the sequence length of the output logits. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`.
            All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ...,
            config.vocab_size - 1]`.
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if labels is not None and labels.max() >= self.config.vocab_size:
            raise ValueError(f"Label values must be <= vocab_size: {self.config.vocab_size}")

        outputs = self.wav2vec2_conformer(
            input_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        hidden_states = outputs[0]
        hidden_states = self.dropout(hidden_states)

        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            # retrieve loss input_lengths from attention_mask
            attention_mask = (
                attention_mask if attention_mask is not None else torch.ones_like(input_values, dtype=torch.long)
            )
            input_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)

            # assuming that padded tokens are filled with -100
            # when not being attended to
            labels_mask = labels >= 0
            target_lengths = labels_mask.sum(-1)
            flattened_targets = labels.masked_select(labels_mask)

            # ctc_loss doesn't support fp16
            log_probs = nn.functional.log_softmax(logits, dim=-1, dtype=torch.float32).transpose(0, 1)

            with torch.backends.cudnn.flags(enabled=False):
                loss = nn.functional.ctc_loss(
                    log_probs,
                    flattened_targets,
                    input_lengths,
                    target_lengths,
                    blank=self.config.pad_token_id,
                    reduction=self.config.ctc_loss_reduction,
                    zero_infinity=self.config.ctc_zero_infinity,
                )

        if not return_dict:
            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]
            return ((loss,) + output) if loss is not None else output

        return CausalLMOutput(
            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions
        )


@auto_docstring(
    custom_intro="""
    Wav2Vec2Conformer Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like
    SUPERB Keyword Spotting.
    """
)
class Wav2Vec2ConformerForSequenceClassification(Wav2Vec2ConformerPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        if hasattr(config, "add_adapter") and config.add_adapter:
            raise ValueError(
                "Sequence classification does not support the use of Wav2Vec2Conformer adapters (config.add_adapter=True)"
            )
        self.wav2vec2_conformer = Wav2Vec2ConformerModel(config)
        num_layers = config.num_hidden_layers + 1  # transformer layers + input embeddings
        if config.use_weighted_layer_sum:
            self.layer_weights = nn.Parameter(torch.ones(num_layers) / num_layers)
        self.projector = nn.Linear(config.hidden_size, config.classifier_proj_size)
        self.classifier = nn.Linear(config.classifier_proj_size, config.num_labels)

        # Initialize weights and apply final processing
        self.post_init()

    def freeze_feature_encoder(self):
        """
        Calling this function will disable the gradient computation for the feature encoder so that its parameter will
        not be updated during training.
        """
        self.wav2vec2_conformer.feature_extractor._freeze_parameters()

    def freeze_base_model(self):
        """
        Calling this function will disable the gradient computation for the base model so that its parameters will not
        be updated during training. Only the classification head will be updated.
        """
        for param in self.wav2vec2_conformer.parameters():
            param.requires_grad = False

    @auto_docstring
    def forward(
        self,
        input_values: Optional[torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        labels: Optional[torch.Tensor] = None,
    ) -> Union[Tuple, SequenceClassifierOutput]:
        r"""
        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file
            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and
            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """

        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        output_hidden_states = True if self.config.use_weighted_layer_sum else output_hidden_states

        outputs = self.wav2vec2_conformer(
            input_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        if self.config.use_weighted_layer_sum:
            hidden_states = outputs[_HIDDEN_STATES_START_POSITION]
            hidden_states = torch.stack(hidden_states, dim=1)
            norm_weights = nn.functional.softmax(self.layer_weights, dim=-1)
            hidden_states = (hidden_states * norm_weights.view(-1, 1, 1)).sum(dim=1)
        else:
            hidden_states = outputs[0]

        hidden_states = self.projector(hidden_states)
        if attention_mask is None:
            pooled_output = hidden_states.mean(dim=1)
        else:
            padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)
            expand_padding_mask = padding_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])
            hidden_states[~expand_padding_mask] = 0.0
            pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)

        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))

        if not return_dict:
            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]
            return ((loss,) + output) if loss is not None else output

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


@auto_docstring
class Wav2Vec2ConformerForAudioFrameClassification(Wav2Vec2ConformerPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        if hasattr(config, "add_adapter") and config.add_adapter:
            raise ValueError(
                "Audio frame classification does not support the use of Wav2Vec2Conformer adapters (config.add_adapter=True)"
            )
        self.wav2vec2_conformer = Wav2Vec2ConformerModel(config)
        num_layers = config.num_hidden_layers + 1  # transformer layers + input embeddings
        if config.use_weighted_layer_sum:
            self.layer_weights = nn.Parameter(torch.ones(num_layers) / num_layers)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)
        self.num_labels = config.num_labels

        self.init_weights()

    def freeze_feature_encoder(self):
        """
        Calling this function will disable the gradient computation for the feature encoder so that its parameter will
        not be updated during training.
        """
        self.wav2vec2_conformer.feature_extractor._freeze_parameters()

    def freeze_base_model(self):
        """
        Calling this function will disable the gradient computation for the base model so that its parameters will not
        be updated during training. Only the classification head will be updated.
        """
        for param in self.wav2vec2_conformer.parameters():
            param.requires_grad = False

    @auto_docstring
    def forward(
        self,
        input_values: Optional[torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, TokenClassifierOutput]:
        r"""
        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file
            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and
            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """

        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        output_hidden_states = True if self.config.use_weighted_layer_sum else output_hidden_states

        outputs = self.wav2vec2_conformer(
            input_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        if self.config.use_weighted_layer_sum:
            hidden_states = outputs[_HIDDEN_STATES_START_POSITION]
            hidden_states = torch.stack(hidden_states, dim=1)
            norm_weights = nn.functional.softmax(self.layer_weights, dim=-1)
            hidden_states = (hidden_states * norm_weights.view(-1, 1, 1)).sum(dim=1)
        else:
            hidden_states = outputs[0]

        logits = self.classifier(hidden_states)

        loss = None
        if labels is not None:
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), torch.argmax(labels.view(-1, self.num_labels), axis=1))

        if not return_dict:
            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]
            return output

        return TokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


class AMSoftmaxLoss(nn.Module):
    def __init__(self, input_dim, num_labels, scale=30.0, margin=0.4):
        super(AMSoftmaxLoss, self).__init__()
        self.scale = scale
        self.margin = margin
        self.num_labels = num_labels
        self.weight = nn.Parameter(torch.randn(input_dim, num_labels), requires_grad=True)
        self.loss = nn.CrossEntropyLoss()

    def forward(self, hidden_states, labels):
        labels = labels.flatten()
        weight = nn.functional.normalize(self.weight, dim=0)
        hidden_states = nn.functional.normalize(hidden_states, dim=1)
        cos_theta = torch.mm(hidden_states, weight)
        psi = cos_theta - self.margin

        onehot = nn.functional.one_hot(labels, self.num_labels)
        logits = self.scale * torch.where(onehot.bool(), psi, cos_theta)
        loss = self.loss(logits, labels)

        return loss


class TDNNLayer(nn.Module):
    def __init__(self, config, layer_id=0):
        super().__init__()
        self.in_conv_dim = config.tdnn_dim[layer_id - 1] if layer_id > 0 else config.tdnn_dim[layer_id]
        self.out_conv_dim = config.tdnn_dim[layer_id]
        self.kernel_size = config.tdnn_kernel[layer_id]
        self.dilation = config.tdnn_dilation[layer_id]

        self.kernel = nn.Linear(self.in_conv_dim * self.kernel_size, self.out_conv_dim)
        self.activation = nn.ReLU()

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        if is_peft_available():
            from peft.tuners.lora import LoraLayer

        if is_peft_available():
            if isinstance(self.kernel, LoraLayer):
                warnings.warn(
                    "Detected LoRA on TDNNLayer. LoRA weights won't be applied due to optimization. "
                    "You should exclude TDNNLayer from LoRA's target modules.",
                )

        # for backward compatibility, we keep nn.Linear but call F.conv1d for speed up
        hidden_states = hidden_states.transpose(1, 2)
        weight = self.kernel.weight.view(self.out_conv_dim, self.kernel_size, self.in_conv_dim).transpose(1, 2)
        hidden_states = nn.functional.conv1d(hidden_states, weight, self.kernel.bias, dilation=self.dilation)
        hidden_states = hidden_states.transpose(1, 2)

        hidden_states = self.activation(hidden_states)
        return hidden_states


@auto_docstring(
    custom_intro="""
    Wav2Vec2Conformer Model with an XVector feature extraction head on top for tasks like Speaker Verification.
    """
)
class Wav2Vec2ConformerForXVector(Wav2Vec2ConformerPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        self.wav2vec2_conformer = Wav2Vec2ConformerModel(config)
        num_layers = config.num_hidden_layers + 1  # transformer layers + input embeddings
        if config.use_weighted_layer_sum:
            self.layer_weights = nn.Parameter(torch.ones(num_layers) / num_layers)
        self.projector = nn.Linear(config.hidden_size, config.tdnn_dim[0])

        tdnn_layers = [TDNNLayer(config, i) for i in range(len(config.tdnn_dim))]
        self.tdnn = nn.ModuleList(tdnn_layers)

        self.feature_extractor = nn.Linear(config.tdnn_dim[-1] * 2, config.xvector_output_dim)
        self.classifier = nn.Linear(config.xvector_output_dim, config.xvector_output_dim)

        self.objective = AMSoftmaxLoss(config.xvector_output_dim, config.num_labels)

        self.init_weights()

    def freeze_feature_encoder(self):
        """
        Calling this function will disable the gradient computation for the feature encoder so that its parameter will
        not be updated during training.
        """
        self.wav2vec2_conformer.feature_extractor._freeze_parameters()

    def freeze_base_model(self):
        """
        Calling this function will disable the gradient computation for the base model so that its parameters will not
        be updated during training. Only the classification head will be updated.
        """
        for param in self.wav2vec2_conformer.parameters():
            param.requires_grad = False

    def _get_tdnn_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):
        """
        Computes the output length of the TDNN layers
        """

        def _conv_out_length(input_length, kernel_size, stride):
            # 1D convolutional layer output length formula taken
            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html
            return (input_length - kernel_size) // stride + 1

        for kernel_size in self.config.tdnn_kernel:
            input_lengths = _conv_out_length(input_lengths, kernel_size, 1)

        return input_lengths

    @auto_docstring
    def forward(
        self,
        input_values: Optional[torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        labels: Optional[torch.Tensor] = None,
    ) -> Union[Tuple, XVectorOutput]:
        r"""
        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file
            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and
            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """

        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        output_hidden_states = True if self.config.use_weighted_layer_sum else output_hidden_states

        outputs = self.wav2vec2_conformer(
            input_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        if self.config.use_weighted_layer_sum:
            hidden_states = outputs[_HIDDEN_STATES_START_POSITION]
            hidden_states = torch.stack(hidden_states, dim=1)
            norm_weights = nn.functional.softmax(self.layer_weights, dim=-1)
            hidden_states = (hidden_states * norm_weights.view(-1, 1, 1)).sum(dim=1)
        else:
            hidden_states = outputs[0]

        hidden_states = self.projector(hidden_states)

        for tdnn_layer in self.tdnn:
            hidden_states = tdnn_layer(hidden_states)

        # Statistic Pooling
        if attention_mask is None:
            mean_features = hidden_states.mean(dim=1)
            std_features = hidden_states.std(dim=1)
        else:
            feat_extract_output_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(dim=1))
            tdnn_output_lengths = self._get_tdnn_output_lengths(feat_extract_output_lengths)
            mean_features = []
            std_features = []
            for i, length in enumerate(tdnn_output_lengths):
                mean_features.append(hidden_states[i, :length].mean(dim=0))
                std_features.append(hidden_states[i, :length].std(dim=0))
            mean_features = torch.stack(mean_features)
            std_features = torch.stack(std_features)
        statistic_pooling = torch.cat([mean_features, std_features], dim=-1)

        output_embeddings = self.feature_extractor(statistic_pooling)
        logits = self.classifier(output_embeddings)

        loss = None
        if labels is not None:
            loss = self.objective(logits, labels)

        if not return_dict:
            output = (logits, output_embeddings) + outputs[_HIDDEN_STATES_START_POSITION:]
            return ((loss,) + output) if loss is not None else output

        return XVectorOutput(
            loss=loss,
            logits=logits,
            embeddings=output_embeddings,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


__all__ = [
    "Wav2Vec2ConformerForAudioFrameClassification",
    "Wav2Vec2ConformerForCTC",
    "Wav2Vec2ConformerForPreTraining",
    "Wav2Vec2ConformerForSequenceClassification",
    "Wav2Vec2ConformerForXVector",
    "Wav2Vec2ConformerModel",
    "Wav2Vec2ConformerPreTrainedModel",
]
  
class PLBartModel(PLBartPreTrainedModel):
    _tied_weights_keys = ["encoder.embed_tokens.weight", "decoder.embed_tokens.weight"]

    def __init__(self, config: PLBartConfig):
        super().__init__(config)

        padding_idx, vocab_size = config.pad_token_id, config.vocab_size
        embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0
        self.shared = PLBartScaledWordEmbedding(vocab_size, config.d_model, padding_idx, embed_scale=embed_scale)

        self.encoder = PLBartEncoder(config, self.shared)
        self.decoder = PLBartDecoder(config, self.shared)

        self.init_weights()

    def get_input_embeddings(self):
        return self.shared

    def set_input_embeddings(self, value):
        self.shared = value
        self.encoder.embed_tokens = self.shared
        self.decoder.embed_tokens = self.shared

    def _tie_weights(self):
        if self.config.tie_word_embeddings:
            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)
            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)

    def get_encoder(self):
        return self.encoder

    def get_decoder(self):
        return self.decoder

    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.LongTensor] = None,
        decoder_input_ids: Optional[torch.LongTensor] = None,
        decoder_attention_mask: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        decoder_head_mask: Optional[torch.LongTensor] = None,
        cross_attn_head_mask: Optional[torch.Tensor] = None,
        encoder_outputs: Optional[List[torch.FloatTensor]] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.Tensor] = None,
    ) -> Union[Tuple[torch.Tensor], Seq2SeqModelOutput]:
        r"""
        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
            Indices of decoder input sequence tokens in the vocabulary.

            Indices can be obtained using [`AutoTokenizer`] or [`PLBartMultiTokenizer`] depending on the checkpoint.
            See [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.

            [What are decoder input IDs?](../glossary#decoder-input-ids)

            PLBart uses a specific language id token as the starting token for `decoder_input_ids` generation that
            varies according to source and target language, *e.g.* 50003 for *en_XX*, and 50001 for *java*. If
            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
            `past_key_values`).

            For translation and summarization training, `decoder_input_ids` should be provided. If no
            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
            for denoising pre-training following the paper.
        decoder_attention_mask (:
            obj:*torch.LongTensor* of shape `(batch_size, target_sequence_length)`, *optional*):
            Default behavior:
            generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also be used by default.
        cross_attn_head_mask (:
            obj:*torch.Tensor* of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
            Mask to nullify
            selected heads of the cross-attention modules in the decoder. Mask values selected in `[0, 1]`:

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        """
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # different to other models, PLBart automatically creates decoder_input_ids from
        # input_ids if no decoder_input_ids are provided
        if decoder_input_ids is None and decoder_inputs_embeds is None:
            decoder_input_ids = shift_tokens_right(input_ids, self.config.pad_token_id)

        if encoder_outputs is None:
            encoder_outputs = self.encoder(
                input_ids=input_ids,
                attention_mask=attention_mask,
                head_mask=head_mask,
                inputs_embeds=inputs_embeds,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
            )
        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True
        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):
            encoder_outputs = BaseModelOutput(
                last_hidden_state=encoder_outputs[0],
                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,
                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,
            )

        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)
        decoder_outputs = self.decoder(
            input_ids=decoder_input_ids,
            attention_mask=decoder_attention_mask,
            encoder_hidden_states=encoder_outputs[0],
            encoder_attention_mask=attention_mask,
            head_mask=decoder_head_mask,
            cross_attn_head_mask=cross_attn_head_mask,
            past_key_values=past_key_values,
            inputs_embeds=decoder_inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )

        if not return_dict:
            return decoder_outputs + encoder_outputs

        return Seq2SeqModelOutput(
            last_hidden_state=decoder_outputs.last_hidden_state,
            past_key_values=decoder_outputs.past_key_values,
            decoder_hidden_states=decoder_outputs.hidden_states,
            decoder_attentions=decoder_outputs.attentions,
            cross_attentions=decoder_outputs.cross_attentions,
            encoder_last_hidden_state=encoder_outputs.last_hidden_state,
            encoder_hidden_states=encoder_outputs.hidden_states,
            encoder_attentions=encoder_outputs.attentions,
        )


@auto_docstring(
    custom_intro="""
    The PLBART Model with a language modeling head. Can be used for code-to-text, text-to-code and code-to-code.
    """
)
class PLBartForConditionalGeneration(PLBartPreTrainedModel, GenerationMixin):
    base_model_prefix = "model"
    _keys_to_ignore_on_load_missing = ["final_logits_bias"]
    _tied_weights_keys = ["encoder.embed_tokens.weight", "decoder.embed_tokens.weight", "lm_head.weight"]

    def __init__(self, config: PLBartConfig):
        super().__init__(config)
        self.model = PLBartModel(config)
        self.register_buffer("final_logits_bias", torch.zeros((1, self.model.shared.num_embeddings)))
        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)

        self.init_weights()

    def get_encoder(self):
        return self.model.get_encoder()

    def get_decoder(self):
        return self.model.get_decoder()

    def resize_token_embeddings(
        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True
    ) -> nn.Embedding:
        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)
        self._resize_final_logits_bias(new_embeddings.weight.shape[0])
        return new_embeddings

    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:
        old_num_tokens = self.final_logits_bias.shape[-1]
        if new_num_tokens <= old_num_tokens:
            new_bias = self.final_logits_bias[:, :new_num_tokens]
        else:
            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)
            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)
        self.register_buffer("final_logits_bias", new_bias)

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.LongTensor] = None,
        decoder_input_ids: Optional[torch.LongTensor] = None,
        decoder_attention_mask: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        decoder_head_mask: Optional[torch.LongTensor] = None,
        cross_attn_head_mask: Optional[torch.Tensor] = None,
        encoder_outputs: Optional[List[torch.FloatTensor]] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.Tensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.Tensor] = None,
    ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:
        r"""
        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
            Indices of decoder input sequence tokens in the vocabulary.

            Indices can be obtained using [`AutoTokenizer`] or [`PLBartMultiTokenizer`] depending on the checkpoint.
            See [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.

            [What are decoder input IDs?](../glossary#decoder-input-ids)

            PLBart uses a specific language id token as the starting token for `decoder_input_ids` generation that
            varies according to source and target language, *e.g.* 50003 for *en_XX*, and 50001 for *java*. If
            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
            `past_key_values`).

            For translation and summarization training, `decoder_input_ids` should be provided. If no
            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
            for denoising pre-training following the paper.
        decoder_attention_mask (:
            obj:*torch.LongTensor* of shape `(batch_size, target_sequence_length)`, *optional*):
            Default behavior:
            generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also be used by default.
        cross_attn_head_mask (:
            obj:*torch.Tensor* of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
            Mask to nullify
            selected heads of the cross-attention modules in the decoder. Mask values selected in `[0, 1]`:

            - 1 indicates the head is **not masked**,
        ÈØÔ   ÆQ     ÿÿÿÿÿÿÿÿdd is **#                 labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

        Example Mask-filling:

        ```python
        >>> from transformers import AutoTokenizer, PLBartForConditionalGeneration

        >>> model = PLBartForConditionalGeneration.from_pretrained("uclanlp/plbart-base")
        >>> tokenizer = AutoTokenizer.from_pretrained("uclanlp/plbart-base")

        >>> # en_XX is the language symbol id <LID> for English
        >>> TXT = "<s> Is 0 the <mask> Fibonacci number ? </s> en_XX"
        >>> input_ids = tokenizer([TXT], add_special_tokens=False, return_tensors="pt").input_ids

        >>> logits = model(input_ids).logits
        >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
        >>> probs = logits[0, masked_index].softmax(dim=0)
        >>> values, predictions = probs.topk(5)

        >>> tokenizer.decode(predictions).split()
        ['first', 'same', 'highest', 'result', 'number']
        ```
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if labels is not None:
            if decoder_input_ids is None and decoder_inputs_embeds is None:
                decoder_input_ids = shift_tokens_right(labels, self.config.pad_token_id)

        outputs = self.model(
            input_ids,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            encoder_outputs=encoder_outputs,
            decoder_attention_mask=decoder_attention_mask,
            head_mask=head_mask,
            decoder_head_mask=decoder_head_mask,
            cross_attn_head_mask=cross_attn_head_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            decoder_inputs_embeds=decoder_inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        lm_logits = self.lm_head(outputs[0])
        lm_logits = lm_logits + self.final_logits_bias.to(lm_logits.device)

        masked_lm_loss = None
        if labels is not None:
            loss_fct = CrossEntropyLoss()
            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))

        if not return_dict:
            output = (lm_logits,) + outputs[1:]
            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output

        return Seq2SeqLMOutput(
            loss=masked_lm_loss,
            logits=lm_logits,
            past_key_values=outputs.past_key_values,
            decoder_hidden_states=outputs.decoder_hidden_states,
            decoder_attentions=outputs.decoder_attentions,
            cross_attentions=outputs.cross_attentions,
            encoder_last_hidden_state=outputs.encoder_last_hidden_state,
            encoder_hidden_states=outputs.encoder_hidden_states,
            encoder_attentions=outputs.encoder_attentions,
        )

    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):
        return shift_tokens_right(labels, self.config.pad_token_id)

    @staticmethod
    def _reorder_cache(past_key_values, beam_idx):
        reordered_past = ()
        for layer_past in past_key_values:
            # cached cross_attention states don't have to be reordered -> they are always the same
            reordered_past += (
                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])
                + layer_past[2:],
            )
        return reordered_past


@auto_docstring(
    custom_intro="""
    PLBart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for code
    classification.
    """
)
class PLBartForSequenceClassification(PLBartPreTrainedModel):
    _tied_weights_keys = ["encoder.embed_tokens.weight", "decoder.embed_tokens.weight"]

    def __init__(self, config: PLBartConfig, **kwargs):
        super().__init__(config, **kwargs)
        self.model = PLBartModel(config)
        self.classification_head = PLBartClassificationHead(
            config.d_model,
            config.d_model,
            config.num_labels,
            config.classifier_dropout,
        )

        # Initialize weights and apply final processing
        self.post_init()

    @auto_docstring
    # Ignore copy
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        decoder_input_ids: Optional[torch.LongTensor] = None,
        decoder_attention_mask: Optional[torch.LongTensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        decoder_head_mask: Optional[torch.Tensor] = None,
        cross_attn_head_mask: Optional[torch.Tensor] = None,
        encoder_outputs: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:
        r"""
        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
            Indices of decoder input sequence tokens in the vocabulary.

            Indices can be obtained using [`AutoTokenizer`] or [`PLBartMultiTokenizer`] depending on the checkpoint.
            See [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.

            [What are decoder input IDs?](../glossary#decoder-input-ids)

            PLBart uses a specific language id token as the starting token for `decoder_input_ids` generation that
            varies according to source and target language, *e.g.* 50003 for *en_XX*, and 50001 for *java*. If
            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
            `past_key_values`).

            For translation and summarization training, `decoder_input_ids` should be provided. If no
            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
            for denoising pre-training following the paper.
        decoder_attention_mask (:
            obj:*torch.LongTensor* of shape `(batch_size, target_sequence_length)`, *optional*):
            Default behavior:
            generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also be used by default.
        cross_attn_head_mask (:
            obj:*torch.Tensor* of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
            Mask to nullify
            selected heads of the cross-attention modules in the decoder. Mask values selected in `[0, 1]`:

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        if labels is not None:
            use_cache = False

        if input_ids is None and inputs_embeds is not None:
            raise NotImplementedError(
                f"Passing input embeddings is currently not supported for {self.__class__.__name__}"
            )

        outputs = self.model(
            input_ids,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
            head_mask=head_mask,
            decoder_head_mask=decoder_head_mask,
            cross_attn_head_mask=cross_attn_head_mask,
            encoder_outputs=encoder_outputs,
            inputs_embeds=inputs_embeds,
            decoder_inputs_embeds=decoder_inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]  # last hidden state

        eos_mask = input_ids.eq(self.config.eos_token_id).to(hidden_states.device)

        if len(torch.unique_consecutive(eos_mask.sum(1))) > 1:
            raise ValueError("All examples must have the same number of <eos> tokens.")
        sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[
            :, -1, :
        ]
        logits = self.classification_head(sentence_representation)

        loss = None
        if labels is not None:
            labels = labels.to(logits.device)
            if self.config.problem_type is None:
                if self.config.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.config.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "regression":
                loss_fct = MSELoss()
                if self.config.num_labels == 1:
                    loss = loss_fct(logits.squeeze(), labels.squeeze())
                else:
                    loss = loss_fct(logits, labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)
        if not return_dict:
            output = (logits,) + outputs[1:]
            return ((loss,) + output) if loss is not None else output

        return Seq2SeqSequenceClassifierOutput(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            decoder_hidden_states=outputs.decoder_hidden_states,
            decoder_attentions=outputs.decoder_attentions,
            cross_attentions=outputs.cross_attentions,
            encoder_last_hidden_state=outputs.encoder_last_hidden_state,
            encoder_hidden_states=outputs.encoder_hidden_states,
            encoder_attentions=outputs.encoder_attentions,
        )


# Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->PLBart
class PLBartDecoderWrapper(PLBartPreTrainedModel):
    """
    This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is
    used in combination with the [`EncoderDecoderModel`] framework.
    """

    def __init__(self, config):
        super().__init__(config)
        self.decoder = PLBartDecoder(config)

    def forward(self, *args, **kwargs):
        return self.decoder(*args, **kwargs)


# Copied from transformers.models.bart.modeling_bart.BartForCausalLM with Bart->PLBart, facebook/bart-base->uclanlp/plbart-base
class PLBartForCausalLM(PLBartPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config):
        config = copy.deepcopy(config)
        config.is_decoder = True
        config.is_encoder_decoder = False
        super().__init__(config)
        self.model = PLBartDecoderWrapper(config)

        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.decoder.embed_tokens

    def set_input_embeddings(self, value):
        self.model.decoder.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model.decoder = decoder

    def get_decoder(self):
        return self.model.decoder

    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        encoder_hidden_states: Optional[torch.FloatTensor] = None,
        encoder_attention_mask: Optional[torch.FloatTensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        cross_attn_head_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:
        r"""
        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

        Example:

        ```python
        >>> from transformers import AutoTokenizer, PLBartForCausalLM

        >>> tokenizer = AutoTokenizer.from_pretrained("uclanlp/plbart-base")
        >>> model = PLBartForCausalLM.from_pretrained("uclanlp/plbart-base", add_cross_attention=False)
        >>> assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
        >>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
        >>> outputs = model(**inputs)

        >>> logits = outputs.logits
        >>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]
        >>> list(logits.shape) == expected_shape
        True
        ```"""

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        outputs = self.model.decoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            head_mask=head_mask,
            cross_attn_head_mask=cross_attn_head_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )

        logits = self.lm_head(outputs[0])

        loss = None
        if labels is not None:
            labels = labels.to(logits.device)
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))

        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return CausalLMOutputWithCrossAttentions(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            cross_attentions=outputs.cross_attentions,
        )

    @staticmethod
    def _reorder_cache(past_key_values, beam_idx):
        reordered_past = ()
        for layer_past in past_key_values:
            reordered_past += (
                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),
            )
        return reordered_past


__all__ = [
    "PLBartForCausalLM",
    "PLBartForConditionalGeneration",
    "PLBartForSequenceClassification",
    "PLBartModel",
    "PLBartPreTrainedModel",
]
  s ) : 
                 s u p e r ( ) . _ _ i n i t _ _ ( c o n f i g ,   * i n p u t s ,   * * k w a r g s ) 
 
                 i f   c o n f i g . i s _ d e c o d e r : 
                         l o g g e r . w a r n i n g ( 
                                 " I f   y o u   w a n t   t o   u s e   ` T F R o F o r m e r F o r M a s k e d L M `   m a k e   s u r e   ` c o n f i g . i s _ d e c o d e r = F a l s e `   f o r   " 
                                 " b i - d i r e c t i o n a l   s e l f - a t t e n t i o n . " 
                         ) 
 
                 s e l f . r o f o r m e r   =   T F R o F o r m e r M a i n L a y e r ( c o n f i g ,   n a m e = " r o f o r m e r " ) 
                 s e l f . m l m   =   T F R o F o r m e r M L M H e a d ( c o n f i g ,   i n p u t _ e m b e d d i n g s = s e l f . r o f o r m e r . e m b e d d i n g s ,   n a m e = " m l m _ _ _ c l s " ) 
 
         d e f   g e t _ l m _ h e a d ( s e l f )   - >   k e r a s . l a y e r s . L a y e r : 
                 r e t u r n   s e l f . m l m . p r e d i c t i o n s 
 
         @ u n p a c k _ i n p u t s 
         @ a d d _ s t a r t _ d o c s t r i n g s _ t o _ m o d e l _ f o r w a r d ( R O F O R M E R _ I N P U T S _ D O C S T R I N G . f o r m a t ( " b a t c h _ s i z e ,   s e q u e n c e _ l e n g t h " ) ) 
         @ a d d _ c o d e _ s a m p l e _ d o c s t r i n g s ( 
                 c h e c k p o i n t = _ C H E C K P O I N T _ F O R _ D O C , 
                 o u t p u t _ t y p e = T F M a s k e d L M O u t p u t , 
                 c o n f i g _ c l a s s = _ C O N F I G _ F O R _ D O C , 
         ) 
         d e f   c a l l ( 
                 s e l f , 
                 i n p u t _ i d s :   T F M o d e l I n p u t T y p e   |   N o n e   =   N o n e , 
                 a t t e n t i o n _ m a s k :   n p . n d a r r a y   |   t f . T e n s o r   |   N o n e   =   N o n e , 
                 t o k e n _ t y p e _ i d s :   n p . n d a r r a y   |   t f . T e n s o r   |   N o n e   =   N o n e , 
                 h e a d _ m a s k :   n p . n d a r r a y   |   t f . T e n s o r   |   N o n e   =   N o n e , 
                 i n p u t s _ e m b e d s :   n p . n d a r r a y   |   t f . T e n s o r   |   N o n e   =   N o n e , 
                 o u t p u t _ a t t e n t i o n s :   O p t i o n a l [ b o o l ]   =   N o n e , 
                 o u t p u t _ h i d d e n _ s t a t e s :   O p t i o n a l [ b o o l ]   =   N o n e , 
                 r e t u r n _ d i c t :   O p t i o n a l [ b o o l ]   =   N o n e , 
                 l a b e l s :   n p . n d a r r a y   |   t f . T e n s o r   |   N o n e   =   N o n e , 
                 t r a i n i n g :   O p t i o n a l [ b o o l ]   =   F a l s e , 
         )   - >   U n i o n [ T F M a s k e d L M O u t p u t ,   T u p l e [ t f . T e n s o r ] ] : 
                 r " " " 
                 l a b e l s   ( ` t f . T e n s o r `   o r   ` n p . n d a r r a y `   o f   s h a p e   ` ( b a t c h _ s i z e ,   s e q u e n c e _ l e n g t h ) ` ,   * o p t i o n a l * ) : 
                         L a b e l s   f o r   c o m p u t i n g   t h e   m a s k e d   l a n g u a g e   m o d e l i n g   l o s s .   I n d i c e s   s h o u l d   b e   i n   ` [ - 1 0 0 ,   0 ,   . . . , 
                         c o n f i g . v o c a b _ s i z e ] `   ( s e e   ` i n p u t _ i d s `   d o c s t r i n g )   T o k e n s   w i t h   i n d i c e s   s e t   t o   ` - 1 0 0 `   a r e   i g n o r e d   ( m a s k e d ) ,   t h e 
                         l o s s   i s   o n l y   c o m p u t e d   f o r   t h e   t o k e n s   w i t h   l a b e l s   i n   ` [ 0 ,   . . . ,   c o n f i g . v o c a b _ s i z e ] ` 
                 " " " 
                 o u t p u t s   =   s e l f . r o f o r m e r ( 
                         i n p u t _ i d s = i n p u t _ i d s , 
                         a t t e n t i o n _ m a s k = a t t e n t i o n _ m a s k , 
                         t o k e n _ t y p e _ i d s = t o k e n _ t y p e _ i d s , 
                         h e a d _ m a s k = h e a d _ m a s k , 
                         i n p u t s _ e m b e d s = i n p u t s _ e m b e d s , 
                         o u t p u t _ a t t e n t i o n s = o u t p u t _ a t t e n t i o n s , 
                         o u t p u t _ h i d d e n _ s t a t e s = o u t p u t _ h i d d e n _ s t a t e s , 
                         r e t u r n _ d i c t = r e t u r n _ d i c t , 
                         t r a i n i n g = t r a i n i n g , 
                 ) 
                 s e q u e n c e _ o u t p u t   =   o u t p u t s [ 0 ] 
                 p r e d i c t i o n _ s c o r e s   =   s e l f . m l m ( s e q u e n c e _ o u t p u t = s e q u e n c e _ o u t p u t ,   t r a i n i n g = t r a i n i n g ) 
                 l o s s   =   N o n e   i f   l a b e l s   i s   N o n e   e l s e   s e l f . h f _ c o m p u t e _ l o s s ( l a b e l s = l a b e l s ,   l o g i t s = p r e d i c t i o n _ s c o r e s ) 
 
                 i f   n o t   r e t u r n _ d i c t : 
                         o u t p u t   =   ( p r e d i c t i o n _ s c o r e s , )   +   o u t p u t s [ 2 : ] 
                         r e t u r n   ( ( l o s s , )   +   o u t p u t )   i f   l o s s   i s   n o t   N o n e   e l s e   o u t p u t 
 
                 r e t u r n   T F M a s k e d L M O u t p u t ( 
                         l o s s = l o s s , 
                         l o g i t s = p r e d i c t i o n _ s c o r e s , 
                         h i d d e n _ s t a t e s = o u t p u t s . h i d d e n _ s t a t e s , 
                         a t t e n t i o n s = o u t p u t s . a t t e n t i o n s , 
                 ) 
 
         d e f   b u i l d ( s e l f ,   i n p u t _ s h a p e = N o n e ) : 
                 i f   s e l f . b u i l t : 
                         r e t u r n 
                 s e l f . b u i l t   =   T r u e 
                 i f   g e t a t t r ( s e l f ,   " r o f o r m e r " ,   N o n e )   i s   n o t   N o n e : 
                         w i t h   t f . n a m e _ s c o p e ( s e l f . r o f o r m e r . n a m e ) : 
                                 s e l f . r o f o r m e r . b u i l d ( N o n e ) 
                 i f   g e t a t t r ( s e l f ,   " m l m " ,   N o n e )   i s   n o t   N o n e : 
                         w i t h   t f . n a m e _ s c o p e ( s e l f . m l m . n a m e ) : 
                                 s e l f . m l m . b u i l d ( N o n e ) 
 
 
 @ a d d _ s t a r t _ d o c s t r i n g s ( 
         " " " R o F o r m e r   M o d e l   w i t h   a   ` l a n g u a g e   m o d e l i n g `   h e a d   o n   t o p   f o r   C L M   f i n e - t u n i n g . " " " ,   R O F O R M E R _ S T A R T _ D O C S T R I N G 
 ) 
 c l a s s   T F R o F o r m e r F o r C a u s a l L M ( T F R o F o r m e r P r e T r a i n e d M o d e l ,   T F C a u s a l L a n g u a g e M o d e l i n g L o s s ) : 
         d e f   _ _ i n i t _ _ ( s e l f ,   c o n f i g :   R o F o r m e r C o n f i g ,   * i n p u t s ,   * * k w a r g s ) : 
                 s u p e r ( ) . _ _ i n i t _ _ ( c o n f i g ,   * i n p u t s ,   * * k w a r g s ) 
 
                 i f   n o t   c o n f i g . i s _ d e c o d e r : 
                         l o g g e r . w a r n i n g ( " I f   y o u   w a n t   t o   u s e   ` T F R o F o r m e r F o r C a u s a l L M `   a s   a   s t a n d a l o n e ,   a d d   ` i s _ d e c o d e r = T r u e . ` " ) 
 
                 s e l f . r o f o r m e r   =   T F R o F o r m e r M a i n L a y e r ( c o n f i g ,   n a m e = " r o f o r m e r " ) 
                 s e l f . m l m   =   T F R o F o r m e r M L M H e a d ( c o n f i g ,   i n p u t _ e m b e d d i n g s = s e l f . r o f o r m e r . e m b e d d i n g s ,   n a m e = " m l m _ _ _ c l s " ) 
 
         d e f   g e t _ l m _ h e a d ( s e l f )   - >   k e r a s . l a y e r s . L a y e r : 
                 r e t u r n   s e l f . m l m . p r e d i c t i o n s 
 
         @ u n p a c k _ i n p u t s 
         @ a d d _ c o d e _ s a m p l e _ d o c s t r i n g s ( 
                 c h e c k p o i n t = _ C H E C K P O I N T _ F O R _ D O C , 
                 o u t p u t _ t y p e = T F C a u s a l L M O u t p u t , 
                 c o n f i g _ c l a s s = _ C O N F I G _ F O R _ D O C , 
         ) 
         d e f   c a l l ( 
                 s e l f , 
                 i n p u t _ i d s :   T F M o d e l I n p u t T y p e   |   N o n e   =   N o n e , 
                 a t t e n t i o n _ m a s k :   n p . n d a r r a y   |   t f . T e n s o r   |   N o n e   =   N o n e , 
                 t o k e n _ t y p e _ i d s :   n p . n d a r r a y   |   t f . T e n s o r   |   N o n e   =   N o n e , 
                 h e a d _ m a s k :   n p . n d a r r a y   |   t f . T e n s o r   |   N o n e   =   N o n e , 
                 i n p u t s _ e m b e d s :   n p . n d a r r a y   |   t f . T e n s o r   |   N o n e   =   N o n e , 
                 o u t p u t _ a t t e n t i o n s :   O p t i o n a l [ b o o l ]   =   N o n e , 
                 o u t p u t _ h i d d e n _ s t a t e s :   O p t i o n a l [ b o o l ]   =   N o n e , 
                 r e t u r n _ d i c t :   O p t i o n a l [ b o o l ]   =   N o n e , 
                 l a b e l s :   n p . n d a r r a y   |   t f . T e n s o r   |   N o n e   =   N o n e , 
                 t r a i n i n g :   O p t i o n a l [ b o o l ]   =   F a l s e , 
         )   - >   U n i o n [ T F C a u s a l L M O u t p u t ,   T u p l e [ t f . T e n s o r ] ] : 
                 r " " " 
                 l a b e l s   ( ` t f . T e n s o r `   o r   ` n p . n d a r r a y `   o f   s h a p e   ` ( b a t c h _ s i z e ,   s e q u e n c e _ l e n g t h ) ` ,   * o p t i o n a l * ) : 
                         L a b e l s   f o r   c o m p u t i n g   t h e   c r o s s   e n t r o p y   c l a s s i f i c a t i o n   l o s s .   I n d i c e s   s h o u l d   b e   i n   ` [ 0 ,   . . . , 
                         c o n f i g . v o c a b _ s i z e   -   1 ] ` . 
                 " " " 
                 o u t p u t s   =   s e l f . r o f o r m e r ( 
                         i n p u t _ i d s = i n p u t _ i d s , 
                         a t t e n t i o n _ m a s k = a t t e n t i o n _ m a s k , 
                         t o k e n _ t y p e _ i d s = t o k e n _ t y p e _ i d s , 
                         h e a d _ m a s k = h e a d _ m a s k , 
                         i n p u t s _ e m b e d s = i n p u t s _ e m b e d s , 
                         o u t p u t _ a t t e n t i o n s = o u t p u t _ a t t e n t i o n s , 
                         o u t p u t _ h i d d e n _ s t a t e s = o u t p u t _ h i d d e n _ s t a t e s , 
                         r e t u r n _ d i c t = r e t u r n _ d i c t , 
                         t r a i n i n g = t r a i n i n g , 
                 ) 
                 s e q u e n c e _ o u t p u t   =   o u t p u t s [ 0 ] 
                 l o g i t s   =   s e l f . m l m ( s e q u e n c e _ o u t p u t = s e q u e n c e _ o u t p u t ,   t r a i n i n g = t r a i n i n g ) 
                 l o s s   =   N o n e 
 
                 i f   l a b e l s   i s   n o t   N o n e : 
                         #   s h i f t   l a b e l s   t o   t h e   l e f t   a n d   c u t   l a s t   l o g i t   t o k e n 
                         s h i f t e d _ l o g i t s   =   l o g i t s [ : ,   : - 1 ] 
                         l a b e l s   =   l a b e l s [ : ,   1 : ] 
                         l o s s   =   s e l f . h f _ c o m p u t e _ l o s s ( l a b e l s = l a b e l s ,   l o g i t s = s h i f t e d _ l o g i t s ) 
 
                 i f   n o t   r e t u r n _ d i c t : 
                         o u t p u t   =   ( l o g i t s , )   +   o u t p u t s [ 2 : ] 
                         r e t u r n   ( ( l o s s , )   +   o u t p u t )   i f   l o s s   i s   n o t   N o n e   e l s e   o u t p u t 
 
                 r e t u r n   T F C a u s a l L M O u t p u t ( 
                         l o s s = l o s s , 
                         l o g i t s = l o g i t s , 
                         h i d d e n _ s t a t e s = o u t p u t s . h i d d e n _ s t a t e s , 
                         a t t e n t i o n s = o u t p u t s . a t t e n t i o n s , 
                 ) 
 
         d e f   b u i l d ( s e l f ,   i n p u t _ s h a p e = N o n e ) : 
                 i f   s e l f . b u i l t : 
                         r e t u r n 
                 s e l f . b u i l t   =   T r u e 
                 i f   g e t a t t r ( s e l f ,   " r o f o r m e r " ,   N o n e )   i s   n o t   N o n e : 
                         w i t h   t f . n a m e _ s c o p e ( s e l f . r o f o r m e r . n a m e ) : 
                                 s e l f . r o f o r m e r . b u i l d ( N o n e ) 
                 i f   g e t a t t r ( s e l f ,   " m l m " ,   N o n e )   i s   n o t   N o n e : 
                         w i t h   t f . n a m e _ s c o p e ( s e l f . m l m . n a m e ) : 
                                 s e l f . m l m . b u i l d ( N o n e ) 
 
 
 c l a s s   T F R o F o r m e r C l a s s i f i c a t i o n H e a d ( k e r a s . l a y e r s . L a y e r ) : 
         " " " H e a d   f o r   s e n t e n c e - l e v e l   c l a s s i f i c a t i o n   t a s k s . " " " 
 
         d e f   _ _ i n i t _ _ ( s e l f ,   c o n f i g :   R o F o r m e r C o n f i g ,   * i n p u t s ,   * * k w a r g s ) : 
                 s u p e r ( ) . _ _ i n i t _ _ ( * i n p u t s ,   * * k w a r g s ) 
 
                 s e l f . d e n s e   =   k e r a s . l a y e r s . D e n s e ( 
                         u n i t s = c o n f i g . h i d d e n _ s i z e ,   k e r n e l _ i n i t i a l i z e r = g e t _ i n i t i a l i z e r ( c o n f i g . i n i t i a l i z e r _ r a n g e ) ,   n a m e = " d e n s e " 
                 ) 
                 s e l f . d r o p o u t   =   k e r a s . l a y e r s . D r o p o u t ( r a t e = c o n f i g . h i d d e n _ d r o p o u t _ p r o b ) 
                 s e l f . o u t _ p r o j   =   k e r a s . l a y e r s . D e n s e ( 
                         u n i t s = c o n f i g . n u m _ l a b e l s ,   k e r n e l _ i n i t i a l i z e r = g e t _ i n i t i a l i z e r ( c o n f i g . i n i t i a l i z e r _ r a n g e ) ,   n a m e = " o u t _ p r o j " 
                 ) 
 
                 i f   i s i n s t a n c e ( c o n f i g . h i d d e n _ a c t ,   s t r ) : 
                         s e l f . c l a s s i f i e r _ a c t _ f n   =   g e t _ t f _ a c t i v a t i o n ( c o n f i g . h i d d e n _ a c t ) 
                 e l s e : 
                         s e l f . c l a s s i f i e r _ a c t _ f n   =   c o n f i g . h i d d e n _ a c t 
                 s e l f . c o n f i g   =   c o n f i g 
 
         d e f   c a l l ( s e l f ,   h i d d e n _ s t a t e s :   t f . T e n s o r ,   t r a i n i n g :   b o o l   =   F a l s e )   - >   t f         ÈØÔ   µ     ÿÿÿÿÿÿÿÿ0 h i d                 #                                                                   ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö 
   #                                               T   h   i   s       f   i   l   e       w   a   s       a   u   t   o   m   a   t   i   c   a   l   l   y       g   e   n   e   r   a   t   e   d       f   r   o   m       s   r   c   /   t   r   a   n   s   f   o   r   m   e   r   s   /   m   o   d   e   l   s   /   d   _   f   i   n   e   /   m   o   d   u   l   a   r   _   d   _   f   i   n   e   .   p   y   .   
   #                                                               D   o       N   O   T       e   d   i   t       t   h   i   s       f   i   l   e       m   a   n   u   a   l   l   y       a   s       a   n   y       e   d   i   t   s       w   i   l   l       b   e       o   v   e   r   w   r   i   t   t   e   n       b   y       t   h   e       g   e   n   e   r   a   t   i   o   n       o   f   
   #                                                       t   h   e       f   i   l   e       f   r   o   m       t   h   e       m   o   d   u   l   a   r   .       I   f       a   n   y       c   h   a   n   g   e       s   h   o   u   l   d       b   e       d   o   n   e   ,       p   l   e   a   s   e       a   p   p   l   y       t   h   e       c   h   a   n   g   e       t   o       t   h   e   
   #                                                                                                           m   o   d   u   l   a   r   _   d   _   f   i   n   e   .   p   y       f   i   l   e       d   i   r   e   c   t   l   y   .       O   n   e       o   f       o   u   r       C   I       e   n   f   o   r   c   e   s       t   h   i   s   .   
   #                                                                   ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö ¨ö 
   #       c   o   d   i   n   g   =   u   t   f   -   8   
   #       C   o   p   y   r   i   g   h   t       2   0   2   5       B   a   i   d   u       I   n   c       a   n   d       T   h   e       H   u   g   g   i   n   g   F   a   c   e       I   n   c   .       t   e   a   m   .   
   #   
   #       L   i   c   e   n   s   e   d       u   n   d   e   r       t   h   e       A   p   a   c   h   e       L   i   c   e   n   s   e   ,       V   e   r   s   i   o   n       2   .   0       (   t   h   e       "   L   i   c   e   n   s   e   "   )   ;   
   #       y   o   u       m   a   y       n   o   t       u   s   e       t   h   i   s       f   i   l   e       e   x   c   e   p   t       i   n       c   o   m   p   l   i   a   n   c   e       w   i   t   h       t   h   e       L   i   c   e   n   s   e   .   
   #       Y   o   u       m   a   y       o   b   t   a   i   n       a       c   o   p   y       o   f       t   h   e       L   i   c   e   n   s   e       a   t   
   #   
   #                       h   t   t   p   :   /   /   w   w   w   .   a   p   a   c   h   e   .   o   r   g   /   l   i   c   e   n   s   e   s   /   L   I   C   E   N   S   E   -   2   .   0   
   #   
   #       U   n   l   e   s   s       r   e   q   u   i   r   e   d       b   y       a   p   p   l   i   c   a   b   l   e       l   a   w       o   r       a   g   r   e   e   d       t   o       i   n       w   r   i   t   i   n   g   ,       s   o   f   t   w   a   r   e   
   #       d   i   s   t   r   i   b   u   t   e   d       u   n   d   e   r       t   h   e       L   i   c   e   n   s   e       i   s       d   i   s   t   r   i   b   u   t   e   d       o   n       a   n       "   A   S       I   S   "       B   A   S   I   S   ,   
   #       W   I   T   H   O   U   T       W   A   R   R   A   N   T   I   E   S       O   R       C   O   N   D   I   T   I   O   N   S       O   F       A   N   Y       K   I   N   D   ,       e   i   t   h   e   r       e   x   p   r   e   s   s       o   r       i   m   p   l   i   e   d   .   
   #       S   e   e       t   h   e       L   i   c   e   n   s   e       f   o   r       t   h   e       s   p   e   c   i   f   i   c       l   a   n   g   u   a   g   e       g   o   v   e   r   n   i   n   g       p   e   r   m   i   s   s   i   o   n   s       a   n   d   
   #       l   i   m   i   t   a   t   i   o   n   s       u   n   d   e   r       t   h   e       L   i   c   e   n   s   e   .   
   i   m   p   o   r   t       m   a   t   h   
   f   r   o   m       d   a   t   a   c   l   a   s   s   e   s       i   m   p   o   r   t       d   a   t   a   c   l   a   s   s   
   f   r   o   m       t   y   p   i   n   g       i   m   p   o   r   t       A   n   y   ,       D   i   c   t   ,       L   i   s   t   ,       O   p   t   i   o   n   a   l   ,       T   u   p   l   e   ,       U   n   i   o   n   
   
   i   m   p   o   r   t       t   o   r   c   h   
   i   m   p   o   r   t       t   o   r   c   h   .   n   n   .   f   u   n   c   t   i   o   n   a   l       a   s       F   
   i   m   p   o   r   t       t   o   r   c   h   .   n   n   .   i   n   i   t       a   s       i   n   i   t   
   f   r   o   m       t   o   r   c   h       i   m   p   o   r   t       T   e   n   s   o   r   ,       n   n   
   
   f   r   o   m       .   .   .   a   c   t   i   v   a   t   i   o   n   s       i   m   p   o   r   t       A   C   T   2   C   L   S   ,       A   C   T   2   F   N   
   f   r   o   m       .   .   .   i   m   a   g   e   _   t   r   a   n   s   f   o   r   m   s       i   m   p   o   r   t       c   e   n   t   e   r   _   t   o   _   c   o   r   n   e   r   s   _   f   o   r   m   a   t   ,       c   o   r   n   e   r   s   _   t   o   _   c   e   n   t   e   r   _   f   o   r   m   a   t   
   f   r   o   m       .   .   .   m   o   d   e   l   i   n   g   _   o   u   t   p   u   t   s       i   m   p   o   r   t       B   a   s   e   M   o   d   e   l   O   u   t   p   u   t   
   f   r   o   m       .   .   .   m   o   d   e   l   i   n   g   _   u   t   i   l   s       i   m   p   o   r   t       P   r   e   T   r   a   i   n   e   d   M   o   d   e   l   
   f   r   o   m       .   .   .   p   y   t   o   r   c   h   _   u   t   i   l   s       i   m   p   o   r   t       c   o   m   p   i   l   e   _   c   o   m   p   a   t   i   b   l   e   _   m   e   t   h   o   d   _   l   r   u   _   c   a   c   h   e   
   f   r   o   m       .   .   .   u   t   i   l   s       i   m   p   o   r   t       M   o   d   e   l   O   u   t   p   u   t   ,       a   u   t   o   _   d   o   c   s   t   r   i   n   g   ,       i   s   _   t   o   r   c   h   d   y   n   a   m   o   _   c   o   m   p   i   l   i   n   g   ,       t   o   r   c   h   _   i   n   t   
   f   r   o   m       .   .   .   u   t   i   l   s   .   b   a   c   k   b   o   n   e   _   u   t   i   l   s       i   m   p   o   r   t       l   o   a   d   _   b   a   c   k   b   o   n   e   
   f   r   o   m       .   c   o   n   f   i   g   u   r   a   t   i   o   n   _   d   _   f   i   n   e       i   m   p   o   r   t       D   F   i   n   e   C   o   n   f   i   g   
   
   
   d   e   f       m   u   l   t   i   _   s   c   a   l   e   _   d   e   f   o   r   m   a   b   l   e   _   a   t   t   e   n   t   i   o   n   _   v   2   (   
                   v   a   l   u   e   :       T   e   n   s   o   r   ,   
                   v   a   l   u   e   _   s   p   a   t   i   a   l   _   s   h   a   p   e   s   :       T   e   n   s   o   r   ,   
                   s   a   m   p   l   i   n   g   _   l   o   c   a   t   i   o   n   s   :       T   e   n   s   o   r   ,   
                   a   t   t   e   n   t   i   o   n   _   w   e   i   g   h   t   s   :       T   e   n   s   o   r   ,   
                   n   u   m   _   p   o   i   n   t   s   _   l   i   s   t   :       L   i   s   t   [   i   n   t   ]   ,   
                   m   e   t   h   o   d   =   "   d   e   f   a   u   l   t   "   ,   
   )       -   >       T   e   n   s   o   r   :   
                   b   a   t   c   h   _   s   i   z   e   ,       _   ,       n   u   m   _   h   e   a   d   s   ,       h   i   d   d   e   n   _   d   i   m       =       v   a   l   u   e   .   s   h   a   p   e   
                   _   ,       n   u   m   _   q   u   e   r   i   e   s   ,       n   u   m   _   h   e   a   d   s   ,       n   u   m   _   l   e   v   e   l   s   ,       n   u   m   _   p   o   i   n   t   s       =       s   a   m   p   l   i   n   g   _   l   o   c   a   t   i   o   n   s   .   s   h   a   p   e   
                   v   a   l   u   e   _   l   i   s   t       =       (   
                                   v   a   l   u   e   .   p   e   r   m   u   t   e   (   0   ,       2   ,       3   ,       1   )   
                                   .   f   l   a   t   t   e   n   (   0   ,       1   )   
                                   .   s   p   l   i   t   (   [   h   e   i   g   h   t       *       w   i   d   t   h       f   o   r       h   e   i   g   h   t   ,       w   i   d   t   h       i   n       v   a   l   u   e   _   s   p   a   t   i   a   l   _   s   h   a   p   e   s   ]   ,       d   i   m   =   -   1   )   
                   )   
                   #       s   a   m   p   l   i   n   g   _   o   f   f   s   e   t   s       [   8   ,       4   8   0   ,       8   ,       1   2   ,       2   ]   
                   i   f       m   e   t   h   o   d       =   =       "   d   e   f   a   u   l   t   "   :   
                                   s   a   m   p   l   i   n   g   _   g   r   i   d   s       =       2       *       s   a   m   p   l   i   n   g   _   l   o   c   a   t   i   o   n   s       -       1   
                   e   l   i   f       m   e   t   h   o   d       =   =       "   d   i   s   c   r   e   t   e   "   :   
                                   s   a   m   p   l   i   n   g   _   g   r   i   d   s       =       s   a   m   p   l   i   n   g   _   l   o   c   a   t   i   o   n   s   
                   s   a   m   p   l   i   n   g   _   g   r   i   d   s       =       s   a   m   p   l   i   n   g   _   g   r   i   d   s   .   p   e   r   m   u   t   e   (   0   ,       2   ,       1   ,       3   ,       4   )   .   f   l   a   t   t   e   n   (   0   ,       1   )   
                   s   a   m   p   l   i   n   g   _   g   r   i   d   s       =       s   a   m   p   l   i   n   g   _   g   r   i   d   s   .   s   p   l   i   t   (   n   u   m   _   p   o   i   n   t   s   _   l   i   s   t   ,       d   i   m   =   -   2   )   
                   s   a   m   p   l   i   n   g   _   v   a   l   u   e   _   l   i   s   t       =       [   ]   
                   f   o   r       l   e   v   e   l   _   i   d   ,       (   h   e   i   g   h   t   ,       w   i   d   t   h   )       i   n       e   n   u   m   e   r   a   t   e   (   v   a   l   u   e   _   s   p   a   t   i   a   l   _   s   h   a   p   e   s   )   :   
                                   #       b   a   t   c   h   _   s   i   z   e   ,       h   e   i   g   h   t   *   w   i   d   t   h   ,       n   u   m   _   h   e   a   d   s   ,       h   i   d   d   e   n   _   d   i   m   
                                   #       -   >       b   a   t   c   h   _   s   i   z   e   ,       h   e   i   g   h   t   *   w   i   d   t   h   ,       n   u   m   _   h   e   a   d   s   *   h   i   d   d   e   n   _   d   i   m   
                                   #       -   >       b   a   t   c   h   _   s   i   z   e   ,       n   u   m   _   h   e   a   d   s   *   h   i   d   d   e   n   _   d   i   m   ,       h   e   i   g   h   t   *   w   i   d   t   h   
                                   #       -   >       b   a   t   c   h   _   s   i   z   e   *   n   u   m   _   h   e   a   d   s   ,       h   i   d   d   e   n   _   d   i   m   ,       h   e   i   g   h   t   ,       w   i   d   t   h   
                                   v   a   l   u   e   _   l   _       =       v   a   l   u   e   _   l   i   s   t   [   l   e   v   e   l   _   i   d   ]   .   r   e   s   h   a   p   e   (   b   a   t   c   h   _   s   i   z   e       *       n   u   m   _   h   e   a   d   s   ,       h   i   d   d   e   n   _   d   i   m   ,       h   e   i   g   h   t   ,       w   i   d   t   h   )   
                                   #       b   a   t   c   h   _   s   i   z   e   ,       n   u   m   _   q   u   e   r   i   e   s   ,       n   u   m   _   h   e   a   d   s   ,       n   u   m   _   p   o   i   n   t   s   ,       2   
                                   #       -   >       b   a   t   c   h   _   s   i   z   e   ,       n   u   m   _   h   e   a   d   s   ,       n   u   m   _   q   u   e   r   i   e   s   ,       n   u   m   _   p   o   i   n   t   s   ,       2   
                                   #       -   >       b   a   t   c   h   _   s   i   z   e   *   n   u   m   _   h   e   a   d   s   ,       n   u   m   _   q   u   e   r   i   e   s   ,       n   u   m   _   p   o   i   n   t   s   ,       2   
                                   s   a   m   p   l   i   n   g   _   g   r   i   d   _   l   _       =       s   a   m   p   l   i   n   g   _   g   r   i   d   s   [   l   e   v   e   l   _   i   d   ]   
                                   #       b   a   t   c   h   _   s   i   z   e   *   n   u   m   _   h   e   a   d   s   ,       h   i   d   d   e   n   _   d   i   m   ,       n   u   m   _   q   u   e   r   i   e   s   ,       n   u   m   _   p   o   i   n   t   s   
                                   i   f       m   e   t   h   o   d       =   =       "   d   e   f   a   u   l   t   "   :   
                                                   s   a   m   p   l   i   n   g   _   v   a   l   u   e   _   l   _       =       n   n   .   f   u   n   c   t   i   o   n   a   l   .   g   r   i   d   _   s   a   m   p   l   e   (   
                                                                   v   a   l   u   e   _   l   _   ,       s   a   m   p   l   i   n   g   _   g   r   i   d   _   l   _   ,       m   o   d   e   =   "   b   i   l   i   n   e   a   r   "   ,       p   a   d   d   i   n   g   _   m   o   d   e   =   "   z   e   r   o   s   "   ,       a   l   i   g   n   _   c   o   r   n   e   r   s   =   F   a   l   s   e   
                                                   )   
                                   e   l   i   f       m   e   t   h   o   d       =   =       "   d   i   s   c   r   e   t   e   "   :   
                                                   s   a   m   p   l   i   n   g   _   c   o   o   r   d       =       (   s   a   m   p   l   i   n   g   _   g   r   i   d   _   l   _       *       t   o   r   c   h   .   t   e   n   s   o   r   (   [   [   w   i   d   t   h   ,       h   e   i   g   h   t   ]   ]   ,       d   e   v   i   c   e   =   v   a   l   u   e   .   d   e   v   i   c   e   )       +       0   .   5   )   .   t   o   (   
                                                                   t   o   r   c   h   .   i   n   t   6   4   
                                                   )   
   
                                                   #       S   e   p   a   r   a   t   e       c   l   a   m   p   i   n   g       f   o   r       x       a   n   d       y       c   o   o   r   d   i   n   a   t   e   s   
                                                   s   a   m   p   l   i   n   g   _   c   o   o   r   d   _   x       =       s   a   m   p   l   i   n   g   _   c   o   o   r   d   [   .   .   .   ,       0   ]   .   c   l   a   m   p   (   0   ,       w   i   d   t   h       -       1   )   
                                                   s   a   m   p   l   i   n   g   _   c   o   o   r   d   _   y       =       s   a   m   p   l   i   n   g   _   c   o   o   r   d   [   .   .   .   ,       1   ]   .   c   l   a   m   p   (   0   ,       h   e   i   g   h   t       -       1   )   
   
                                                   #       C   o   m   b   i   n   e       t   h   e       c   l   a   m   p   e   d       c   o   o   r   d   i   n   a   t   e   s   
                                                   s   a   m   p   l   i   n   g   _   c   o   o   r   d       =       t   o   r   c   h   .   s   t   a   c   k   (   [   s   a   m   p   l   i   n   g   _   c   o   o   r   d   _   x   ,       s   a   m   p   l   i   n   g   _   c   o   o   r   d   _   y   ]   ,       d   i   m   =   -   1   )   
                                                   s   a   m   p   l   i   n   g   _   c   o   o   r   d       =       s   a   m   p   l   i   n   g   _   c   o   o   r   d   .   r   e   s   h   a   p   e   (   b   a   t   c   h   _   s   i   z   e       *       n   u   m   _   h   e   a   d   s   ,       n   u   m   _   q   u   e   r   i   e   s       *       n   u   m   _   p   o   i   n   t   s   _   l   i   s   t   [   l   e   v   e   l   _   i   d   ]   ,       2   )   
                                                   s   a   m   p   l   i   n   g   _   i   d   x       =       (   
                                                                   t   o   r   c   h   .   a   r   a   n   g   e   (   s   a   m   p   l   i   n   g   _   c   o   o   r   d   .   s   h   a   p   e   [   0   ]   ,       d   e   v   i   c   e   =   v   a   l   u   e   .   d   e   v   i   c   e   )   
                                                                   .   u   n   s   q   u   e   e   z   e   (   -   1   )   
                                                                   .   r   e   p   e   a   t   (   1   ,       s   a   m   p   l   i   n   g   _   c   o   o   r   d   .   s   h   a   p   e   [   1   ]   )   
                                                   )   
                                                   s   a   m   p   l   i   n   g   _   v   a   l   u   e   _   l   _       =       v   a   l   u   e   _   l   _   [   s   a   m   p   l   i   n   g   _   i   d   x   ,       :   ,       s   a   m   p   l   i   n   g   _   c   o   o   r   d   [   .   .   .   ,       1   ]   ,       s   a   m   p   l   i   n   g   _   c   o   o   r   d   [   .   .   .   ,       0   ]   ]   
                                                   s   a   m   p   l   i   n   g   _   v   a   l   u   e   _   l   _       =       s   a   m   p   l   i   n   g   _   v   a   l   u   e   _   l   _   .   p   e   r   m   u   t   e   (   0   ,       2   ,       1   )   .   r   e   s   h   a   p   e   (   
                                                                   b   a   t   c   h   _   s   i   z   e       *       n   u   m   _   h   e   a   d   s   ,       h   i   d   d   e   n   _   d   i   m   ,       n   u   m   _   q   u   e   r   i   e   s   ,       n   u   m   _   p   o   i   n   t   s   _   l   i   s   t   [   l   e   v   e   l   _   i   d   ]   
                                                   )   
                                   s   a   m   p   l   i   n   g   _   v   a   l   u   e   _   l   i   s   t   .   a   p   p   e   n   d   (   s   a   m   p   l   i   n   g   _   v   a   l   u   e   _   l   _   )   
                   #       (   b   a   t   c   h   _   s   i   z   e   ,       n   u   m   _   q   u   e   r   i   e   s   ,       n   u   m   _   h   e   a   d   s   ,       n   u   m   _   l   e   v   e   l   s   ,       n   u   m   _   p   o   i   n   t   s   )   
                   #       -   >       (   b   a   t   c   h   _   s   i   z   e   ,       n   u   m   _   h   e   a   d   s   ,       n   u   m   _   q   u   e   r   i   e   s   ,       n   u   m   _   l   e   v   e   l   s   ,       n   u   m   _   p   o   i   n   t   s   )   
                   #       -   >       (   b   a   t   c   h   _   s   i   z   e   ,       n   u   m   _   h   e   a   d   s   ,       1   ,       n   u   m   _   q   u   e   r   i   e   s   ,       n   u   m   _   l   e   v   e   l   s   *   n   u   m   _   p   o   i   n   t   s   )   
                   a   t   t   e   n   t   i   o   n   _   w   e   i   g   h   t   s       =       a   t   t   e   n   t   i   o   n   _   w   e   i   g   h   t   s   .   p   e   r   m   u   t   e   (   0   ,       2   ,       1   ,       3   )   .   r   e   s   h   a   p   e   (   
                                   b   a   t   c   h   _   s   i   z   e       *       n   u   m   _   h   e   a   d   s   ,       1   ,       n   u   m   _   q   u   e   r   i   e   s   ,       s   u   m   (   n   u   m   _   p   o   i   n   t   s   _   l   i   s   t   )   
                   )   
                   o   u   t   p   u   t       =       (   
                                   (   t   o   r   c   h   .   c   o   n   c   a   t   (   s   a   m   p   l   i   n   g   _   v   a   l   u   e   _   l   i   s   t   ,       d   i   m   =   -   1   )       *       a   t   t   e   n   t   i   o   n   _   w   e   i   g   h   t   s   )   
                                   .   s   u   m   (   -   1   )   
                                   .   v   i   e   w   (   b   a   t   c   h   _   s   i   z   e   ,       n   u   m   _   h   e   a   d   s       *       h   i   d   d   e   n   _   d   i   m   ,       n   u   m   _   q   u   e   r   i   e   s   )   
                   )   
                   r   e   t   u   r   n       o   u   t   p   u   t   .   t   r   a   n   s   p   o   s   e   (   1   ,       2   )   .   c   o   n   t   i   g   u   o   u   s   (   )   
   
   
   c   l   a   s   s       D   F   i   n   e   M   u   l   t   i   s   c   a   l   e   D   e   f   o   r   m   a   b   l   e   A   t   t   e   n   t   i   o   n   (   n   n   .   M   o   d   u   l   e   )   :   
                   d   e   f       _   _   i   n   i   t   _   _   (   s   e   l   f   ,       c   o   n   f   i   g   :       D   F   i   n   e   C   o   n   f   i   g   )   :   
                                   "   "   "   
                                   D   -   F   i   n   e       v   e   r   s   i   o   n       o   f       m   u   l   t   i   s   c   a   l   e       d   e   f   o   r   m   a   b   l   e       a   t   t   e   n   t   i   o   n   
                                   "   "   "   
                                   s   u   p   e   r   (   )   .   _   _   i   n   i   t   _   _   (   )   
                                   s   e   l   f   .   d   _   m   o   d   e   l       =       c   o   n   f   i   g   .   d   _   m   o   d   e   l   
                                   s   e   l   f   .   n   _   h   e   a   d   s       =       c   o   n   f   i   g   .   d   e   c   o   d   e   r   _   a   t   t   e   n   t   i   o   n   _   h   e   a   d   s   
                                   s   e   l   f   .   n   _   l   e   v   e   l   s       =       c   o   n   f   i   g   .   n   u   m   _   f   e   a   t   u   r   e   _   l   e   v   e   l   s   
                                   s   e   l   f   .   o   f   f   s   e   t   _   s   c   a   l   e       =       c   o   n   f   i   g   .   d   e   c   o   d   e   r   _   o   f   f   s   e   t   _   s   c   a   l   e   
                                   s   e   l   f   .   d   e   c   o   d   e   r   _   m   e   t   h   o   d       =       c   o   n   f   i   g   .   d   e   c   o   d   e   r   _   m   e   t   h   o   d   
                                   s   e   l   f   .   n   _   p   o   i   n   t   s       =       c   o   n   f   i   g   .   d   e   c   o   d   e   r   _   n   _   p   o   i   n   t   s   
   
                                   i   f       i   s   i   n   s   t   a   n   c   e   (   s   e   l   f   .   n   _   p   o   i   n   t   s   ,       l   i   s   t   )   :   
                                                   n   u   m   _   p   o   i   n   t   s   _   l   i   s   t       =       s   e   l   f   .   n   _   p   o   i   n   t   s   
                                   e   l   s   e   :   
                                                   n   u   m   _   p   o   i   n   t   s   _   l   i   s   t       =       [   s   e   l   f   .   n   _   p   o   i   n   t   s       f   o   r       _       i   n       r   a   n   g   e   (   s   e   l   f   .   n   _   l   e   v   e   l   s   )   ]   
   
                                   s   e   l   f   .   n   u   m   _   p   o   i   n   t   s   _   l   i   s   t       =       n   u   m   _   p   o   i   n   t   s   _   l   i   s   t   
                                   n   u   m   _   p   o   i   n   t   s   _   s   c   a   l   e       =       [   1       /       n       f   o   r       n       i   n       s   e   l   f   .   n   u   m   _   p   o   i   n   t   s   _   l   i   s   t       f   o   r       _       i   n       r   a   n   g   e   (   n   )   ]   
                                   s   e   l   f   .   r   e   g   i   s   t   e   r   _   b   u   f   f   e   r   (   "   n   u   m   _   p   o   i   n   t   s   _   s   c   a   l   e   "   ,       t   o   r   c   h   .   t   e   n   s   o   r   (   n   u   m   _   p   o   i   n   t   s   _   s   c   a   l   e   ,       d   t   y   p   e   =   t   o   r   c   h   .   f   l   o   a   t   3   2   )   )   
   
                                   s   e   l   f   .   t   o   t   a   l   _   p   o   i   n   t   s       =       s   e   l   f   .   n   _   h   e   a   d   s       *       s   u   m   (   s   e   l   f   .   n   u   m   _   p   o   i   n   t   s   _   l   i   s   t   )   
   
                                   s   e   l   f   .   s   a   m   p   l   i   n   g   _   o   f   f   s   e   t   s       =       n   n   .   L   i   n   e   a   r   (   s   e   l   f   .   d   _   m   o   d   e   l   ,       s   e   l   f   .   t   o   t   a   l   _   p   o   i   n   t   s       *       2   )   
                                   s   e   l   f   .   a   t   t   e   n   t   i   o   n   _   w   e   i   g   h   t   s       =       n   n   .   L   i   n   e   a   r   (   s   e   l   f   .   d   _   m   o   d   e   l   ,       s   e   l   f   .   t   o   t   a   l   _   p   o   i   n   t   s   )   
   
                           